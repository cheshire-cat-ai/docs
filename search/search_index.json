{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello dear","text":""},{"location":"#hello-dear","title":"Hello dear","text":"<p>The Cheshire Cat is an open-source, hackable and production-ready framework that allows you to easily build and deploy AI agents.</p> <pre><code>\"Every adventure requires a first step\".\n(Alice's Adventures in Wonderland - Lewis Carroll)\n</code></pre>"},{"location":"#quick-links","title":"Quick links","text":"<p>\ud83c\udfc3 Get started now</p> <p>\ud83e\ude9d Available Hooks</p> <p>\ud83d\udd0c Plugin documentation</p> <p>\ud83d\ude80 Docker Compose examples</p>"},{"location":"#why-the-cat","title":"Why the Cat","text":"<ul> <li>easy to customize</li> <li>easy to deploy</li> <li>here since 2023</li> </ul>"},{"location":"#get-in-touch-with-us","title":"Get in touch with us","text":"<p>Join our Discord  community where you can:</p> <ul> <li>connect with other AI builders</li> <li>ask for support</li> <li>see what people have done with the Cat</li> <li>join community meetings</li> </ul>"},{"location":"#license","title":"License","text":"<p>GNU General Public License v3.0</p>"},{"location":"#are-you-lost","title":"Are you lost?","text":"<p>Paid support and customization available.</p> <p></p> <pre><code>\"Would you tell me, please, which way I ought to go from here?\"\n\"That depends a good deal on where you want to get to,\" said the Cat.\n\"I don't much care where--\" said Alice.\n\"Then it doesn't matter which way you go,\" said the Cat.\n\n(Alice's Adventures in Wonderland - Lewis Carroll)\n</code></pre>"},{"location":"API_Documentation/SUMMARY/","title":"SUMMARY","text":"<ul> <li>agents<ul> <li>base_agent</li> <li>form_agent</li> <li>main_agent</li> <li>memory_agent</li> <li>procedures_agent</li> </ul> </li> <li>auth<ul> <li>auth_utils</li> </ul> </li> <li>log</li> <li>looking_glass<ul> <li>cheshire_cat</li> <li>stray_cat</li> </ul> </li> <li>mad_hatter<ul> <li>core_plugin<ul> <li>hooks<ul> <li>agent</li> <li>flow</li> <li>prompt</li> <li>rabbithole</li> </ul> </li> </ul> </li> <li>mad_hatter</li> <li>plugin</li> </ul> </li> <li>memory<ul> <li>vector_memory</li> <li>vector_memory_collection</li> <li>working_memory</li> </ul> </li> <li>rabbit_hole</li> <li>routes<ul> <li>settings</li> </ul> </li> <li>utils</li> </ul>"},{"location":"API_Documentation/log/","title":"log","text":"<p>The log engine.</p>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine","title":"<code>CatLogEngine</code>","text":"<p>The log engine.</p> <p>Engine to filter the logs in the terminal according to the level of severity.</p> <p>Attributes:</p> Name Type Description <code>LOG_LEVEL</code> <code>str</code> <p>Level of logging set in the <code>.env</code> file.</p> Notes <p>The logging level set in the <code>.env</code> file will print all the logs from that level to above. Available levels are:</p> <pre><code>- `DEBUG`\n- `INFO`\n- `WARNING`\n- `ERROR`\n- `CRITICAL`\n</code></pre> <p>Default to <code>CCAT_LOG_LEVEL</code> env variable (<code>INFO</code>).</p> Source code in <code>cat/log.py</code> <pre><code>class CatLogEngine:\n    \"\"\"The log engine.\n\n    Engine to filter the logs in the terminal according to the level of severity.\n\n    Attributes\n    ----------\n    LOG_LEVEL : str\n        Level of logging set in the `.env` file.\n\n    Notes\n    -----\n    The logging level set in the `.env` file will print all the logs from that level to above.\n    Available levels are:\n\n        - `DEBUG`\n        - `INFO`\n        - `WARNING`\n        - `ERROR`\n        - `CRITICAL`\n\n    Default to `CCAT_LOG_LEVEL` env variable (`INFO`).\n    \"\"\"\n\n    def __init__(self):\n        self.LOG_LEVEL = get_log_level()\n        self.default_log()\n\n        # workaround for pdfminer logging\n        # https://github.com/pdfminer/pdfminer.six/issues/347\n        logging.getLogger(\"pdfminer\").setLevel(logging.WARNING)\n\n    def show_log_level(self, record):\n        \"\"\"Allows to show stuff in the log based on the global setting.\n\n        Parameters\n        ----------\n        record : dict\n\n        Returns\n        -------\n        bool\n\n        \"\"\"\n        return record[\"level\"].no &gt;= logger.level(self.LOG_LEVEL).no\n\n    def default_log(self):\n        \"\"\"Set the same debug level to all the project dependencies.\n\n        Returns\n        -------\n        \"\"\"\n\n        level = \"&lt;level&gt;{level}:&lt;/level&gt;\"\n        # time = \"&lt;green&gt;[{time:YYYY-MM-DD HH:mm:ss.SSS}]&lt;/green&gt;\"\n        # origin = \"&lt;level&gt;{extra[original_name]}.{extra[original_class]}.{extra[original_caller]}::{extra[original_line]}&lt;/level&gt;\"\n        message = \"&lt;level&gt;{message}&lt;/level&gt;\"\n        log_format = f\"{level}\\t{message}\"\n\n        logger.remove()\n        logger.add(\n            sys.stdout,\n            level=self.LOG_LEVEL,\n            colorize=True,\n            format=log_format,\n            # backtrace=True,\n            # diagnose=True,\n            filter=self.show_log_level,\n        )\n\n    def __call__(self, msg, level=\"DEBUG\"):\n        \"\"\"Alias of self.log()\"\"\"\n        self.log(msg, level)\n\n    def debug(self, msg):\n        \"\"\"Logs a DEBUG message\"\"\"\n        self.log(msg, level=\"DEBUG\")\n\n    def info(self, msg):\n        \"\"\"Logs an INFO message\"\"\"\n        self.log(msg, level=\"INFO\")\n\n    def warning(self, msg):\n        \"\"\"Logs a WARNING message\"\"\"\n        self.log(msg, level=\"WARNING\")\n\n    def error(self, msg):\n        \"\"\"Logs an ERROR message\"\"\"\n        self.log(msg, level=\"ERROR\")\n\n        # Only print the traceback if an exception handler is being executed\n        if sys.exc_info()[0] is not None:\n            traceback.print_exc()\n\n    def critical(self, msg):\n        \"\"\"Logs a CRITICAL message\"\"\"\n        self.log(msg, level=\"CRITICAL\")\n\n        # Only print the traceback if an exception handler is being executed\n        if sys.exc_info()[0] is not None:\n            traceback.print_exc()\n\n    def log(self, msg, level=\"DEBUG\"):\n        \"\"\"Log a message\n\n        Parameters\n        ----------\n        msg :\n            Message to be logged.\n        level : str\n            Logging level.\"\"\"\n\n        # prettify\n        if isinstance(msg, str):\n            pass\n        elif type(msg) in [dict, list]:  # TODO: should be recursive\n            try:\n                msg = json.dumps(msg, indent=4)\n            except Exception:\n                pass\n        else:\n            msg = pformat(msg)\n\n        # actual log\n        lines = msg.split(\"\\n\")\n        for line in lines:\n            logger.log(level, line)\n\n    def welcome(self):\n        \"\"\"Welcome message in the terminal.\"\"\"\n        secure = \"s\" if get_env(\"CCAT_CORE_USE_SECURE_PROTOCOLS\") in (\"true\", \"1\") else \"\"\n\n        cat_host = get_env(\"CCAT_CORE_HOST\")\n        cat_port = get_env(\"CCAT_CORE_PORT\")\n        cat_address = f\"http{secure}://{cat_host}:{cat_port}\"\n\n        print(\"\\n\\n\")\n        with open(\"cat/welcome.txt\", \"r\") as f:\n            print(f.read())\n\n        left_margin = \" \" * 15\n        print(f\"\\n\\n{left_margin} Cat REST API:   {cat_address}/docs\")\n        print(f\"{left_margin} Cat ADMIN:      {cat_address}/admin\\n\\n\")\n\n        # self.log_examples()\n\n\n    def log_examples(self):\n        \"\"\"Log examples for the log engine.\"\"\"\n\n        for c in [self, \"Hello there!\", {\"ready\", \"set\", \"go\"}, [1, 4, \"sdfsf\"], {\"a\": 1, \"b\": {\"c\": 2}}]:\n            self.debug(c)\n            self.info(c)\n            self.warning(c)\n            self.error(c)\n            self.critical(c)\n\n        def intentional_error():\n            print(42/0)\n\n        try:\n            intentional_error()\n        except Exception:\n            self.error(\"This error is just for demonstration purposes.\")\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine.__call__","title":"<code>__call__(msg, level='DEBUG')</code>","text":"<p>Alias of self.log()</p> Source code in <code>cat/log.py</code> <pre><code>def __call__(self, msg, level=\"DEBUG\"):\n    \"\"\"Alias of self.log()\"\"\"\n    self.log(msg, level)\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine.critical","title":"<code>critical(msg)</code>","text":"<p>Logs a CRITICAL message</p> Source code in <code>cat/log.py</code> <pre><code>def critical(self, msg):\n    \"\"\"Logs a CRITICAL message\"\"\"\n    self.log(msg, level=\"CRITICAL\")\n\n    # Only print the traceback if an exception handler is being executed\n    if sys.exc_info()[0] is not None:\n        traceback.print_exc()\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine.debug","title":"<code>debug(msg)</code>","text":"<p>Logs a DEBUG message</p> Source code in <code>cat/log.py</code> <pre><code>def debug(self, msg):\n    \"\"\"Logs a DEBUG message\"\"\"\n    self.log(msg, level=\"DEBUG\")\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine.default_log","title":"<code>default_log()</code>","text":"<p>Set the same debug level to all the project dependencies.</p> Source code in <code>cat/log.py</code> <pre><code>def default_log(self):\n    \"\"\"Set the same debug level to all the project dependencies.\n\n    Returns\n    -------\n    \"\"\"\n\n    level = \"&lt;level&gt;{level}:&lt;/level&gt;\"\n    # time = \"&lt;green&gt;[{time:YYYY-MM-DD HH:mm:ss.SSS}]&lt;/green&gt;\"\n    # origin = \"&lt;level&gt;{extra[original_name]}.{extra[original_class]}.{extra[original_caller]}::{extra[original_line]}&lt;/level&gt;\"\n    message = \"&lt;level&gt;{message}&lt;/level&gt;\"\n    log_format = f\"{level}\\t{message}\"\n\n    logger.remove()\n    logger.add(\n        sys.stdout,\n        level=self.LOG_LEVEL,\n        colorize=True,\n        format=log_format,\n        # backtrace=True,\n        # diagnose=True,\n        filter=self.show_log_level,\n    )\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine.error","title":"<code>error(msg)</code>","text":"<p>Logs an ERROR message</p> Source code in <code>cat/log.py</code> <pre><code>def error(self, msg):\n    \"\"\"Logs an ERROR message\"\"\"\n    self.log(msg, level=\"ERROR\")\n\n    # Only print the traceback if an exception handler is being executed\n    if sys.exc_info()[0] is not None:\n        traceback.print_exc()\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine.info","title":"<code>info(msg)</code>","text":"<p>Logs an INFO message</p> Source code in <code>cat/log.py</code> <pre><code>def info(self, msg):\n    \"\"\"Logs an INFO message\"\"\"\n    self.log(msg, level=\"INFO\")\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine.log","title":"<code>log(msg, level='DEBUG')</code>","text":"<p>Log a message</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <p>Message to be logged.</p> required <code>level</code> <code>str</code> <p>Logging level.</p> <code>'DEBUG'</code> Source code in <code>cat/log.py</code> <pre><code>def log(self, msg, level=\"DEBUG\"):\n    \"\"\"Log a message\n\n    Parameters\n    ----------\n    msg :\n        Message to be logged.\n    level : str\n        Logging level.\"\"\"\n\n    # prettify\n    if isinstance(msg, str):\n        pass\n    elif type(msg) in [dict, list]:  # TODO: should be recursive\n        try:\n            msg = json.dumps(msg, indent=4)\n        except Exception:\n            pass\n    else:\n        msg = pformat(msg)\n\n    # actual log\n    lines = msg.split(\"\\n\")\n    for line in lines:\n        logger.log(level, line)\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine.log_examples","title":"<code>log_examples()</code>","text":"<p>Log examples for the log engine.</p> Source code in <code>cat/log.py</code> <pre><code>def log_examples(self):\n    \"\"\"Log examples for the log engine.\"\"\"\n\n    for c in [self, \"Hello there!\", {\"ready\", \"set\", \"go\"}, [1, 4, \"sdfsf\"], {\"a\": 1, \"b\": {\"c\": 2}}]:\n        self.debug(c)\n        self.info(c)\n        self.warning(c)\n        self.error(c)\n        self.critical(c)\n\n    def intentional_error():\n        print(42/0)\n\n    try:\n        intentional_error()\n    except Exception:\n        self.error(\"This error is just for demonstration purposes.\")\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine.show_log_level","title":"<code>show_log_level(record)</code>","text":"<p>Allows to show stuff in the log based on the global setting.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>dict</code> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>cat/log.py</code> <pre><code>def show_log_level(self, record):\n    \"\"\"Allows to show stuff in the log based on the global setting.\n\n    Parameters\n    ----------\n    record : dict\n\n    Returns\n    -------\n    bool\n\n    \"\"\"\n    return record[\"level\"].no &gt;= logger.level(self.LOG_LEVEL).no\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine.warning","title":"<code>warning(msg)</code>","text":"<p>Logs a WARNING message</p> Source code in <code>cat/log.py</code> <pre><code>def warning(self, msg):\n    \"\"\"Logs a WARNING message\"\"\"\n    self.log(msg, level=\"WARNING\")\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.CatLogEngine.welcome","title":"<code>welcome()</code>","text":"<p>Welcome message in the terminal.</p> Source code in <code>cat/log.py</code> <pre><code>def welcome(self):\n    \"\"\"Welcome message in the terminal.\"\"\"\n    secure = \"s\" if get_env(\"CCAT_CORE_USE_SECURE_PROTOCOLS\") in (\"true\", \"1\") else \"\"\n\n    cat_host = get_env(\"CCAT_CORE_HOST\")\n    cat_port = get_env(\"CCAT_CORE_PORT\")\n    cat_address = f\"http{secure}://{cat_host}:{cat_port}\"\n\n    print(\"\\n\\n\")\n    with open(\"cat/welcome.txt\", \"r\") as f:\n        print(f.read())\n\n    left_margin = \" \" * 15\n    print(f\"\\n\\n{left_margin} Cat REST API:   {cat_address}/docs\")\n    print(f\"{left_margin} Cat ADMIN:      {cat_address}/admin\\n\\n\")\n</code></pre>"},{"location":"API_Documentation/log/#cat.log.get_log_level","title":"<code>get_log_level()</code>","text":"<p>Return the global LOG level.</p> Source code in <code>cat/log.py</code> <pre><code>def get_log_level():\n    \"\"\"Return the global LOG level.\"\"\"\n    return get_env(\"CCAT_LOG_LEVEL\")\n</code></pre>"},{"location":"API_Documentation/rabbit_hole/","title":"rabbit_hole","text":""},{"location":"API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole","title":"<code>RabbitHole</code>","text":"<p>Manages content ingestion. I'm late... I'm late!</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>@singleton\nclass RabbitHole:\n    \"\"\"Manages content ingestion. I'm late... I'm late!\"\"\"\n\n    def __init__(self, cat) -&gt; None:\n        self.__cat = cat\n\n    # each time we access the file handlers, plugins can intervene\n    def __reload_file_handlers(self):\n        # default file handlers\n        self.__file_handlers = {\n            \"application/pdf\": PDFMinerParser(),\n            \"text/plain\": TextParser(),\n            \"text/markdown\": TextParser(),\n            \"text/html\": BS4HTMLParser(),\n        }\n\n        # no access to StrayCat yet\n        self.__file_handlers = self.__cat.mad_hatter.execute_hook(\n            \"rabbithole_instantiates_parsers\", self.__file_handlers, cat=self.__cat\n        )\n\n    def __reload_text_splitter(self):\n        # default text splitter\n        self.__text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            chunk_size=256,\n            chunk_overlap=64,\n            separators=[\"\\\\n\\\\n\", \"\\n\\n\", \".\\\\n\", \".\\n\", \"\\\\n\", \"\\n\", \" \", \"\"],\n            encoding_name=\"cl100k_base\",\n            keep_separator=True,\n            strip_whitespace=True,\n        )\n\n        # no access to StrayCat yet\n        self.__text_splitter = self.__cat.mad_hatter.execute_hook(\n            \"rabbithole_instantiates_splitter\", self.__text_splitter, cat=self.__cat\n        )\n\n    def ingest_memory(\n            self,\n            cat,\n            file: UploadFile\n        ):\n        \"\"\"Upload memories to the declarative memory from a JSON file.\n\n        Parameters\n        ----------\n        file : UploadFile\n            File object sent via `rabbithole/memory` hook.\n\n        Notes\n        -----\n        This method allows uploading a JSON file containing vector and text memories directly to the declarative memory.\n        When doing this, please, make sure the embedder used to export the memories is the same as the one used\n        when uploading.\n        The method also performs a check on the dimensionality of the embeddings (i.e. length of each vector).\n\n        \"\"\"\n\n        # Get file bytes\n        file_bytes = file.file.read()\n\n        # Load fyle byte in a dict\n        memories = json.loads(file_bytes.decode(\"utf-8\"))\n\n        # Check the embedder used for the uploaded memories is the same the Cat is using now\n        upload_embedder = memories[\"embedder\"]\n        cat_embedder = str(cat.embedder.__class__.__name__)\n\n        if upload_embedder != cat_embedder:\n            message = f\"Embedder mismatch: file embedder {upload_embedder} is different from {cat_embedder}\"\n            raise Exception(message)\n\n        # Get Declarative memories in file\n        declarative_memories = memories[\"collections\"][\"declarative\"]\n\n        # Store data to upload the memories in batch\n        ids = [i[\"id\"] for i in declarative_memories]\n        payloads = [\n            {\"page_content\": p[\"page_content\"], \"metadata\": p[\"metadata\"]}\n            for p in declarative_memories\n        ]\n        vectors = [v[\"vector\"] for v in declarative_memories]\n\n        log.info(f\"Preparing to load {len(vectors)} vector memories\")\n\n        # Check embedding size is correct\n        embedder_size = cat.memory.vectors.declarative.embedder_size\n        len_mismatch = [len(v) == embedder_size for v in vectors]\n\n        if not any(len_mismatch):\n            message = (\n                f\"Embedding size mismatch: vectors length should be {embedder_size}\"\n            )\n            raise Exception(message)\n\n        # Upsert memories in batch mode # TODO REFACTOR: use VectorMemoryCollection.add_point\n        cat.memory.vectors.vector_db.upsert(\n            collection_name=\"declarative\",\n            points=models.Batch(ids=ids, payloads=payloads, vectors=vectors),\n        )\n\n    def ingest_file(\n        self,\n        cat,\n        file: Union[str, UploadFile],\n        chunk_size: int | None = None,\n        chunk_overlap: int | None = None,\n        metadata: dict = {}\n    ):\n        \"\"\"Load a file in the Cat's declarative memory.\n\n        The method splits and converts the file in Langchain `Document`. Then, it stores the `Document` in the Cat's\n        memory.\n\n        Parameters\n        ----------\n        file : str, UploadFile\n            The file can be a path passed as a string or an `UploadFile` object if the document is ingested using the\n            `rabbithole` endpoint.\n        chunk_size : int\n            Number of tokens in each document chunk.\n        chunk_overlap : int\n            Number of overlapping tokens between consecutive chunks.\n        metadata : dict\n            Metadata to be stored with each chunk.\n\n        Notes\n        ----------\n        Currently supported formats are `.txt`, `.pdf` and `.md`.\n        You cn add custom ones or substitute the above via RabbitHole hooks.\n\n        See Also\n        ----------\n        before_rabbithole_stores_documents\n        \"\"\"\n\n        # split file into a list of docs\n        docs = self.file_to_docs(\n            cat=cat,\n            file=file,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap\n        )\n\n        # store in memory\n        if isinstance(file, str):\n            filename = file\n        else:\n            filename = file.filename\n\n        self.store_documents(cat=cat, docs=docs, source=filename, metadata=metadata)\n\n    def file_to_docs(\n        self,\n        cat,\n        file: Union[str, UploadFile],\n        chunk_size: int | None = None,\n        chunk_overlap: int | None = None\n    ) -&gt; List[Document]:\n        \"\"\"Load and convert files to Langchain `Document`.\n\n        This method takes a file either from a Python script, from the `/rabbithole/` or `/rabbithole/web` endpoints.\n        Hence, it loads it in memory and splits it in overlapped chunks of text.\n\n        Parameters\n        ----------\n        file : str, UploadFile\n            The file can be either a string path if loaded programmatically, a FastAPI `UploadFile`\n            if coming from the `/rabbithole/` endpoint or a URL if coming from the `/rabbithole/web` endpoint.\n        chunk_size : int\n            Number of tokens in each document chunk.\n        chunk_overlap : int\n            Number of overlapping tokens between consecutive chunks.\n\n        Returns\n        -------\n        docs : List[Document]\n            List of Langchain `Document` of chunked text.\n\n        Notes\n        -----\n        This method is used by both `/rabbithole/` and `/rabbithole/web` endpoints.\n        Currently supported files are `.txt`, `.pdf`, `.md` and web pages.\n\n        \"\"\"\n\n        # Check type of incoming file.\n        if isinstance(file, UploadFile):\n            # Get mime type and source of UploadFile\n            content_type = mimetypes.guess_type(file.filename)[0]\n            source = file.filename\n\n            # Get file bytes\n            file_bytes = file.file.read()\n        elif isinstance(file, str):\n            # Check if string file is a string or url\n            parsed_file = urlparse(file)\n            is_url = all([parsed_file.scheme, parsed_file.netloc])\n\n            if is_url:\n                # Make a request with a fake browser name\n                request = httpx.get(file, headers={\"User-Agent\": \"Magic Browser\"})\n\n                # Define mime type and source of url\n                content_type = request.headers[\"Content-Type\"].split(\";\")[0]\n                source = file\n\n                try:\n                    # Get binary content of url\n                    file_bytes = request.content\n                except HTTPError as e:\n                    log.error(e)\n            else:\n                # Get mime type from file extension and source\n                content_type = mimetypes.guess_type(file)[0]\n                source = os.path.basename(file)\n\n                # Get file bytes\n                with open(file, \"rb\") as f:\n                    file_bytes = f.read()\n        else:\n            raise ValueError(f\"{type(file)} is not a valid type.\")\n        return self.string_to_docs(\n            cat=cat,\n            file_bytes=file_bytes,\n            source=source,\n            content_type=content_type,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap\n        )\n\n    def string_to_docs(\n        self,\n        cat,\n        file_bytes: str,\n        source: str = None,\n        content_type: str = \"text/plain\",\n        chunk_size: int | None = None,\n        chunk_overlap: int | None = None\n    ) -&gt; List[Document]:\n        \"\"\"Convert string to Langchain `Document`.\n\n        Takes a string, converts it to langchain `Document`.\n        Hence, loads it in memory and splits it in overlapped chunks of text.\n\n        Parameters\n        ----------\n        file_bytes : str\n            The string to be converted.\n        source: str\n            Source filename.\n        content_type:\n            Mimetype of content.\n        chunk_size : int\n            Number of tokens in each document chunk.\n        chunk_overlap : int\n            Number of overlapping tokens between consecutive chunks.\n\n        Returns\n        -------\n        docs : List[Document]\n            List of Langchain `Document` of chunked text.\n        \"\"\"\n\n        # Load the bytes in the Blob schema\n        blob = Blob(data=file_bytes, mimetype=content_type, source=source).from_data(\n            data=file_bytes, mime_type=content_type, path=source\n        )\n        # Parser based on the mime type\n        parser = MimeTypeBasedParser(handlers=self.file_handlers)\n\n        # Parse the text\n        cat.send_ws_message(\n            \"I'm parsing the content. Big content could require some minutes...\"\n        )\n        super_docs = parser.parse(blob)\n\n        # Split\n        cat.send_ws_message(\"Parsing completed. Now let's go with reading process...\")\n        docs = self.__split_text(\n            cat=cat,\n            text=super_docs,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n        )\n        return docs\n\n    def store_documents(\n            self,\n            cat,\n            docs: List[Document],\n            source: str, # TODOV2: is this necessary?\n            metadata: dict = {}\n        ) -&gt; None:\n        \"\"\"Add documents to the Cat's declarative memory.\n\n        This method loops a list of Langchain `Document` and adds some metadata. Namely, the source filename and the\n        timestamp of insertion. Once done, the method notifies the client via Websocket connection.\n\n        Parameters\n        ----------\n        docs : List[Document]\n            List of Langchain `Document` to be inserted in the Cat's declarative memory.\n        source : str\n            Source name to be added as a metadata. It can be a file name or an URL.\n        metadata : dict\n            Metadata to be stored with each chunk.\n\n        Notes\n        -------\n        At this point, it is possible to customize the Cat's behavior using the `before_rabbithole_insert_memory` hook\n        to edit the memories before they are inserted in the vector database.\n\n        See Also\n        --------\n        before_rabbithole_insert_memory\n        \"\"\"\n\n        log.info(f\"Preparing to memorize {len(docs)} vectors\")\n\n        # hook the docs before they are stored in the vector memory\n        docs = cat.mad_hatter.execute_hook(\n            \"before_rabbithole_stores_documents\", docs, cat=cat\n        )\n\n        # classic embed\n        time_last_notification = time.time()\n        time_interval = 10  # a notification every 10 secs\n        stored_points = []\n        for d, doc in enumerate(docs):\n            if time.time() - time_last_notification &gt; time_interval:\n                time_last_notification = time.time()\n                perc_read = int(d / len(docs) * 100)\n                read_message = f\"Read {perc_read}% of {source}\"\n                cat.send_ws_message(read_message)\n                log.info(read_message)\n\n            # add default metadata\n            doc.metadata[\"source\"] = source\n            doc.metadata[\"when\"] = time.time()\n            # add custom metadata (sent via endpoint)\n            for k,v in metadata.items():\n                doc.metadata[k] = v\n\n            doc = cat.mad_hatter.execute_hook(\n                \"before_rabbithole_insert_memory\", doc, cat=cat\n            )\n            inserting_info = f\"{d + 1}/{len(docs)}):    {doc.page_content}\"\n            if doc.page_content != \"\":\n                doc_embedding = cat.embedder.embed_documents([doc.page_content])\n                stored_point = cat.memory.vectors.declarative.add_point(\n                    doc.page_content,\n                    doc_embedding[0],\n                    doc.metadata,\n                )\n                stored_points.append(stored_point)\n\n                log.info(f\"Inserted into memory ({inserting_info})\")\n            else:\n                log.info(f\"Skipped memory insertion of empty doc ({inserting_info})\")\n\n            # wait a little to avoid APIs rate limit errors\n            time.sleep(0.05)\n\n        # hook the points after they are stored in the vector memory\n        cat.mad_hatter.execute_hook(\n            \"after_rabbithole_stored_documents\", source, stored_points, cat=cat\n        )\n\n        # notify client\n        finished_reading_message = (\n            f\"Finished reading {source}, I made {len(docs)} thoughts on it.\"\n        )\n\n        cat.send_ws_message(finished_reading_message)\n\n        log.info(f\"Done uploading {source}\")\n\n    def __split_text(self, cat, text, chunk_size, chunk_overlap):\n        \"\"\"Split text in overlapped chunks.\n\n        This method executes the `rabbithole_splits_text` to split the incoming text in overlapped\n        chunks of text. Other two hooks are available to edit the text before and after the split step.\n\n        Parameters\n        ----------\n        text : str\n            Content of the loaded file.\n        chunk_size : int\n            Number of tokens in each document chunk.\n        chunk_overlap : int\n            Number of overlapping tokens between consecutive chunks.\n\n        Returns\n        -------\n        docs : List[Document]\n            List of split Langchain `Document`.\n\n        Notes\n        -----\n        The default behavior only executes the `rabbithole_splits_text` hook. `before_rabbithole_splits_text` and\n        `after_rabbithole_splitted_text` hooks return the original input without any modification.\n\n        See Also\n        --------\n        before_rabbithole_splits_text\n        rabbithole_splits_text\n        after_rabbithole_splitted_text\n\n        \"\"\"\n        # do something on the text before it is split\n        text = cat.mad_hatter.execute_hook(\n            \"before_rabbithole_splits_text\", text, cat=cat\n        )\n\n        # hooks decide the test splitter (see @property .text_splitter)\n        text_splitter = self.text_splitter\n\n        # override chunk_size and chunk_overlap only if the request has those info\n        if chunk_size:\n            text_splitter._chunk_size = chunk_size\n        if chunk_overlap:\n            text_splitter._chunk_overlap = chunk_overlap\n\n        log.info(f\"Chunk size: {chunk_size}, chunk overlap: {chunk_overlap}\")\n        # split text\n        docs = text_splitter.split_documents(text)\n        # remove short texts (page numbers, isolated words, etc.)\n        # TODO: join each short chunk with previous one, instead of deleting them\n        docs = list(filter(lambda d: len(d.page_content) &gt; 10, docs))\n\n        # do something on the text after it is split\n        docs = cat.mad_hatter.execute_hook(\n            \"after_rabbithole_splitted_text\", docs, cat=cat\n        )\n\n        return docs\n\n    # each time we access the file handlers, plugins can intervene\n    @property\n    def file_handlers(self):\n        self.__reload_file_handlers()\n        return self.__file_handlers\n\n    # each time we access the text splitter, plugins can intervene\n    @property\n    def text_splitter(self):\n        self.__reload_text_splitter()\n        return self.__text_splitter\n</code></pre>"},{"location":"API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.__split_text","title":"<code>__split_text(cat, text, chunk_size, chunk_overlap)</code>","text":"<p>Split text in overlapped chunks.</p> <p>This method executes the <code>rabbithole_splits_text</code> to split the incoming text in overlapped chunks of text. Other two hooks are available to edit the text before and after the split step.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Content of the loaded file.</p> required <code>chunk_size</code> <code>int</code> <p>Number of tokens in each document chunk.</p> required <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping tokens between consecutive chunks.</p> required <p>Returns:</p> Name Type Description <code>docs</code> <code>List[Document]</code> <p>List of split Langchain <code>Document</code>.</p> Notes <p>The default behavior only executes the <code>rabbithole_splits_text</code> hook. <code>before_rabbithole_splits_text</code> and <code>after_rabbithole_splitted_text</code> hooks return the original input without any modification.</p> See Also <p>before_rabbithole_splits_text rabbithole_splits_text after_rabbithole_splitted_text</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def __split_text(self, cat, text, chunk_size, chunk_overlap):\n    \"\"\"Split text in overlapped chunks.\n\n    This method executes the `rabbithole_splits_text` to split the incoming text in overlapped\n    chunks of text. Other two hooks are available to edit the text before and after the split step.\n\n    Parameters\n    ----------\n    text : str\n        Content of the loaded file.\n    chunk_size : int\n        Number of tokens in each document chunk.\n    chunk_overlap : int\n        Number of overlapping tokens between consecutive chunks.\n\n    Returns\n    -------\n    docs : List[Document]\n        List of split Langchain `Document`.\n\n    Notes\n    -----\n    The default behavior only executes the `rabbithole_splits_text` hook. `before_rabbithole_splits_text` and\n    `after_rabbithole_splitted_text` hooks return the original input without any modification.\n\n    See Also\n    --------\n    before_rabbithole_splits_text\n    rabbithole_splits_text\n    after_rabbithole_splitted_text\n\n    \"\"\"\n    # do something on the text before it is split\n    text = cat.mad_hatter.execute_hook(\n        \"before_rabbithole_splits_text\", text, cat=cat\n    )\n\n    # hooks decide the test splitter (see @property .text_splitter)\n    text_splitter = self.text_splitter\n\n    # override chunk_size and chunk_overlap only if the request has those info\n    if chunk_size:\n        text_splitter._chunk_size = chunk_size\n    if chunk_overlap:\n        text_splitter._chunk_overlap = chunk_overlap\n\n    log.info(f\"Chunk size: {chunk_size}, chunk overlap: {chunk_overlap}\")\n    # split text\n    docs = text_splitter.split_documents(text)\n    # remove short texts (page numbers, isolated words, etc.)\n    # TODO: join each short chunk with previous one, instead of deleting them\n    docs = list(filter(lambda d: len(d.page_content) &gt; 10, docs))\n\n    # do something on the text after it is split\n    docs = cat.mad_hatter.execute_hook(\n        \"after_rabbithole_splitted_text\", docs, cat=cat\n    )\n\n    return docs\n</code></pre>"},{"location":"API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.file_to_docs","title":"<code>file_to_docs(cat, file, chunk_size=None, chunk_overlap=None)</code>","text":"<p>Load and convert files to Langchain <code>Document</code>.</p> <p>This method takes a file either from a Python script, from the <code>/rabbithole/</code> or <code>/rabbithole/web</code> endpoints. Hence, it loads it in memory and splits it in overlapped chunks of text.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>(str, UploadFile)</code> <p>The file can be either a string path if loaded programmatically, a FastAPI <code>UploadFile</code> if coming from the <code>/rabbithole/</code> endpoint or a URL if coming from the <code>/rabbithole/web</code> endpoint.</p> required <code>chunk_size</code> <code>int</code> <p>Number of tokens in each document chunk.</p> <code>None</code> <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping tokens between consecutive chunks.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>docs</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code> of chunked text.</p> Notes <p>This method is used by both <code>/rabbithole/</code> and <code>/rabbithole/web</code> endpoints. Currently supported files are <code>.txt</code>, <code>.pdf</code>, <code>.md</code> and web pages.</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def file_to_docs(\n    self,\n    cat,\n    file: Union[str, UploadFile],\n    chunk_size: int | None = None,\n    chunk_overlap: int | None = None\n) -&gt; List[Document]:\n    \"\"\"Load and convert files to Langchain `Document`.\n\n    This method takes a file either from a Python script, from the `/rabbithole/` or `/rabbithole/web` endpoints.\n    Hence, it loads it in memory and splits it in overlapped chunks of text.\n\n    Parameters\n    ----------\n    file : str, UploadFile\n        The file can be either a string path if loaded programmatically, a FastAPI `UploadFile`\n        if coming from the `/rabbithole/` endpoint or a URL if coming from the `/rabbithole/web` endpoint.\n    chunk_size : int\n        Number of tokens in each document chunk.\n    chunk_overlap : int\n        Number of overlapping tokens between consecutive chunks.\n\n    Returns\n    -------\n    docs : List[Document]\n        List of Langchain `Document` of chunked text.\n\n    Notes\n    -----\n    This method is used by both `/rabbithole/` and `/rabbithole/web` endpoints.\n    Currently supported files are `.txt`, `.pdf`, `.md` and web pages.\n\n    \"\"\"\n\n    # Check type of incoming file.\n    if isinstance(file, UploadFile):\n        # Get mime type and source of UploadFile\n        content_type = mimetypes.guess_type(file.filename)[0]\n        source = file.filename\n\n        # Get file bytes\n        file_bytes = file.file.read()\n    elif isinstance(file, str):\n        # Check if string file is a string or url\n        parsed_file = urlparse(file)\n        is_url = all([parsed_file.scheme, parsed_file.netloc])\n\n        if is_url:\n            # Make a request with a fake browser name\n            request = httpx.get(file, headers={\"User-Agent\": \"Magic Browser\"})\n\n            # Define mime type and source of url\n            content_type = request.headers[\"Content-Type\"].split(\";\")[0]\n            source = file\n\n            try:\n                # Get binary content of url\n                file_bytes = request.content\n            except HTTPError as e:\n                log.error(e)\n        else:\n            # Get mime type from file extension and source\n            content_type = mimetypes.guess_type(file)[0]\n            source = os.path.basename(file)\n\n            # Get file bytes\n            with open(file, \"rb\") as f:\n                file_bytes = f.read()\n    else:\n        raise ValueError(f\"{type(file)} is not a valid type.\")\n    return self.string_to_docs(\n        cat=cat,\n        file_bytes=file_bytes,\n        source=source,\n        content_type=content_type,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n</code></pre>"},{"location":"API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.ingest_file","title":"<code>ingest_file(cat, file, chunk_size=None, chunk_overlap=None, metadata={})</code>","text":"<p>Load a file in the Cat's declarative memory.</p> <p>The method splits and converts the file in Langchain <code>Document</code>. Then, it stores the <code>Document</code> in the Cat's memory.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>(str, UploadFile)</code> <p>The file can be a path passed as a string or an <code>UploadFile</code> object if the document is ingested using the <code>rabbithole</code> endpoint.</p> required <code>chunk_size</code> <code>int</code> <p>Number of tokens in each document chunk.</p> <code>None</code> <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping tokens between consecutive chunks.</p> <code>None</code> <code>metadata</code> <code>dict</code> <p>Metadata to be stored with each chunk.</p> <code>{}</code> Notes <p>Currently supported formats are <code>.txt</code>, <code>.pdf</code> and <code>.md</code>. You cn add custom ones or substitute the above via RabbitHole hooks.</p> See Also <p>before_rabbithole_stores_documents</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def ingest_file(\n    self,\n    cat,\n    file: Union[str, UploadFile],\n    chunk_size: int | None = None,\n    chunk_overlap: int | None = None,\n    metadata: dict = {}\n):\n    \"\"\"Load a file in the Cat's declarative memory.\n\n    The method splits and converts the file in Langchain `Document`. Then, it stores the `Document` in the Cat's\n    memory.\n\n    Parameters\n    ----------\n    file : str, UploadFile\n        The file can be a path passed as a string or an `UploadFile` object if the document is ingested using the\n        `rabbithole` endpoint.\n    chunk_size : int\n        Number of tokens in each document chunk.\n    chunk_overlap : int\n        Number of overlapping tokens between consecutive chunks.\n    metadata : dict\n        Metadata to be stored with each chunk.\n\n    Notes\n    ----------\n    Currently supported formats are `.txt`, `.pdf` and `.md`.\n    You cn add custom ones or substitute the above via RabbitHole hooks.\n\n    See Also\n    ----------\n    before_rabbithole_stores_documents\n    \"\"\"\n\n    # split file into a list of docs\n    docs = self.file_to_docs(\n        cat=cat,\n        file=file,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n\n    # store in memory\n    if isinstance(file, str):\n        filename = file\n    else:\n        filename = file.filename\n\n    self.store_documents(cat=cat, docs=docs, source=filename, metadata=metadata)\n</code></pre>"},{"location":"API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.ingest_memory","title":"<code>ingest_memory(cat, file)</code>","text":"<p>Upload memories to the declarative memory from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>UploadFile</code> <p>File object sent via <code>rabbithole/memory</code> hook.</p> required Notes <p>This method allows uploading a JSON file containing vector and text memories directly to the declarative memory. When doing this, please, make sure the embedder used to export the memories is the same as the one used when uploading. The method also performs a check on the dimensionality of the embeddings (i.e. length of each vector).</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def ingest_memory(\n        self,\n        cat,\n        file: UploadFile\n    ):\n    \"\"\"Upload memories to the declarative memory from a JSON file.\n\n    Parameters\n    ----------\n    file : UploadFile\n        File object sent via `rabbithole/memory` hook.\n\n    Notes\n    -----\n    This method allows uploading a JSON file containing vector and text memories directly to the declarative memory.\n    When doing this, please, make sure the embedder used to export the memories is the same as the one used\n    when uploading.\n    The method also performs a check on the dimensionality of the embeddings (i.e. length of each vector).\n\n    \"\"\"\n\n    # Get file bytes\n    file_bytes = file.file.read()\n\n    # Load fyle byte in a dict\n    memories = json.loads(file_bytes.decode(\"utf-8\"))\n\n    # Check the embedder used for the uploaded memories is the same the Cat is using now\n    upload_embedder = memories[\"embedder\"]\n    cat_embedder = str(cat.embedder.__class__.__name__)\n\n    if upload_embedder != cat_embedder:\n        message = f\"Embedder mismatch: file embedder {upload_embedder} is different from {cat_embedder}\"\n        raise Exception(message)\n\n    # Get Declarative memories in file\n    declarative_memories = memories[\"collections\"][\"declarative\"]\n\n    # Store data to upload the memories in batch\n    ids = [i[\"id\"] for i in declarative_memories]\n    payloads = [\n        {\"page_content\": p[\"page_content\"], \"metadata\": p[\"metadata\"]}\n        for p in declarative_memories\n    ]\n    vectors = [v[\"vector\"] for v in declarative_memories]\n\n    log.info(f\"Preparing to load {len(vectors)} vector memories\")\n\n    # Check embedding size is correct\n    embedder_size = cat.memory.vectors.declarative.embedder_size\n    len_mismatch = [len(v) == embedder_size for v in vectors]\n\n    if not any(len_mismatch):\n        message = (\n            f\"Embedding size mismatch: vectors length should be {embedder_size}\"\n        )\n        raise Exception(message)\n\n    # Upsert memories in batch mode # TODO REFACTOR: use VectorMemoryCollection.add_point\n    cat.memory.vectors.vector_db.upsert(\n        collection_name=\"declarative\",\n        points=models.Batch(ids=ids, payloads=payloads, vectors=vectors),\n    )\n</code></pre>"},{"location":"API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.store_documents","title":"<code>store_documents(cat, docs, source, metadata={})</code>","text":"<p>Add documents to the Cat's declarative memory.</p> <p>This method loops a list of Langchain <code>Document</code> and adds some metadata. Namely, the source filename and the timestamp of insertion. Once done, the method notifies the client via Websocket connection.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code> to be inserted in the Cat's declarative memory.</p> required <code>source</code> <code>str</code> <p>Source name to be added as a metadata. It can be a file name or an URL.</p> required <code>metadata</code> <code>dict</code> <p>Metadata to be stored with each chunk.</p> <code>{}</code> Notes <p>At this point, it is possible to customize the Cat's behavior using the <code>before_rabbithole_insert_memory</code> hook to edit the memories before they are inserted in the vector database.</p> See Also <p>before_rabbithole_insert_memory</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def store_documents(\n        self,\n        cat,\n        docs: List[Document],\n        source: str, # TODOV2: is this necessary?\n        metadata: dict = {}\n    ) -&gt; None:\n    \"\"\"Add documents to the Cat's declarative memory.\n\n    This method loops a list of Langchain `Document` and adds some metadata. Namely, the source filename and the\n    timestamp of insertion. Once done, the method notifies the client via Websocket connection.\n\n    Parameters\n    ----------\n    docs : List[Document]\n        List of Langchain `Document` to be inserted in the Cat's declarative memory.\n    source : str\n        Source name to be added as a metadata. It can be a file name or an URL.\n    metadata : dict\n        Metadata to be stored with each chunk.\n\n    Notes\n    -------\n    At this point, it is possible to customize the Cat's behavior using the `before_rabbithole_insert_memory` hook\n    to edit the memories before they are inserted in the vector database.\n\n    See Also\n    --------\n    before_rabbithole_insert_memory\n    \"\"\"\n\n    log.info(f\"Preparing to memorize {len(docs)} vectors\")\n\n    # hook the docs before they are stored in the vector memory\n    docs = cat.mad_hatter.execute_hook(\n        \"before_rabbithole_stores_documents\", docs, cat=cat\n    )\n\n    # classic embed\n    time_last_notification = time.time()\n    time_interval = 10  # a notification every 10 secs\n    stored_points = []\n    for d, doc in enumerate(docs):\n        if time.time() - time_last_notification &gt; time_interval:\n            time_last_notification = time.time()\n            perc_read = int(d / len(docs) * 100)\n            read_message = f\"Read {perc_read}% of {source}\"\n            cat.send_ws_message(read_message)\n            log.info(read_message)\n\n        # add default metadata\n        doc.metadata[\"source\"] = source\n        doc.metadata[\"when\"] = time.time()\n        # add custom metadata (sent via endpoint)\n        for k,v in metadata.items():\n            doc.metadata[k] = v\n\n        doc = cat.mad_hatter.execute_hook(\n            \"before_rabbithole_insert_memory\", doc, cat=cat\n        )\n        inserting_info = f\"{d + 1}/{len(docs)}):    {doc.page_content}\"\n        if doc.page_content != \"\":\n            doc_embedding = cat.embedder.embed_documents([doc.page_content])\n            stored_point = cat.memory.vectors.declarative.add_point(\n                doc.page_content,\n                doc_embedding[0],\n                doc.metadata,\n            )\n            stored_points.append(stored_point)\n\n            log.info(f\"Inserted into memory ({inserting_info})\")\n        else:\n            log.info(f\"Skipped memory insertion of empty doc ({inserting_info})\")\n\n        # wait a little to avoid APIs rate limit errors\n        time.sleep(0.05)\n\n    # hook the points after they are stored in the vector memory\n    cat.mad_hatter.execute_hook(\n        \"after_rabbithole_stored_documents\", source, stored_points, cat=cat\n    )\n\n    # notify client\n    finished_reading_message = (\n        f\"Finished reading {source}, I made {len(docs)} thoughts on it.\"\n    )\n\n    cat.send_ws_message(finished_reading_message)\n\n    log.info(f\"Done uploading {source}\")\n</code></pre>"},{"location":"API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.string_to_docs","title":"<code>string_to_docs(cat, file_bytes, source=None, content_type='text/plain', chunk_size=None, chunk_overlap=None)</code>","text":"<p>Convert string to Langchain <code>Document</code>.</p> <p>Takes a string, converts it to langchain <code>Document</code>. Hence, loads it in memory and splits it in overlapped chunks of text.</p> <p>Parameters:</p> Name Type Description Default <code>file_bytes</code> <code>str</code> <p>The string to be converted.</p> required <code>source</code> <code>str</code> <p>Source filename.</p> <code>None</code> <code>content_type</code> <code>str</code> <p>Mimetype of content.</p> <code>'text/plain'</code> <code>chunk_size</code> <code>int</code> <p>Number of tokens in each document chunk.</p> <code>None</code> <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping tokens between consecutive chunks.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>docs</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code> of chunked text.</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def string_to_docs(\n    self,\n    cat,\n    file_bytes: str,\n    source: str = None,\n    content_type: str = \"text/plain\",\n    chunk_size: int | None = None,\n    chunk_overlap: int | None = None\n) -&gt; List[Document]:\n    \"\"\"Convert string to Langchain `Document`.\n\n    Takes a string, converts it to langchain `Document`.\n    Hence, loads it in memory and splits it in overlapped chunks of text.\n\n    Parameters\n    ----------\n    file_bytes : str\n        The string to be converted.\n    source: str\n        Source filename.\n    content_type:\n        Mimetype of content.\n    chunk_size : int\n        Number of tokens in each document chunk.\n    chunk_overlap : int\n        Number of overlapping tokens between consecutive chunks.\n\n    Returns\n    -------\n    docs : List[Document]\n        List of Langchain `Document` of chunked text.\n    \"\"\"\n\n    # Load the bytes in the Blob schema\n    blob = Blob(data=file_bytes, mimetype=content_type, source=source).from_data(\n        data=file_bytes, mime_type=content_type, path=source\n    )\n    # Parser based on the mime type\n    parser = MimeTypeBasedParser(handlers=self.file_handlers)\n\n    # Parse the text\n    cat.send_ws_message(\n        \"I'm parsing the content. Big content could require some minutes...\"\n    )\n    super_docs = parser.parse(blob)\n\n    # Split\n    cat.send_ws_message(\"Parsing completed. Now let's go with reading process...\")\n    docs = self.__split_text(\n        cat=cat,\n        text=super_docs,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n    )\n    return docs\n</code></pre>"},{"location":"API_Documentation/utils/","title":"utils","text":"<p>Various utiles used from the projects.</p>"},{"location":"API_Documentation/utils/#cat.utils.deprecation_warning","title":"<code>deprecation_warning(message, skip=3)</code>","text":"<p>Log a deprecation warning with caller's information. \"skip\" is the number of stack levels to go back to the caller info.</p> Source code in <code>cat/utils.py</code> <pre><code>def deprecation_warning(message: str, skip=3):\n    \"\"\"Log a deprecation warning with caller's information.\n        \"skip\" is the number of stack levels to go back to the caller info.\"\"\"\n\n    caller = get_caller_info(skip, return_short=False)\n\n    # Format and log the warning message\n    log.warning(\n        f\"{caller} Deprecation Warning: {message})\"\n    )\n</code></pre>"},{"location":"API_Documentation/utils/#cat.utils.get_base_path","title":"<code>get_base_path()</code>","text":"<p>Allows exposing the base path.</p> Source code in <code>cat/utils.py</code> <pre><code>def get_base_path():\n    \"\"\"Allows exposing the base path.\"\"\"\n    return \"cat/\"\n</code></pre>"},{"location":"API_Documentation/utils/#cat.utils.get_base_url","title":"<code>get_base_url()</code>","text":"<p>Allows exposing the base url.</p> Source code in <code>cat/utils.py</code> <pre><code>def get_base_url():\n    \"\"\"Allows exposing the base url.\"\"\"\n    secure = \"s\" if get_env(\"CCAT_CORE_USE_SECURE_PROTOCOLS\") in (\"true\", \"1\") else \"\"\n    cat_host = get_env(\"CCAT_CORE_HOST\")\n    cat_port = get_env(\"CCAT_CORE_PORT\")\n    return f\"http{secure}://{cat_host}:{cat_port}/\"\n</code></pre>"},{"location":"API_Documentation/utils/#cat.utils.get_caller_info","title":"<code>get_caller_info(skip=2, return_short=True, return_string=True)</code>","text":"<p>Get the name of a caller in the format module.class.method.</p> <p>Adapted from: https://gist.github.com/techtonik/2151727</p> <p>Parameters:</p> Name Type Description Default <code>skip</code> <code> int</code> <p>Specifies how many levels of stack to skip while getting caller name.</p> <code>2</code> <code>return_string</code> <code>bool</code> <p>If True, returns the caller info as a string, otherwise as a tuple.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>package</code> <code>str</code> <p>Caller package.</p> <code>module</code> <code>str</code> <p>Caller module.</p> <code>klass</code> <code>str</code> <p>Caller classname if one otherwise None.</p> <code>caller</code> <code>str</code> <p>Caller function or method (if a class exist).</p> <code>line</code> <code>int</code> <p>The line of the call.</p> Notes <p>skip=1 means \"who calls me\", skip=2 \"who calls my caller\" etc.</p> <p>None is returned if skipped levels exceed stack height.</p> Source code in <code>cat/utils.py</code> <pre><code>def get_caller_info(skip=2, return_short=True, return_string=True):\n    \"\"\"Get the name of a caller in the format module.class.method.\n\n    Adapted from: https://gist.github.com/techtonik/2151727\n\n    Parameters\n    ----------\n    skip :  int\n        Specifies how many levels of stack to skip while getting caller name.\n    return_string : bool\n        If True, returns the caller info as a string, otherwise as a tuple.\n\n    Returns\n    -------\n    package : str\n        Caller package.\n    module : str\n        Caller module.\n    klass : str\n        Caller classname if one otherwise None.\n    caller : str\n        Caller function or method (if a class exist).\n    line : int\n        The line of the call.\n\n\n    Notes\n    -----\n    skip=1 means \"who calls me\",\n    skip=2 \"who calls my caller\" etc.\n\n    None is returned if skipped levels exceed stack height.\n    \"\"\"\n\n    stack = inspect.stack()\n    start = 0 + skip\n    if len(stack) &lt; start + 1:\n        return None\n\n    parentframe = stack[start][0]\n\n    # module and packagename.\n    module_info = inspect.getmodule(parentframe)\n    if module_info:\n        mod = module_info.__name__.split(\".\")\n        package = mod[0]\n        module = \".\".join(mod[1:])\n\n    # class name.\n    klass = \"\"\n    if \"self\" in parentframe.f_locals:\n        klass = parentframe.f_locals[\"self\"].__class__.__name__\n\n    # method or function name.\n    caller = None\n    if parentframe.f_code.co_name != \"&lt;module&gt;\":  # top level usually\n        caller = parentframe.f_code.co_name\n\n    # call line.\n    line = parentframe.f_lineno\n\n    # Remove reference to frame\n    # See: https://docs.python.org/3/library/inspect.html#the-interpreter-stack\n    del parentframe\n\n    if return_string:\n        if return_short:\n            return f\"{klass}.{caller}\"\n        else:\n            return f\"{package}.{module}.{klass}.{caller}::{line}\"\n    return package, module, klass, caller, line\n</code></pre>"},{"location":"API_Documentation/utils/#cat.utils.get_plugins_path","title":"<code>get_plugins_path()</code>","text":"<p>Allows exposing the plugins' path.</p> Source code in <code>cat/utils.py</code> <pre><code>def get_plugins_path():\n    \"\"\"Allows exposing the plugins' path.\"\"\"\n    return os.path.join(get_base_path(), \"plugins/\")\n</code></pre>"},{"location":"API_Documentation/utils/#cat.utils.get_static_path","title":"<code>get_static_path()</code>","text":"<p>Allows exposing the static files' path.</p> Source code in <code>cat/utils.py</code> <pre><code>def get_static_path():\n    \"\"\"Allows exposing the static files' path.\"\"\"\n    return os.path.join(get_base_path(), \"static/\")\n</code></pre>"},{"location":"API_Documentation/utils/#cat.utils.get_static_url","title":"<code>get_static_url()</code>","text":"<p>Allows exposing the static server url.</p> Source code in <code>cat/utils.py</code> <pre><code>def get_static_url():\n    \"\"\"Allows exposing the static server url.\"\"\"\n    return get_base_url() + \"static/\"\n</code></pre>"},{"location":"API_Documentation/utils/#cat.utils.match_prompt_variables","title":"<code>match_prompt_variables(prompt_variables, prompt_template)</code>","text":"<p>Ensure prompt variables and prompt placeholders map, so there are no issues on mismatches</p> Source code in <code>cat/utils.py</code> <pre><code>def match_prompt_variables(\n        prompt_variables: Dict,\n        prompt_template: str\n    ) -&gt; Tuple[Dict, str]:\n    \"\"\"Ensure prompt variables and prompt placeholders map, so there are no issues on mismatches\"\"\"\n\n    tmp_prompt = PromptTemplate.from_template(\n        template=prompt_template\n    )\n\n    # outer set difference\n    prompt_mismatches = set(prompt_variables.keys()) ^ set(tmp_prompt.input_variables)\n\n    # clean up\n    for m in prompt_mismatches:\n        if m in prompt_variables.keys():\n            log.debug(f\"Prompt variable '{m}' not found in prompt template, removed\")\n            del prompt_variables[m]\n        if m in tmp_prompt.input_variables:\n            prompt_template = \\\n                prompt_template.replace(\"{\" + m + \"}\", \"\")\n            log.debug(f\"Placeholder '{m}' not found in prompt variables, removed\")\n\n    return prompt_variables, prompt_template\n</code></pre>"},{"location":"API_Documentation/utils/#cat.utils.to_camel_case","title":"<code>to_camel_case(text)</code>","text":"<p>Format string to camel case.</p> <p>Takes a string of words separated by either hyphens or underscores and returns a string of words in camel case.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>String of hyphens or underscores separated words.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Camel case formatted string.</p> Source code in <code>cat/utils.py</code> <pre><code>def to_camel_case(text: str) -&gt; str:\n    \"\"\"Format string to camel case.\n\n    Takes a string of words separated by either hyphens or underscores and returns a string of words in camel case.\n\n    Parameters\n    ----------\n    text : str\n        String of hyphens or underscores separated words.\n\n    Returns\n    -------\n    str\n        Camel case formatted string.\n    \"\"\"\n    s = text.replace(\"-\", \" \").replace(\"_\", \" \").capitalize()\n    s = s.split()\n    if len(text) == 0:\n        return text\n    return s[0] + \"\".join(i.capitalize() for i in s[1:])\n</code></pre>"},{"location":"API_Documentation/utils/#cat.utils.verbal_timedelta","title":"<code>verbal_timedelta(td)</code>","text":"<p>Convert a timedelta in human form.</p> <p>The function takes a timedelta and converts it to a human-readable string format.</p> <p>Parameters:</p> Name Type Description Default <code>td</code> <code>timedelta</code> <p>Difference between two dates.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Human-readable string of time difference.</p> Notes <p>This method is used to give the Language Model information time information about the memories retrieved from the vector database.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print(verbal_timedelta(timedelta(days=2, weeks=1))\n'One week and two days ago'\n</code></pre> Source code in <code>cat/utils.py</code> <pre><code>def verbal_timedelta(td: timedelta) -&gt; str:\n    \"\"\"Convert a timedelta in human form.\n\n    The function takes a timedelta and converts it to a human-readable string format.\n\n    Parameters\n    ----------\n    td : timedelta\n        Difference between two dates.\n\n    Returns\n    -------\n    str\n        Human-readable string of time difference.\n\n    Notes\n    -----\n    This method is used to give the Language Model information time information about the memories retrieved from\n    the vector database.\n\n    Examples\n    --------\n    &gt;&gt;&gt; print(verbal_timedelta(timedelta(days=2, weeks=1))\n    'One week and two days ago'\n    \"\"\"\n\n    if td.days != 0:\n        abs_days = abs(td.days)\n        if abs_days &gt; 7:\n            abs_delta = \"{} weeks\".format(td.days // 7)\n        else:\n            abs_delta = \"{} days\".format(td.days)\n    else:\n        abs_minutes = abs(td.seconds) // 60\n        if abs_minutes &gt; 60:\n            abs_delta = \"{} hours\".format(abs_minutes // 60)\n        else:\n            abs_delta = \"{} minutes\".format(abs_minutes)\n    if td &lt; timedelta(0):\n        return \"{} ago\".format(abs_delta)\n    else:\n        return \"{} ago\".format(abs_delta)\n</code></pre>"},{"location":"API_Documentation/agents/base_agent/","title":"base_agent","text":""},{"location":"API_Documentation/agents/form_agent/","title":"form_agent","text":""},{"location":"API_Documentation/agents/main_agent/","title":"main_agent","text":""},{"location":"API_Documentation/agents/main_agent/#cat.agents.main_agent.MainAgent","title":"<code>MainAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> <p>Main Agent. This class manages sub agents that in turn use the LLM.</p> Source code in <code>cat/agents/main_agent.py</code> <pre><code>class MainAgent(BaseAgent):\n    \"\"\"Main Agent.\n    This class manages sub agents that in turn use the LLM.\n    \"\"\"\n\n    def __init__(self):\n        self.mad_hatter = MadHatter()\n\n        if get_env(\"CCAT_LOG_LEVEL\") in [\"DEBUG\", \"INFO\"]:\n            self.verbose = True\n        else:\n            self.verbose = False\n\n    def execute(self, cat) -&gt; AgentOutput:\n        \"\"\"Execute the agents.\n\n        Returns\n        -------\n        agent_output : AgentOutput\n            Reply of the agent, instance of AgentOutput.\n        \"\"\"\n\n        # prepare input to be passed to the agent.\n        #   Info will be extracted from working memory\n        # Note: agent_input works both as a dict and as an object\n        agent_input : BaseModelDict = self.format_agent_input(cat)\n        agent_input = self.mad_hatter.execute_hook(\n            \"before_agent_starts\", agent_input, cat=cat\n        )\n\n        # store the agent input inside the working memory\n        cat.working_memory.agent_input = agent_input\n\n        # should we run the default agents?\n        agent_fast_reply = self.mad_hatter.execute_hook(\n            \"agent_fast_reply\", {}, cat=cat\n        )\n        if isinstance(agent_fast_reply, AgentOutput):\n            return agent_fast_reply\n        if isinstance(agent_fast_reply, dict) and \"output\" in agent_fast_reply:\n            return AgentOutput(**agent_fast_reply)\n\n        # obtain prompt parts from plugins\n        prompt_prefix = self.mad_hatter.execute_hook(\n            \"agent_prompt_prefix\", prompts.MAIN_PROMPT_PREFIX, cat=cat\n        )\n        prompt_suffix = self.mad_hatter.execute_hook(\n            \"agent_prompt_suffix\", prompts.MAIN_PROMPT_SUFFIX, cat=cat\n        )\n\n        # run tools and forms\n        procedures_agent = ProceduresAgent()\n        procedures_agent_out : AgentOutput = procedures_agent.execute(cat)\n        if procedures_agent_out.return_direct:\n            return procedures_agent_out\n\n        # we run memory agent if:\n        # - no procedures were recalled or selected or\n        # - procedures have all return_direct=False\n        memory_agent = MemoryAgent()\n        memory_agent_out : AgentOutput = memory_agent.execute(\n            # TODO: should all agents only receive StrayCat?\n            cat, prompt_prefix, prompt_suffix\n        )\n\n        memory_agent_out.intermediate_steps += procedures_agent_out.intermediate_steps\n\n        return memory_agent_out\n\n    def format_agent_input(self, cat):\n        \"\"\"Format the input for the Agent.\n\n        The method formats the strings of recalled memories and chat history that will be provided to the Langchain\n        Agent and inserted in the prompt.\n\n        Returns\n        -------\n        BaseModelDict\n            Formatted output to be parsed by the Agent executor. Works both as a dict and as an object.\n\n        Notes\n        -----\n        The context of memories and conversation history is properly formatted before being parsed by the and, hence,\n        information are inserted in the main prompt.\n        All the formatting pipeline is hookable and memories can be edited.\n\n        See Also\n        --------\n        agent_prompt_episodic_memories\n        agent_prompt_declarative_memories\n        agent_prompt_chat_history\n        \"\"\"\n\n        # format memories to be inserted in the prompt\n        episodic_memory_formatted_content = self.agent_prompt_episodic_memories(\n            cat.working_memory.episodic_memories\n        )\n        declarative_memory_formatted_content = self.agent_prompt_declarative_memories(\n            cat.working_memory.declarative_memories\n        )\n\n        # format conversation history to be inserted in the prompt\n        # TODOV2: take away\n        conversation_history_formatted_content = cat.working_memory.stringify_chat_history()\n\n        return BaseModelDict(**{\n            \"episodic_memory\": episodic_memory_formatted_content,\n            \"declarative_memory\": declarative_memory_formatted_content,\n            \"tools_output\": \"\",\n            \"input\": cat.working_memory.user_message_json.text,  # TODOV2: take away\n            \"chat_history\": conversation_history_formatted_content, # TODOV2: take away\n        })\n\n    def agent_prompt_episodic_memories(\n        self, memory_docs: List[Tuple[Document, float]]\n    ) -&gt; str:\n        \"\"\"Formats episodic memories to be inserted into the prompt.\n\n        Parameters\n        ----------\n        memory_docs : List[Document]\n            List of Langchain `Document` retrieved from the episodic memory.\n\n        Returns\n        -------\n        memory_content : str\n            String of retrieved context from the episodic memory.\n        \"\"\"\n\n        # convert docs to simple text\n        memory_texts = [m[0].page_content.replace(\"\\n\", \". \") for m in memory_docs]\n\n        # add time information (e.g. \"2 days ago\")\n        memory_timestamps = []\n        for m in memory_docs:\n            # Get Time information in the Document metadata\n            timestamp = m[0].metadata[\"when\"]\n\n            # Get Current Time - Time when memory was stored\n            delta = timedelta(seconds=(time.time() - timestamp))\n\n            # Convert and Save timestamps to Verbal (e.g. \"2 days ago\")\n            memory_timestamps.append(f\" ({verbal_timedelta(delta)})\")\n\n        # Join Document text content with related temporal information\n        memory_texts = [a + b for a, b in zip(memory_texts, memory_timestamps)]\n\n        # Format the memories for the output\n        memories_separator = \"\\n  - \"\n        memory_content = (\n            \"## Context of things the Human said in the past: \"\n            + memories_separator\n            + memories_separator.join(memory_texts)\n        )\n\n        # if no data is retrieved from memory don't erite anithing in the prompt\n        if len(memory_texts) == 0:\n            memory_content = \"\"\n\n        return memory_content\n\n    def agent_prompt_declarative_memories(\n        self, memory_docs: List[Tuple[Document, float]]\n    ) -&gt; str:\n        \"\"\"Formats the declarative memories for the prompt context.\n        Such context is placed in the `agent_prompt_prefix` in the place held by {declarative_memory}.\n\n        Parameters\n        ----------\n        memory_docs : List[Document]\n            list of Langchain `Document` retrieved from the declarative memory.\n\n        Returns\n        -------\n        memory_content : str\n            String of retrieved context from the declarative memory.\n        \"\"\"\n\n        # convert docs to simple text\n        memory_texts = [m[0].page_content.replace(\"\\n\", \". \") for m in memory_docs]\n\n        # add source information (e.g. \"extracted from file.txt\")\n        memory_sources = []\n        for m in memory_docs:\n            # Get and save the source of the memory\n            source = m[0].metadata[\"source\"]\n            memory_sources.append(f\" (extracted from {source})\")\n\n        # Join Document text content with related source information\n        memory_texts = [a + b for a, b in zip(memory_texts, memory_sources)]\n\n        # Format the memories for the output\n        memories_separator = \"\\n  - \"\n\n        memory_content = (\n            \"## Context of documents containing relevant information: \"\n            + memories_separator\n            + memories_separator.join(memory_texts)\n        )\n\n        # if no data is retrieved from memory don't write anithing in the prompt\n        if len(memory_texts) == 0:\n            memory_content = \"\"\n\n        return memory_content\n</code></pre>"},{"location":"API_Documentation/agents/main_agent/#cat.agents.main_agent.MainAgent.agent_prompt_declarative_memories","title":"<code>agent_prompt_declarative_memories(memory_docs)</code>","text":"<p>Formats the declarative memories for the prompt context. Such context is placed in the <code>agent_prompt_prefix</code> in the place held by {declarative_memory}.</p> <p>Parameters:</p> Name Type Description Default <code>memory_docs</code> <code>List[Document]</code> <p>list of Langchain <code>Document</code> retrieved from the declarative memory.</p> required <p>Returns:</p> Name Type Description <code>memory_content</code> <code>str</code> <p>String of retrieved context from the declarative memory.</p> Source code in <code>cat/agents/main_agent.py</code> <pre><code>def agent_prompt_declarative_memories(\n    self, memory_docs: List[Tuple[Document, float]]\n) -&gt; str:\n    \"\"\"Formats the declarative memories for the prompt context.\n    Such context is placed in the `agent_prompt_prefix` in the place held by {declarative_memory}.\n\n    Parameters\n    ----------\n    memory_docs : List[Document]\n        list of Langchain `Document` retrieved from the declarative memory.\n\n    Returns\n    -------\n    memory_content : str\n        String of retrieved context from the declarative memory.\n    \"\"\"\n\n    # convert docs to simple text\n    memory_texts = [m[0].page_content.replace(\"\\n\", \". \") for m in memory_docs]\n\n    # add source information (e.g. \"extracted from file.txt\")\n    memory_sources = []\n    for m in memory_docs:\n        # Get and save the source of the memory\n        source = m[0].metadata[\"source\"]\n        memory_sources.append(f\" (extracted from {source})\")\n\n    # Join Document text content with related source information\n    memory_texts = [a + b for a, b in zip(memory_texts, memory_sources)]\n\n    # Format the memories for the output\n    memories_separator = \"\\n  - \"\n\n    memory_content = (\n        \"## Context of documents containing relevant information: \"\n        + memories_separator\n        + memories_separator.join(memory_texts)\n    )\n\n    # if no data is retrieved from memory don't write anithing in the prompt\n    if len(memory_texts) == 0:\n        memory_content = \"\"\n\n    return memory_content\n</code></pre>"},{"location":"API_Documentation/agents/main_agent/#cat.agents.main_agent.MainAgent.agent_prompt_episodic_memories","title":"<code>agent_prompt_episodic_memories(memory_docs)</code>","text":"<p>Formats episodic memories to be inserted into the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>memory_docs</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code> retrieved from the episodic memory.</p> required <p>Returns:</p> Name Type Description <code>memory_content</code> <code>str</code> <p>String of retrieved context from the episodic memory.</p> Source code in <code>cat/agents/main_agent.py</code> <pre><code>def agent_prompt_episodic_memories(\n    self, memory_docs: List[Tuple[Document, float]]\n) -&gt; str:\n    \"\"\"Formats episodic memories to be inserted into the prompt.\n\n    Parameters\n    ----------\n    memory_docs : List[Document]\n        List of Langchain `Document` retrieved from the episodic memory.\n\n    Returns\n    -------\n    memory_content : str\n        String of retrieved context from the episodic memory.\n    \"\"\"\n\n    # convert docs to simple text\n    memory_texts = [m[0].page_content.replace(\"\\n\", \". \") for m in memory_docs]\n\n    # add time information (e.g. \"2 days ago\")\n    memory_timestamps = []\n    for m in memory_docs:\n        # Get Time information in the Document metadata\n        timestamp = m[0].metadata[\"when\"]\n\n        # Get Current Time - Time when memory was stored\n        delta = timedelta(seconds=(time.time() - timestamp))\n\n        # Convert and Save timestamps to Verbal (e.g. \"2 days ago\")\n        memory_timestamps.append(f\" ({verbal_timedelta(delta)})\")\n\n    # Join Document text content with related temporal information\n    memory_texts = [a + b for a, b in zip(memory_texts, memory_timestamps)]\n\n    # Format the memories for the output\n    memories_separator = \"\\n  - \"\n    memory_content = (\n        \"## Context of things the Human said in the past: \"\n        + memories_separator\n        + memories_separator.join(memory_texts)\n    )\n\n    # if no data is retrieved from memory don't erite anithing in the prompt\n    if len(memory_texts) == 0:\n        memory_content = \"\"\n\n    return memory_content\n</code></pre>"},{"location":"API_Documentation/agents/main_agent/#cat.agents.main_agent.MainAgent.execute","title":"<code>execute(cat)</code>","text":"<p>Execute the agents.</p> <p>Returns:</p> Name Type Description <code>agent_output</code> <code>AgentOutput</code> <p>Reply of the agent, instance of AgentOutput.</p> Source code in <code>cat/agents/main_agent.py</code> <pre><code>def execute(self, cat) -&gt; AgentOutput:\n    \"\"\"Execute the agents.\n\n    Returns\n    -------\n    agent_output : AgentOutput\n        Reply of the agent, instance of AgentOutput.\n    \"\"\"\n\n    # prepare input to be passed to the agent.\n    #   Info will be extracted from working memory\n    # Note: agent_input works both as a dict and as an object\n    agent_input : BaseModelDict = self.format_agent_input(cat)\n    agent_input = self.mad_hatter.execute_hook(\n        \"before_agent_starts\", agent_input, cat=cat\n    )\n\n    # store the agent input inside the working memory\n    cat.working_memory.agent_input = agent_input\n\n    # should we run the default agents?\n    agent_fast_reply = self.mad_hatter.execute_hook(\n        \"agent_fast_reply\", {}, cat=cat\n    )\n    if isinstance(agent_fast_reply, AgentOutput):\n        return agent_fast_reply\n    if isinstance(agent_fast_reply, dict) and \"output\" in agent_fast_reply:\n        return AgentOutput(**agent_fast_reply)\n\n    # obtain prompt parts from plugins\n    prompt_prefix = self.mad_hatter.execute_hook(\n        \"agent_prompt_prefix\", prompts.MAIN_PROMPT_PREFIX, cat=cat\n    )\n    prompt_suffix = self.mad_hatter.execute_hook(\n        \"agent_prompt_suffix\", prompts.MAIN_PROMPT_SUFFIX, cat=cat\n    )\n\n    # run tools and forms\n    procedures_agent = ProceduresAgent()\n    procedures_agent_out : AgentOutput = procedures_agent.execute(cat)\n    if procedures_agent_out.return_direct:\n        return procedures_agent_out\n\n    # we run memory agent if:\n    # - no procedures were recalled or selected or\n    # - procedures have all return_direct=False\n    memory_agent = MemoryAgent()\n    memory_agent_out : AgentOutput = memory_agent.execute(\n        # TODO: should all agents only receive StrayCat?\n        cat, prompt_prefix, prompt_suffix\n    )\n\n    memory_agent_out.intermediate_steps += procedures_agent_out.intermediate_steps\n\n    return memory_agent_out\n</code></pre>"},{"location":"API_Documentation/agents/main_agent/#cat.agents.main_agent.MainAgent.format_agent_input","title":"<code>format_agent_input(cat)</code>","text":"<p>Format the input for the Agent.</p> <p>The method formats the strings of recalled memories and chat history that will be provided to the Langchain Agent and inserted in the prompt.</p> <p>Returns:</p> Type Description <code>BaseModelDict</code> <p>Formatted output to be parsed by the Agent executor. Works both as a dict and as an object.</p> Notes <p>The context of memories and conversation history is properly formatted before being parsed by the and, hence, information are inserted in the main prompt. All the formatting pipeline is hookable and memories can be edited.</p> See Also <p>agent_prompt_episodic_memories agent_prompt_declarative_memories agent_prompt_chat_history</p> Source code in <code>cat/agents/main_agent.py</code> <pre><code>def format_agent_input(self, cat):\n    \"\"\"Format the input for the Agent.\n\n    The method formats the strings of recalled memories and chat history that will be provided to the Langchain\n    Agent and inserted in the prompt.\n\n    Returns\n    -------\n    BaseModelDict\n        Formatted output to be parsed by the Agent executor. Works both as a dict and as an object.\n\n    Notes\n    -----\n    The context of memories and conversation history is properly formatted before being parsed by the and, hence,\n    information are inserted in the main prompt.\n    All the formatting pipeline is hookable and memories can be edited.\n\n    See Also\n    --------\n    agent_prompt_episodic_memories\n    agent_prompt_declarative_memories\n    agent_prompt_chat_history\n    \"\"\"\n\n    # format memories to be inserted in the prompt\n    episodic_memory_formatted_content = self.agent_prompt_episodic_memories(\n        cat.working_memory.episodic_memories\n    )\n    declarative_memory_formatted_content = self.agent_prompt_declarative_memories(\n        cat.working_memory.declarative_memories\n    )\n\n    # format conversation history to be inserted in the prompt\n    # TODOV2: take away\n    conversation_history_formatted_content = cat.working_memory.stringify_chat_history()\n\n    return BaseModelDict(**{\n        \"episodic_memory\": episodic_memory_formatted_content,\n        \"declarative_memory\": declarative_memory_formatted_content,\n        \"tools_output\": \"\",\n        \"input\": cat.working_memory.user_message_json.text,  # TODOV2: take away\n        \"chat_history\": conversation_history_formatted_content, # TODOV2: take away\n    })\n</code></pre>"},{"location":"API_Documentation/agents/memory_agent/","title":"memory_agent","text":""},{"location":"API_Documentation/agents/procedures_agent/","title":"procedures_agent","text":""},{"location":"API_Documentation/auth/auth_utils/","title":"auth_utils","text":""},{"location":"API_Documentation/auth/auth_utils/#cat.auth.auth_utils.is_jwt","title":"<code>is_jwt(token)</code>","text":"<p>Returns whether a given string is a JWT.</p> Source code in <code>cat/auth/auth_utils.py</code> <pre><code>def is_jwt(token: str) -&gt; bool:\n    \"\"\"\n    Returns whether a given string is a JWT.\n    \"\"\"\n    try:\n        # Decode the JWT without verification to check its structure\n        jwt.decode(token, options={\"verify_signature\": False})\n        return True\n    except InvalidTokenError:\n        return False\n</code></pre>"},{"location":"API_Documentation/looking_glass/cheshire_cat/","title":"cheshire_cat","text":""},{"location":"API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat","title":"<code>CheshireCat</code>","text":"<p>The Cheshire Cat.</p> <p>This is the main class that manages the whole AI application. It contains references to all the main modules and is responsible for the bootstrapping of the application.</p> <p>In most cases you will not need to interact with this class directly, but rather with class <code>StrayCat</code> which will be available in your plugin's hooks, tools, forms end endpoints.</p> <p>Attributes:</p> Name Type Description <code>todo</code> <code>list</code> <p>Yet to be written.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>@singleton\nclass CheshireCat:\n    \"\"\"The Cheshire Cat.\n\n    This is the main class that manages the whole AI application.\n    It contains references to all the main modules and is responsible for the bootstrapping of the application.\n\n    In most cases you will not need to interact with this class directly, but rather with class `StrayCat` which will be available in your plugin's hooks, tools, forms end endpoints.\n\n    Attributes\n    ----------\n    todo : list\n        Yet to be written.\n\n    \"\"\"\n\n    def __init__(self, fastapi_app):\n        \"\"\"Cat initialization.\n\n        At init time the Cat executes the bootstrap.\n        \"\"\"\n\n        # bootstrap the Cat! ^._.^\n\n        # get reference to the FastAPI app\n        self.fastapi_app = fastapi_app\n\n        # load AuthHandler\n        self.load_auth()\n\n        # Start scheduling system\n        self.white_rabbit = WhiteRabbit()\n\n        # instantiate MadHatter (loads all plugins' hooks and tools)\n        self.mad_hatter = MadHatter()\n\n        # allows plugins to do something before cat components are loaded\n        self.mad_hatter.execute_hook(\"before_cat_bootstrap\", cat=self)\n\n        # load LLM and embedder\n        self.load_natural_language()\n\n        # Load memories (vector collections and working_memory)\n        self.load_memory()\n\n        # After memory is loaded, we can get/create tools embeddings      \n        self.mad_hatter.on_finish_plugins_sync_callback = self.on_finish_plugins_sync_callback\n\n        # First time launched manually       \n        self.on_finish_plugins_sync_callback()\n\n        # Main agent instance (for reasoning)\n        self.main_agent = MainAgent()\n\n        # Rabbit Hole Instance\n        self.rabbit_hole = RabbitHole(self)  # :(\n\n        # Cache for sessions / working memories et al.\n        self.cache = CacheManager().cache\n\n        # allows plugins to do something after the cat bootstrap is complete\n        self.mad_hatter.execute_hook(\"after_cat_bootstrap\", cat=self)\n\n    def load_natural_language(self):\n        \"\"\"Load Natural Language related objects.\n\n        The method exposes in the Cat all the NLP related stuff. Specifically, it sets the language models\n        (LLM and Embedder).\n\n        Warnings\n        --------\n        When using small Language Models it is suggested to turn off the memories and make the main prompt smaller\n        to prevent them to fail.\n\n        See Also\n        --------\n        agent_prompt_prefix\n        \"\"\"\n        # LLM and embedder\n        self._llm = self.load_language_model()\n        self.embedder = self.load_language_embedder()\n\n    def load_language_model(self) -&gt; BaseLanguageModel:\n        \"\"\"Large Language Model (LLM) selection at bootstrap time.\n\n        Returns\n        -------\n        llm : BaseLanguageModel\n            Langchain `BaseLanguageModel` instance of the selected model.\n\n        Notes\n        -----\n        Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories,\n        the *Main Agent*, the *Rabbit Hole* and the *White Rabbit*.\n        \"\"\"\n\n        selected_llm = crud.get_setting_by_name(name=\"llm_selected\")\n\n        if selected_llm is None:\n            # Return default LLM\n            return LLMDefaultConfig.get_llm_from_config({})\n\n        # Get LLM factory class\n        selected_llm_class = selected_llm[\"value\"][\"name\"]\n        FactoryClass = get_llm_from_name(selected_llm_class)\n\n        # Obtain configuration and instantiate LLM\n        selected_llm_config = crud.get_setting_by_name(name=selected_llm_class)\n        try:\n            llm = FactoryClass.get_llm_from_config(selected_llm_config[\"value\"])\n            return llm\n        except Exception:\n            log.error(\"Error during LLM instantiation\")\n            return LLMDefaultConfig.get_llm_from_config({})\n\n    def load_language_embedder(self) -&gt; embedders.EmbedderSettings:\n        \"\"\"Hook into the  embedder selection.\n\n        Allows to modify how the Cat selects the embedder at bootstrap time.\n\n        Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories,\n        the *Main Agent*, the *Rabbit Hole* and the *White Rabbit*.\n\n        Parameters\n        ----------\n        cat: CheshireCat\n            Cheshire Cat instance.\n\n        Returns\n        -------\n        embedder : Embeddings\n            Selected embedder model.\n        \"\"\"\n        # Embedding LLM\n\n        selected_embedder = crud.get_setting_by_name(name=\"embedder_selected\")\n\n        if selected_embedder is not None:\n            # get Embedder factory class\n            selected_embedder_class = selected_embedder[\"value\"][\"name\"]\n            FactoryClass = get_embedder_from_name(selected_embedder_class)\n\n            # obtain configuration and instantiate Embedder\n            selected_embedder_config = crud.get_setting_by_name(\n                name=selected_embedder_class\n            )\n            try:\n                embedder = FactoryClass.get_embedder_from_config(\n                    selected_embedder_config[\"value\"]\n                )\n            except Exception:\n                log.error(\"Error during Embedder instantiation\")\n                embedder = embedders.EmbedderDumbConfig.get_embedder_from_config({})\n            return embedder\n\n        # OpenAI embedder\n        if type(self._llm) in [OpenAI, ChatOpenAI]:\n            embedder = embedders.EmbedderOpenAIConfig.get_embedder_from_config(\n                {\n                    \"openai_api_key\": self._llm.openai_api_key,\n                }\n            )\n\n        # For Azure avoid automatic embedder selection\n\n        # Cohere\n        elif type(self._llm) in [ChatCohere]:\n            embedder = embedders.EmbedderCohereConfig.get_embedder_from_config(\n                {\n                    \"cohere_api_key\": self._llm.cohere_api_key,\n                    \"model\": \"embed-multilingual-v2.0\",\n                    # Now the best model for embeddings is embed-multilingual-v2.0\n                }\n            )\n\n        elif type(self._llm) in [ChatGoogleGenerativeAI]:\n            embedder = embedders.EmbedderGeminiChatConfig.get_embedder_from_config(\n                {\n                    \"model\": \"models/embedding-001\",\n                    \"google_api_key\": self._llm.google_api_key,\n                }\n            )\n\n        else:\n            # If no embedder matches vendor, and no external embedder is configured, we use the DumbEmbedder.\n            #   `This embedder is not a model properly trained\n            #    and this makes it not suitable to effectively embed text,\n            #    \"but it does not know this and embeds anyway\".` - cit. Nicola Corbellini\n            embedder = embedders.EmbedderDumbConfig.get_embedder_from_config({})\n\n        return embedder\n\n    def load_auth(self):\n\n        # Custom auth_handler # TODOAUTH: change the name to custom_auth\n        selected_auth_handler = crud.get_setting_by_name(name=\"auth_handler_selected\")\n\n        # if no auth_handler is saved, use default one and save to db\n        if selected_auth_handler is None:\n            # create the auth settings\n            crud.upsert_setting_by_name(\n                models.Setting(\n                    name=\"CoreOnlyAuthConfig\", category=\"auth_handler_factory\", value={}\n                )\n            )\n            crud.upsert_setting_by_name(\n                models.Setting(\n                    name=\"auth_handler_selected\",\n                    category=\"auth_handler_factory\",\n                    value={\"name\": \"CoreOnlyAuthConfig\"},\n                )\n            )\n\n            # reload from db\n            selected_auth_handler = crud.get_setting_by_name(\n                name=\"auth_handler_selected\"\n            )\n\n        # get AuthHandler factory class\n        selected_auth_handler_class = selected_auth_handler[\"value\"][\"name\"]\n        FactoryClass = get_auth_handler_from_name(selected_auth_handler_class)\n\n        # obtain configuration and instantiate AuthHandler\n        selected_auth_handler_config = crud.get_setting_by_name(\n            name=selected_auth_handler_class\n        )\n        try:\n            auth_handler = FactoryClass.get_auth_handler_from_config(\n                selected_auth_handler_config[\"value\"]\n            )\n        except Exception:\n            log.error(\"Error during AuthHandler instantiation\")\n\n            auth_handler = (\n                auth_handlers.CoreOnlyAuthConfig.get_auth_handler_from_config({})\n            )\n\n        self.custom_auth_handler = auth_handler\n        self.core_auth_handler = CoreAuthHandler()\n\n    def load_memory(self):\n        \"\"\"Load LongTerMemory and WorkingMemory.\"\"\"\n        # Memory\n\n        # Get embedder size (langchain classes do not store it)\n        embedder_size = len(self.embedder.embed_query(\"hello world\"))\n\n        # Get embedder name (useful for for vectorstore aliases)\n        if hasattr(self.embedder, \"model\"):\n            embedder_name = self.embedder.model\n        elif hasattr(self.embedder, \"repo_id\"):\n            embedder_name = self.embedder.repo_id\n        else:\n            embedder_name = \"default_embedder\"\n\n        # instantiate long term memory\n        vector_memory_config = {\n            \"embedder_name\": embedder_name,\n            \"embedder_size\": embedder_size,\n        }\n        self.memory = LongTermMemory(vector_memory_config=vector_memory_config)\n\n    def build_embedded_procedures_hashes(self, embedded_procedures):\n        hashes = {}\n        for ep in embedded_procedures:\n            metadata = ep.payload[\"metadata\"]\n            content = ep.payload[\"page_content\"]\n            source = metadata[\"source\"]\n            # there may be legacy points with no trigger_type\n            trigger_type = metadata.get(\"trigger_type\", \"unsupported\")\n\n            p_hash = f\"{source}.{trigger_type}.{content}\"\n            hashes[p_hash] = ep.id\n\n        return hashes\n\n    def build_active_procedures_hashes(self, active_procedures):\n        hashes = {}\n        for ap in active_procedures:\n            for trigger_type, trigger_list in ap.triggers_map.items():\n                for trigger_content in trigger_list:\n                    p_hash = f\"{ap.name}.{trigger_type}.{trigger_content}\"\n                    hashes[p_hash] = {\n                        \"obj\": ap,\n                        \"source\": ap.name,\n                        \"type\": ap.procedure_type,\n                        \"trigger_type\": trigger_type,\n                        \"content\": trigger_content,\n                    }\n        return hashes\n\n    def on_finish_plugins_sync_callback(self):\n        self.activate_endpoints()\n        self.embed_procedures()\n\n    def activate_endpoints(self):\n        for endpoint in self.mad_hatter.endpoints:\n            if endpoint.plugin_id in self.mad_hatter.active_plugins:\n                endpoint.activate(self.fastapi_app)\n\n    def embed_procedures(self):\n        # Retrieve from vectorDB all procedural embeddings\n        embedded_procedures, _ = self.memory.vectors.procedural.get_all_points()\n        embedded_procedures_hashes = self.build_embedded_procedures_hashes(\n            embedded_procedures\n        )\n\n        # Easy access to active procedures in mad_hatter (source of truth!)\n        active_procedures_hashes = self.build_active_procedures_hashes(\n            self.mad_hatter.procedures\n        )\n\n        # points_to_be_kept     = set(active_procedures_hashes.keys()) and set(embedded_procedures_hashes.keys()) not necessary\n        points_to_be_deleted = set(embedded_procedures_hashes.keys()) - set(\n            active_procedures_hashes.keys()\n        )\n        points_to_be_embedded = set(active_procedures_hashes.keys()) - set(\n            embedded_procedures_hashes.keys()\n        )\n\n        points_to_be_deleted_ids = [\n            embedded_procedures_hashes[p] for p in points_to_be_deleted\n        ]\n        if points_to_be_deleted_ids:\n            log.info(\"Deleting procedural triggers:\")\n            log.info(points_to_be_deleted)\n            self.memory.vectors.procedural.delete_points(points_to_be_deleted_ids)\n\n        active_triggers_to_be_embedded = [\n            active_procedures_hashes[p] for p in points_to_be_embedded\n        ]\n\n        if active_triggers_to_be_embedded:\n            log.info(\"Embedding new procedural triggers:\")\n        for t in active_triggers_to_be_embedded:\n\n\n            metadata = {\n                \"source\": t[\"source\"],\n                \"type\": t[\"type\"],\n                \"trigger_type\": t[\"trigger_type\"],\n                \"when\": time.time(),\n            }\n\n            trigger_embedding = self.embedder.embed_documents([t[\"content\"]])\n            self.memory.vectors.procedural.add_point(\n                t[\"content\"],\n                trigger_embedding[0],\n                metadata,\n            )\n\n            log.info(\n                f\" {t['source']}.{t['trigger_type']}.{t['content']}\"\n            )\n\n    def send_ws_message(self, content: str, msg_type=\"notification\"):\n        log.error(\"CheshireCat has no websocket connection. Call `send_ws_message` from a StrayCat instance.\")\n\n    # REFACTOR: cat.llm should be available here, without streaming clearly\n    # (one could be interested in calling the LLM anytime, not only when there is a session)\n    def llm(self, prompt, *args, **kwargs) -&gt; str:\n        \"\"\"Generate a response using the LLM model.\n\n        This method is useful for generating a response with both a chat and a completion model using the same syntax\n\n        Parameters\n        ----------\n        prompt : str\n            The prompt for generating the response.\n\n        Returns\n        -------\n        str\n            The generated response.\n\n        \"\"\"\n\n        # Add a token counter to the callbacks\n        caller = utils.get_caller_info()\n\n        # here we deal with motherfucking langchain\n        prompt = ChatPromptTemplate(\n            messages=[\n                HumanMessage(content=prompt)\n            ]\n        )\n\n        chain = (\n            prompt\n            | RunnableLambda(lambda x: utils.langchain_log_prompt(x, f\"{caller} prompt\"))\n            | self._llm\n            | RunnableLambda(lambda x: utils.langchain_log_output(x, f\"{caller} prompt output\"))\n            | StrOutputParser()\n        )\n\n        output = chain.invoke(\n            {}, # in case we need to pass info to the template\n        )\n\n        return output\n</code></pre>"},{"location":"API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.__init__","title":"<code>__init__(fastapi_app)</code>","text":"<p>Cat initialization.</p> <p>At init time the Cat executes the bootstrap.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def __init__(self, fastapi_app):\n    \"\"\"Cat initialization.\n\n    At init time the Cat executes the bootstrap.\n    \"\"\"\n\n    # bootstrap the Cat! ^._.^\n\n    # get reference to the FastAPI app\n    self.fastapi_app = fastapi_app\n\n    # load AuthHandler\n    self.load_auth()\n\n    # Start scheduling system\n    self.white_rabbit = WhiteRabbit()\n\n    # instantiate MadHatter (loads all plugins' hooks and tools)\n    self.mad_hatter = MadHatter()\n\n    # allows plugins to do something before cat components are loaded\n    self.mad_hatter.execute_hook(\"before_cat_bootstrap\", cat=self)\n\n    # load LLM and embedder\n    self.load_natural_language()\n\n    # Load memories (vector collections and working_memory)\n    self.load_memory()\n\n    # After memory is loaded, we can get/create tools embeddings      \n    self.mad_hatter.on_finish_plugins_sync_callback = self.on_finish_plugins_sync_callback\n\n    # First time launched manually       \n    self.on_finish_plugins_sync_callback()\n\n    # Main agent instance (for reasoning)\n    self.main_agent = MainAgent()\n\n    # Rabbit Hole Instance\n    self.rabbit_hole = RabbitHole(self)  # :(\n\n    # Cache for sessions / working memories et al.\n    self.cache = CacheManager().cache\n\n    # allows plugins to do something after the cat bootstrap is complete\n    self.mad_hatter.execute_hook(\"after_cat_bootstrap\", cat=self)\n</code></pre>"},{"location":"API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.llm","title":"<code>llm(prompt, *args, **kwargs)</code>","text":"<p>Generate a response using the LLM model.</p> <p>This method is useful for generating a response with both a chat and a completion model using the same syntax</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt for generating the response.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generated response.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def llm(self, prompt, *args, **kwargs) -&gt; str:\n    \"\"\"Generate a response using the LLM model.\n\n    This method is useful for generating a response with both a chat and a completion model using the same syntax\n\n    Parameters\n    ----------\n    prompt : str\n        The prompt for generating the response.\n\n    Returns\n    -------\n    str\n        The generated response.\n\n    \"\"\"\n\n    # Add a token counter to the callbacks\n    caller = utils.get_caller_info()\n\n    # here we deal with motherfucking langchain\n    prompt = ChatPromptTemplate(\n        messages=[\n            HumanMessage(content=prompt)\n        ]\n    )\n\n    chain = (\n        prompt\n        | RunnableLambda(lambda x: utils.langchain_log_prompt(x, f\"{caller} prompt\"))\n        | self._llm\n        | RunnableLambda(lambda x: utils.langchain_log_output(x, f\"{caller} prompt output\"))\n        | StrOutputParser()\n    )\n\n    output = chain.invoke(\n        {}, # in case we need to pass info to the template\n    )\n\n    return output\n</code></pre>"},{"location":"API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.load_language_embedder","title":"<code>load_language_embedder()</code>","text":"<p>Hook into the  embedder selection.</p> <p>Allows to modify how the Cat selects the embedder at bootstrap time.</p> <p>Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories, the Main Agent, the Rabbit Hole and the White Rabbit.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>embedder</code> <code>Embeddings</code> <p>Selected embedder model.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def load_language_embedder(self) -&gt; embedders.EmbedderSettings:\n    \"\"\"Hook into the  embedder selection.\n\n    Allows to modify how the Cat selects the embedder at bootstrap time.\n\n    Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories,\n    the *Main Agent*, the *Rabbit Hole* and the *White Rabbit*.\n\n    Parameters\n    ----------\n    cat: CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    embedder : Embeddings\n        Selected embedder model.\n    \"\"\"\n    # Embedding LLM\n\n    selected_embedder = crud.get_setting_by_name(name=\"embedder_selected\")\n\n    if selected_embedder is not None:\n        # get Embedder factory class\n        selected_embedder_class = selected_embedder[\"value\"][\"name\"]\n        FactoryClass = get_embedder_from_name(selected_embedder_class)\n\n        # obtain configuration and instantiate Embedder\n        selected_embedder_config = crud.get_setting_by_name(\n            name=selected_embedder_class\n        )\n        try:\n            embedder = FactoryClass.get_embedder_from_config(\n                selected_embedder_config[\"value\"]\n            )\n        except Exception:\n            log.error(\"Error during Embedder instantiation\")\n            embedder = embedders.EmbedderDumbConfig.get_embedder_from_config({})\n        return embedder\n\n    # OpenAI embedder\n    if type(self._llm) in [OpenAI, ChatOpenAI]:\n        embedder = embedders.EmbedderOpenAIConfig.get_embedder_from_config(\n            {\n                \"openai_api_key\": self._llm.openai_api_key,\n            }\n        )\n\n    # For Azure avoid automatic embedder selection\n\n    # Cohere\n    elif type(self._llm) in [ChatCohere]:\n        embedder = embedders.EmbedderCohereConfig.get_embedder_from_config(\n            {\n                \"cohere_api_key\": self._llm.cohere_api_key,\n                \"model\": \"embed-multilingual-v2.0\",\n                # Now the best model for embeddings is embed-multilingual-v2.0\n            }\n        )\n\n    elif type(self._llm) in [ChatGoogleGenerativeAI]:\n        embedder = embedders.EmbedderGeminiChatConfig.get_embedder_from_config(\n            {\n                \"model\": \"models/embedding-001\",\n                \"google_api_key\": self._llm.google_api_key,\n            }\n        )\n\n    else:\n        # If no embedder matches vendor, and no external embedder is configured, we use the DumbEmbedder.\n        #   `This embedder is not a model properly trained\n        #    and this makes it not suitable to effectively embed text,\n        #    \"but it does not know this and embeds anyway\".` - cit. Nicola Corbellini\n        embedder = embedders.EmbedderDumbConfig.get_embedder_from_config({})\n\n    return embedder\n</code></pre>"},{"location":"API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.load_language_model","title":"<code>load_language_model()</code>","text":"<p>Large Language Model (LLM) selection at bootstrap time.</p> <p>Returns:</p> Name Type Description <code>llm</code> <code>BaseLanguageModel</code> <p>Langchain <code>BaseLanguageModel</code> instance of the selected model.</p> Notes <p>Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories, the Main Agent, the Rabbit Hole and the White Rabbit.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def load_language_model(self) -&gt; BaseLanguageModel:\n    \"\"\"Large Language Model (LLM) selection at bootstrap time.\n\n    Returns\n    -------\n    llm : BaseLanguageModel\n        Langchain `BaseLanguageModel` instance of the selected model.\n\n    Notes\n    -----\n    Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories,\n    the *Main Agent*, the *Rabbit Hole* and the *White Rabbit*.\n    \"\"\"\n\n    selected_llm = crud.get_setting_by_name(name=\"llm_selected\")\n\n    if selected_llm is None:\n        # Return default LLM\n        return LLMDefaultConfig.get_llm_from_config({})\n\n    # Get LLM factory class\n    selected_llm_class = selected_llm[\"value\"][\"name\"]\n    FactoryClass = get_llm_from_name(selected_llm_class)\n\n    # Obtain configuration and instantiate LLM\n    selected_llm_config = crud.get_setting_by_name(name=selected_llm_class)\n    try:\n        llm = FactoryClass.get_llm_from_config(selected_llm_config[\"value\"])\n        return llm\n    except Exception:\n        log.error(\"Error during LLM instantiation\")\n        return LLMDefaultConfig.get_llm_from_config({})\n</code></pre>"},{"location":"API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.load_memory","title":"<code>load_memory()</code>","text":"<p>Load LongTerMemory and WorkingMemory.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def load_memory(self):\n    \"\"\"Load LongTerMemory and WorkingMemory.\"\"\"\n    # Memory\n\n    # Get embedder size (langchain classes do not store it)\n    embedder_size = len(self.embedder.embed_query(\"hello world\"))\n\n    # Get embedder name (useful for for vectorstore aliases)\n    if hasattr(self.embedder, \"model\"):\n        embedder_name = self.embedder.model\n    elif hasattr(self.embedder, \"repo_id\"):\n        embedder_name = self.embedder.repo_id\n    else:\n        embedder_name = \"default_embedder\"\n\n    # instantiate long term memory\n    vector_memory_config = {\n        \"embedder_name\": embedder_name,\n        \"embedder_size\": embedder_size,\n    }\n    self.memory = LongTermMemory(vector_memory_config=vector_memory_config)\n</code></pre>"},{"location":"API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.load_natural_language","title":"<code>load_natural_language()</code>","text":"<p>Load Natural Language related objects.</p> <p>The method exposes in the Cat all the NLP related stuff. Specifically, it sets the language models (LLM and Embedder).</p> Warnings <p>When using small Language Models it is suggested to turn off the memories and make the main prompt smaller to prevent them to fail.</p> See Also <p>agent_prompt_prefix</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def load_natural_language(self):\n    \"\"\"Load Natural Language related objects.\n\n    The method exposes in the Cat all the NLP related stuff. Specifically, it sets the language models\n    (LLM and Embedder).\n\n    Warnings\n    --------\n    When using small Language Models it is suggested to turn off the memories and make the main prompt smaller\n    to prevent them to fail.\n\n    See Also\n    --------\n    agent_prompt_prefix\n    \"\"\"\n    # LLM and embedder\n    self._llm = self.load_language_model()\n    self.embedder = self.load_language_embedder()\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/","title":"stray_cat","text":""},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat","title":"<code>StrayCat</code>","text":"<p>Session object containing user data, conversation state and many utility pointers. The framework creates an instance for every http request and websocket connection, making it available for plugins.</p> <p>You will be interacting with an instance of this class directly from within your plugins:</p> <ul> <li>in <code>@hook</code>, <code>@tool</code> and <code>@endpoint</code> decorated functions will be passed as argument <code>cat</code> or <code>stray</code></li> <li>in <code>@form</code> decorated classes you can access it via <code>self.cat</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>user_data</code> <code>AuthUserInfo</code> <p>User data object containing user information.</p> required Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>class StrayCat:\n    \"\"\"Session object containing user data, conversation state and many utility pointers.\n    The framework creates an instance for every http request and websocket connection, making it available for plugins.\n\n    You will be interacting with an instance of this class directly from within your plugins:\n\n     - in `@hook`, `@tool` and `@endpoint` decorated functions will be passed as argument `cat` or `stray`\n     - in `@form` decorated classes you can access it via `self.cat`\n\n    Parameters\n    ----------\n    user_data : AuthUserInfo\n        User data object containing user information.\n    \"\"\"\n\n    working_memory: WorkingMemory\n    \"\"\"State machine containing the conversation state, acting as a simple dictionary / object.\n    Can be used in plugins to store and retrieve data to drive the conversation or do anything else.\n\n    Examples\n    --------\n    Store a value in the working memory during conversation\n    &gt;&gt;&gt; cat.working_memory[\"location\"] = \"Rome\"\n    or\n    &gt;&gt;&gt; cat.working_memory.location = \"Rome\"\n\n    Retrieve a value in later conversation turns\n    &gt;&gt;&gt; cat.working_memory[\"location\"]\n    \"Rome\"\n    &gt;&gt;&gt; cat.working_memory.location\n    \"Rome\"\n    \"\"\"\n\n    def __init__(\n        self,\n        user_data: AuthUserInfo\n    ):\n\n        # user data\n        self.__user_id = user_data.name # TODOV2: use id\n        self.__user_data = user_data\n\n        # get working memory from cache or create a new one\n        self.load_working_memory_from_cache()\n\n    def __repr__(self):\n        return f\"StrayCat(user_id={self.user_id}, user_name={self.user_data.name})\"\n\n    def __send_ws_json(self, data: Any):\n        # Run the corutine in the main event loop in the main thread\n        # and wait for the result\n\n        app = CheshireCat().fastapi_app\n        ws_manager = app.state.websocket_manager\n        ws_connection = ws_manager.get_connection(self.user_id)\n        if not ws_connection:\n            log.debug(f\"No websocket connection is open for user {self.user_id}\")\n            return\n\n        asyncio.run_coroutine_threadsafe(\n            ws_connection.send_json(data),\n            app.state.event_loop,\n        ).result()\n\n    def __build_why(self) -&gt; MessageWhy:\n        # build data structure for output (response and why with memories)\n        # TODO: these 3 lines are a mess, simplify\n        episodic_report = [\n            dict(d[0]) | {\"score\": float(d[1]), \"id\": d[3]}\n            for d in self.working_memory.episodic_memories\n        ]\n        declarative_report = [\n            dict(d[0]) | {\"score\": float(d[1]), \"id\": d[3]}\n            for d in self.working_memory.declarative_memories\n        ]\n        procedural_report = [\n            dict(d[0]) | {\"score\": float(d[1]), \"id\": d[3]}\n            for d in self.working_memory.procedural_memories\n        ]\n\n        # why this response?\n        why = MessageWhy(\n            input=self.working_memory.user_message_json.text,\n            intermediate_steps=[],\n            memory={\n                \"episodic\": episodic_report,\n                \"declarative\": declarative_report,\n                \"procedural\": procedural_report,\n            },\n            model_interactions=self.working_memory.model_interactions,\n        )\n\n        return why\n\n    def load_working_memory_from_cache(self):\n        \"\"\"Load the working memory from the cache.\"\"\"\n\n        self.working_memory = \\\n            self.cache.get_value(f\"{self.user_id}_working_memory\") or WorkingMemory()\n\n    def update_working_memory_cache(self):\n        \"\"\"Update the working memory in the cache.\"\"\"\n\n        updated_cache_item = CacheItem(f\"{self.user_id}_working_memory\", self.working_memory, -1)\n        self.cache.insert(updated_cache_item)\n\n    def send_ws_message(self, content: str | dict, msg_type: MSG_TYPES = \"notification\"):\n        \"\"\"Send a message via websocket.\n\n        This method is useful for sending a message via websocket directly without passing through the LLM.  \n        In case there is no connection the message is skipped and a warning is logged.\n\n        Parameters\n        ----------\n        content : str\n            The content of the message.\n        msg_type : str\n            The type of the message. Should be either `notification` (default), `chat`, `chat_token` or `error`\n\n        Examples\n        --------\n        Send a notification via websocket\n        &gt;&gt;&gt; cat.send_ws_message(\"Hello, I'm a notification!\")\n\n        Send a chat message via websocket\n        &gt;&gt;&gt; cat.send_ws_message(\"Meooow!\", msg_type=\"chat\")\n\n        Send an error message via websocket\n        &gt;&gt;&gt; cat.send_ws_message(\"Something went wrong\", msg_type=\"error\")\n\n        Send custom data\n        &gt;&gt;&gt; cat.send_ws_message({\"What day it is?\": \"It's my unbirthday\"})\n        \"\"\"\n\n        options = get_args(MSG_TYPES)\n\n        if msg_type not in options:\n            raise ValueError(\n                f\"The message type `{msg_type}` is not valid. Valid types: {', '.join(options)}\"\n            )\n\n        if msg_type == \"error\":\n            self.__send_ws_json(\n                {\"type\": msg_type, \"name\": \"GenericError\", \"description\": str(content)}\n            )\n        else:\n            self.__send_ws_json({\"type\": msg_type, \"content\": content})\n\n    def send_chat_message(self, message: str | CatMessage, save=False):\n        \"\"\"Sends a chat message to the user using the active WebSocket connection.  \n        In case there is no connection the message is skipped and a warning is logged\n\n        Parameters\n        ----------\n        message: str, CatMessage\n            Message to send\n        save: bool | optional\n            Save the message in the conversation history. Defaults to False.\n\n        Examples\n        --------\n        Send a chat message during conversation from a hook, tool or form\n        &gt;&gt;&gt; cat.send_chat_message(\"Hello, dear!\")\n\n        Using a `CatMessage` object\n        &gt;&gt;&gt; message = CatMessage(text=\"Hello, dear!\", user_id=cat.user_id)\n        ... cat.send_chat_message(message)\n        \"\"\"\n\n        if isinstance(message, str):\n            why = self.__build_why()\n            message = CatMessage(text=message, user_id=self.user_id, why=why)\n\n        if save:\n            self.working_memory.update_history(\n                message\n            )\n\n        self.__send_ws_json(message.model_dump())\n\n    def send_notification(self, content: str):\n        \"\"\"Sends a notification message to the user using the active WebSocket connection.  \n        In case there is no connection the message is skipped and a warning is logged\n\n        Parameters\n        ----------\n        content: str\n            Message to send\n\n        Examples\n        --------\n        Send a notification to the user\n        &gt;&gt;&gt; cat.send_notification(\"It's late!\")\n        \"\"\"\n        self.send_ws_message(content=content, msg_type=\"notification\")\n\n    def send_error(self, error: Union[str, Exception]):\n        \"\"\"Sends an error message to the user using the active WebSocket connection.\n\n        In case there is no connection the message is skipped and a warning is logged\n\n        Parameters\n        ----------\n        error: str, Exception\n            Message to send\n\n        Examples\n        --------\n        Send an error message to the user\n        &gt;&gt;&gt; cat.send_error(\"Something went wrong!\")\n        or\n        &gt;&gt;&gt; cat.send_error(CustomException(\"Something went wrong!\"))\n        \"\"\"\n\n        if isinstance(error, str):\n            error_message = {\n                \"type\": \"error\",\n                \"name\": \"GenericError\",\n                \"description\": str(error),\n            }\n        else:\n            error_message = {\n                \"type\": \"error\",\n                \"name\": error.__class__.__name__,\n                \"description\": str(error),\n            }\n\n        self.__send_ws_json(error_message)\n\n    def recall_relevant_memories_to_working_memory(self, query=None):\n        \"\"\"Retrieve context from memory.\n\n        The method retrieves the relevant memories from the vector collections that are given as context to the LLM.\n        Recalled memories are stored in the working memory.\n\n        Parameters\n        ----------\n        query : str, optional\n            The query used to make a similarity search in the Cat's vector memories.  \n            If not provided, the query will be derived from the last user's message.\n\n        Examples\n        --------\n        Recall memories from custom query\n        &gt;&gt;&gt; cat.recall_relevant_memories_to_working_memory(query=\"What was written on the bottle?\")\n\n        Notes\n        -----\n        The user's message is used as a query to make a similarity search in the Cat's vector memories.\n        Five hooks allow to customize the recall pipeline before and after it is done.\n\n        See Also\n        --------\n        cat_recall_query\n        before_cat_recalls_memories\n        before_cat_recalls_episodic_memories\n        before_cat_recalls_declarative_memories\n        before_cat_recalls_procedural_memories\n        after_cat_recalls_memories\n        \"\"\"\n\n        recall_query = query\n\n        if query is None:\n            # If query is not provided, use the user's message as the query\n            recall_query = self.working_memory.user_message_json.text\n\n        # We may want to search in memory\n        recall_query = self.mad_hatter.execute_hook(\n            \"cat_recall_query\", recall_query, cat=self\n        )\n        log.info(f\"Recall query: '{recall_query}'\")\n\n        # Embed recall query\n        recall_query_embedding = self.embedder.embed_query(recall_query)\n        self.working_memory.recall_query = recall_query\n\n        # keep track of embedder model usage\n        self.working_memory.model_interactions.append(\n            EmbedderModelInteraction(\n                prompt=[recall_query],\n                source=utils.get_caller_info(skip=1),\n                reply=recall_query_embedding, # TODO: should we avoid storing the embedding?\n                input_tokens=len(tiktoken.get_encoding(\"cl100k_base\").encode(recall_query)),\n            )\n        )\n\n        # hook to do something before recall begins\n        self.mad_hatter.execute_hook(\"before_cat_recalls_memories\", cat=self)\n\n        # Setting default recall configs for each memory\n        # TODO: can these data structures become instances of a RecallSettings class?\n        default_episodic_recall_config = {\n            \"embedding\": recall_query_embedding,\n            \"k\": 3,\n            \"threshold\": 0.7,\n            \"metadata\": {\"source\": self.user_id},\n        }\n\n        default_declarative_recall_config = {\n            \"embedding\": recall_query_embedding,\n            \"k\": 3,\n            \"threshold\": 0.7,\n            \"metadata\": {},\n        }\n\n        default_procedural_recall_config = {\n            \"embedding\": recall_query_embedding,\n            \"k\": 3,\n            \"threshold\": 0.7,\n            \"metadata\": {},\n        }\n\n        # hooks to change recall configs for each memory\n        recall_configs = [\n            self.mad_hatter.execute_hook(\n                \"before_cat_recalls_episodic_memories\",\n                default_episodic_recall_config,\n                cat=self,\n            ),\n            self.mad_hatter.execute_hook(\n                \"before_cat_recalls_declarative_memories\",\n                default_declarative_recall_config,\n                cat=self,\n            ),\n            self.mad_hatter.execute_hook(\n                \"before_cat_recalls_procedural_memories\",\n                default_procedural_recall_config,\n                cat=self,\n            ),\n        ]\n\n        memory_types = self.memory.vectors.collections.keys()\n\n        for config, memory_type in zip(recall_configs, memory_types):\n            memory_key = f\"{memory_type}_memories\"\n\n            # recall relevant memories for collection\n            vector_memory = getattr(self.memory.vectors, memory_type)\n            memories = vector_memory.recall_memories_from_embedding(**config)\n\n            setattr(\n                self.working_memory, memory_key, memories\n            )  # self.working_memory.procedural_memories = ...\n\n        # hook to modify/enrich retrieved memories\n        self.mad_hatter.execute_hook(\"after_cat_recalls_memories\", cat=self)\n\n\n    def llm(self, prompt: str, stream: bool = False) -&gt; str:\n        \"\"\"Generate a response using the Large Language Model.\n\n        Parameters\n        ----------\n        prompt : str\n            The prompt for generating the response.\n        stream : bool\n            Whether to stream the tokens via websocket or not.\n\n        Returns\n        -------\n        str\n            The generated LLM response.\n\n        Examples\n        -------\n        Detect profanity in a message\n        &gt;&gt;&gt; message = cat.working_memory.user_message_json.text\n        ... cat.llm(f\"Does this message contain profanity: '{message}'?  Reply with 'yes' or 'no'.\")\n        \"no\"\n\n        Run the LLM and stream the tokens via websocket\n        &gt;&gt;&gt; cat.llm(\"Tell me which way to go?\", stream=True)\n        \"It doesn't matter which way you go\"\n        \"\"\"\n\n        # should we stream the tokens?\n        callbacks = []\n        if stream:\n            callbacks.append(NewTokenHandler(self))\n\n        # Add a token counter to the callbacks\n        caller = utils.get_caller_info(return_short=False)\n        callbacks.append(ModelInteractionHandler(self, caller or \"StrayCat\"))\n\n        # here we deal with motherfucking langchain\n        prompt = ChatPromptTemplate(\n            messages=[\n                HumanMessage(content=prompt) # We decided to use HumanMessage for wide-range compatibility even if it could bring some problem with tokenizers\n                # TODO: add here optional convo history passed to the method,\n                #  or taken from working memory\n            ]\n        )\n\n        chain = (\n            prompt\n            | RunnableLambda(lambda x: utils.langchain_log_prompt(x, f\"{caller} prompt\"))\n            | self._llm\n            | RunnableLambda(lambda x: utils.langchain_log_output(x, f\"{caller} prompt output\"))\n            | StrOutputParser()\n        )\n\n        output = chain.invoke(\n            {}, # in case we need to pass info to the template\n            config=RunnableConfig(callbacks=callbacks)\n        )\n\n        return output\n\n    def __call__(self, message_dict):\n        \"\"\"Run the conversation turn.\n\n        This method is called on the user's message received from the client.  \n        It is the main pipeline of the Cat, it is called automatically.\n\n        Parameters\n        ----------\n        message_dict : dict\n            Dictionary received from the client via http or websocket.\n\n        Returns\n        -------\n        final_output : CatMessage\n            CatMessage object, the Cat's answer to be sent back to the client.\n        \"\"\"\n\n        # Impose user_id as the one authenticated\n        # (ws message may contain a fake id)\n        message_dict[\"user_id\"] = self.user_id\n\n        # Parse websocket message into UserMessage obj\n        user_message = UserMessage.model_validate(message_dict)\n        log.info(user_message)\n\n        ### setup working memory for this convo turn\n        # keeping track of model interactions\n        self.working_memory.model_interactions = []\n        # latest user message\n        self.working_memory.user_message_json = user_message\n\n        # Run a totally custom reply (skips all the side effects of the framework)\n        fast_reply = self.mad_hatter.execute_hook(\n            \"fast_reply\", {}, cat=self\n        )\n        if isinstance(fast_reply, CatMessage):\n            return fast_reply\n        if isinstance(fast_reply, dict) and \"output\" in fast_reply:\n            return CatMessage(\n                user_id=self.user_id, text=str(fast_reply[\"output\"])\n            )\n\n        # hook to modify/enrich user input\n        self.working_memory.user_message_json = self.mad_hatter.execute_hook(\n            \"before_cat_reads_message\", self.working_memory.user_message_json, cat=self\n        )\n\n        # update conversation history (Human turn)\n        self.working_memory.update_history(\n            self.working_memory.user_message_json\n        )\n\n        # recall episodic and declarative memories from vector collections\n        #   and store them in working_memory\n        try:\n            self.recall_relevant_memories_to_working_memory()\n        except Exception:\n            log.error(\"Error during recall.\")\n\n            err_message = \"An error occurred while recalling relevant memories.\"\n\n            return {\n                \"type\": \"error\",\n                \"name\": \"VectorMemoryError\",\n                \"description\": err_message,\n            }\n\n        # reply with agent\n        try:\n            agent_output: AgentOutput = self.main_agent.execute(self)\n        except Exception as e:\n            # This error happens when the LLM\n            #   does not respect prompt instructions.\n            # We grab the LLM output here anyway, so small and\n            #   non instruction-fine-tuned models can still be used.\n            error_description = str(e)\n\n            log.error(error_description)\n            if \"Could not parse LLM output: `\" not in error_description:\n                raise e\n\n            unparsable_llm_output = error_description.replace(\n                \"Could not parse LLM output: `\", \"\"\n            ).replace(\"`\", \"\")\n            agent_output = AgentOutput(\n                output=unparsable_llm_output,\n            )\n\n        log.info(agent_output)\n\n        self._store_user_message_in_episodic_memory(\n            self.working_memory.user_message_json.text\n        )\n\n        # why this response?\n        why = self.__build_why()\n        # TODO: should these assignations be included in self.__build_why ?\n        why.intermediate_steps = agent_output.intermediate_steps\n        why.agent_output = agent_output.model_dump()\n\n        # prepare final cat message\n        final_output = CatMessage(\n            user_id=self.user_id, text=str(agent_output.output), why=why\n        )\n\n        # run message through plugins\n        final_output = self.mad_hatter.execute_hook(\n            \"before_cat_sends_message\", final_output, cat=self\n        )\n\n        # update conversation history (AI turn)\n        self.working_memory.update_history(\n            final_output\n        )\n\n        return final_output\n\n    def run(self, user_message_json, return_message=False):\n        try:\n            # run main flow\n            cat_message = self.__call__(user_message_json)\n            # save working memory to cache\n            self.update_working_memory_cache()\n\n            if return_message:\n                # return the message for HTTP usage\n                return cat_message\n            else:\n                # send message back to client via WS\n                self.send_chat_message(cat_message)\n        except Exception as e:\n            log.error(e)\n            if return_message:\n                return {\"error\": str(e)}\n            else:\n                try:\n                    self.send_error(e)\n                except ConnectionClosedOK as ex:\n                    log.warning(ex)\n\n    def classify(\n        self, sentence: str, labels: List[str] | Dict[str, List[str]], score_threshold: float = 0.5\n    ) -&gt; str | None:\n        \"\"\"Classify a sentence.\n\n        Parameters\n        ----------\n        sentence : str\n            Sentence to be classified.\n        labels : List[str] or Dict[str, List[str]]\n            Possible output categories and optional examples.\n\n        Returns\n        -------\n        label : str\n            Sentence category.\n\n        Examples\n        -------\n        &gt;&gt;&gt; cat.classify(\"I feel good\", labels=[\"positive\", \"negative\"])\n        \"positive\"\n\n        Or giving examples for each category:\n\n        &gt;&gt;&gt; example_labels = {\n        ...     \"positive\": [\"I feel nice\", \"happy today\"],\n        ...     \"negative\": [\"I feel bad\", \"not my best day\"],\n        ... }\n        ... cat.classify(\"it is a bad day\", labels=example_labels)\n        \"negative\"\n\n        \"\"\"\n\n        if isinstance(labels, dict):\n            labels_names = labels.keys()\n            examples_list = \"\\n\\nExamples:\"\n            for label, examples in labels.items():\n                for ex in examples:\n                    examples_list += f'\\n\"{ex}\" -&gt; \"{label}\"'\n        else:\n            labels_names = labels\n            examples_list = \"\"\n\n        labels_list = '\"' + '\", \"'.join(labels_names) + '\"'\n\n        prompt = f\"\"\"Classify this sentence:\n\"{sentence}\"\n\nAllowed classes are:\n{labels_list}{examples_list}\n\nJust output the class, nothing else.\"\"\"\n\n        response = self.llm(prompt)\n\n        # find the closest match and its score with levenshtein distance\n        best_label, score = min(\n            ((label, utils.levenshtein_distance(response, label)) for label in labels_names),\n            key=lambda x: x[1],\n        )\n\n        return best_label if score &lt; score_threshold else None\n\n    def langchainfy_chat_history(self, latest_n: int = 20) -&gt; List[BaseMessage]:\n        \"\"\"Redirects to WorkingMemory.langchainfy_chat_history. Will be removed from this class in v2.\"\"\"\n        return self.working_memory.langchainfy_chat_history(latest_n)\n\n    def stringify_chat_history(self, latest_n: int = 20) -&gt; str:\n        \"\"\"Redirects to WorkingMemory.stringify_chat_history. Will be removed from this class in v2.\"\"\"\n        return self.working_memory.stringify_chat_history(latest_n)\n\n    def _store_user_message_in_episodic_memory(self, user_message_text: str):\n        doc = Document(\n            page_content=user_message_text,\n            metadata={\"source\": self.user_id, \"when\": time.time()},\n        )\n        doc = self.mad_hatter.execute_hook(\n            \"before_cat_stores_episodic_memory\", doc, cat=self\n        )\n        # store user message in episodic memory\n        # TODO: vectorize and store also conversation chunks\n        #   (not raw dialog, but summarization)\n        user_message_embedding = self.embedder.embed_documents([user_message_text])\n        _ = self.memory.vectors.episodic.add_point(\n            doc.page_content,\n            user_message_embedding[0],\n            doc.metadata,\n        )\n\n    @property\n    def user_id(self) -&gt; str:\n        \"\"\"The user's id.\n\n        Returns\n        -------\n        user_id : str\n            Current user's id.\n        \"\"\"\n        return self.__user_id\n\n    @property\n    def user_data(self) -&gt; AuthUserInfo:\n        \"\"\"`AuthUserInfo` object containing user data.\n\n        Returns\n        -------\n        user_data : AuthUserInfo\n            Current user's data.\n        \"\"\"\n        return self.__user_data\n\n    @property\n    def _llm(self):\n        \"\"\"Instance of langchain `LLM`.\n        Only use it if you directly want to deal with langchain, prefer method `cat.llm(prompt)` otherwise.\n        \"\"\"\n        return CheshireCat()._llm\n\n    @property\n    def embedder(self):\n        \"\"\"Langchain `Embeddings` object.\n\n        Returns\n        -------\n        embedder : langchain `Embeddings`\n            Langchain embedder to turn text into a vector.\n\n\n        Examples\n        --------\n        &gt;&gt;&gt; cat.embedder.embed_query(\"Oh dear!\")\n        [0.2, 0.02, 0.4, ...]\n        \"\"\"\n        return CheshireCat().embedder\n\n    @property\n    def memory(self):\n        \"\"\"Gives access to the long term memory, containing vector DB collections (episodic, declarative, procedural).\n\n        Returns\n        -------\n        memory : LongTermMemory\n            Long term memory of the Cat.\n\n\n        Examples\n        --------\n        &gt;&gt;&gt; cat.memory.vectors.episodic\n        VectorMemoryCollection object for the episodic memory.\n        \"\"\"\n        return CheshireCat().memory\n\n    @property\n    def rabbit_hole(self):\n        \"\"\"Gives access to the `RabbitHole`, to upload documents and URLs into the vector DB.\n\n        Returns\n        -------\n        rabbit_hole : RabbitHole\n            Module to ingest documents and URLs for RAG.\n\n\n        Examples\n        --------\n        &gt;&gt;&gt; cat.rabbit_hole.ingest_file(...)\n        \"\"\"\n        return CheshireCat().rabbit_hole\n\n    @property\n    def mad_hatter(self):\n        \"\"\"Gives access to the `MadHatter` plugin manager.\n\n        Returns\n        -------\n        mad_hatter : MadHatter\n            Module to manage plugins.\n\n\n        Examples\n        --------\n\n        Obtain the path in which your plugin is located\n        &gt;&gt;&gt; cat.mad_hatter.get_plugin().path\n        /app/cat/plugins/my_plugin\n\n        Obtain plugin settings\n        &gt;&gt;&gt; cat.mad_hatter.get_plugin().load_settings()\n        {\"num_cats\": 44, \"rows\": 6, \"remainder\": 0}\n        \"\"\"\n        return CheshireCat().mad_hatter\n\n    @property\n    def main_agent(self):\n        \"\"\"Gives access to the default main agent.\n        \"\"\"\n        return CheshireCat().main_agent\n\n    @property\n    def white_rabbit(self):\n        \"\"\"Gives access to `WhiteRabbit`, to schedule repeatable tasks.\n\n        Returns\n        -------\n        white_rabbit : WhiteRabbit\n            Module to manage cron tasks via `APScheduler`.\n\n        Examples\n        --------\n        Send a websocket message after 30 seconds\n        &gt;&gt;&gt; def ring_alarm_api():\n        ...     cat.send_chat_message(\"It's late!\")\n        ...\n        ... cat.white_rabbit.schedule_job(ring_alarm_api, seconds=30)\n        \"\"\"\n        return CheshireCat().white_rabbit\n\n    @property\n    def cache(self):\n        \"\"\"Gives access to internal cache.\"\"\"\n        return CheshireCat().cache\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.cache","title":"<code>cache</code>  <code>property</code>","text":"<p>Gives access to internal cache.</p>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.embedder","title":"<code>embedder</code>  <code>property</code>","text":"<p>Langchain <code>Embeddings</code> object.</p> <p>Returns:</p> Name Type Description <code>embedder</code> <code>langchain `Embeddings`</code> <p>Langchain embedder to turn text into a vector.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cat.embedder.embed_query(\"Oh dear!\")\n[0.2, 0.02, 0.4, ...]\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.mad_hatter","title":"<code>mad_hatter</code>  <code>property</code>","text":"<p>Gives access to the <code>MadHatter</code> plugin manager.</p> <p>Returns:</p> Name Type Description <code>mad_hatter</code> <code>MadHatter</code> <p>Module to manage plugins.</p> <p>Examples:</p> <p>Obtain the path in which your plugin is located</p> <pre><code>&gt;&gt;&gt; cat.mad_hatter.get_plugin().path\n/app/cat/plugins/my_plugin\n</code></pre> <p>Obtain plugin settings</p> <pre><code>&gt;&gt;&gt; cat.mad_hatter.get_plugin().load_settings()\n{\"num_cats\": 44, \"rows\": 6, \"remainder\": 0}\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.main_agent","title":"<code>main_agent</code>  <code>property</code>","text":"<p>Gives access to the default main agent.</p>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.memory","title":"<code>memory</code>  <code>property</code>","text":"<p>Gives access to the long term memory, containing vector DB collections (episodic, declarative, procedural).</p> <p>Returns:</p> Name Type Description <code>memory</code> <code>LongTermMemory</code> <p>Long term memory of the Cat.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cat.memory.vectors.episodic\nVectorMemoryCollection object for the episodic memory.\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.rabbit_hole","title":"<code>rabbit_hole</code>  <code>property</code>","text":"<p>Gives access to the <code>RabbitHole</code>, to upload documents and URLs into the vector DB.</p> <p>Returns:</p> Name Type Description <code>rabbit_hole</code> <code>RabbitHole</code> <p>Module to ingest documents and URLs for RAG.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cat.rabbit_hole.ingest_file(...)\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.user_data","title":"<code>user_data</code>  <code>property</code>","text":"<p><code>AuthUserInfo</code> object containing user data.</p> <p>Returns:</p> Name Type Description <code>user_data</code> <code>AuthUserInfo</code> <p>Current user's data.</p>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.user_id","title":"<code>user_id</code>  <code>property</code>","text":"<p>The user's id.</p> <p>Returns:</p> Name Type Description <code>user_id</code> <code>str</code> <p>Current user's id.</p>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.white_rabbit","title":"<code>white_rabbit</code>  <code>property</code>","text":"<p>Gives access to <code>WhiteRabbit</code>, to schedule repeatable tasks.</p> <p>Returns:</p> Name Type Description <code>white_rabbit</code> <code>WhiteRabbit</code> <p>Module to manage cron tasks via <code>APScheduler</code>.</p> <p>Examples:</p> <p>Send a websocket message after 30 seconds</p> <pre><code>&gt;&gt;&gt; def ring_alarm_api():\n...     cat.send_chat_message(\"It's late!\")\n...\n... cat.white_rabbit.schedule_job(ring_alarm_api, seconds=30)\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.working_memory","title":"<code>working_memory</code>  <code>instance-attribute</code>","text":"<p>State machine containing the conversation state, acting as a simple dictionary / object. Can be used in plugins to store and retrieve data to drive the conversation or do anything else.</p> <p>Examples:</p> <p>Store a value in the working memory during conversation</p> <pre><code>&gt;&gt;&gt; cat.working_memory[\"location\"] = \"Rome\"\nor\n&gt;&gt;&gt; cat.working_memory.location = \"Rome\"\n</code></pre> <p>Retrieve a value in later conversation turns</p> <pre><code>&gt;&gt;&gt; cat.working_memory[\"location\"]\n\"Rome\"\n&gt;&gt;&gt; cat.working_memory.location\n\"Rome\"\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.__call__","title":"<code>__call__(message_dict)</code>","text":"<p>Run the conversation turn.</p> <p>This method is called on the user's message received from the client. It is the main pipeline of the Cat, it is called automatically.</p> <p>Parameters:</p> Name Type Description Default <code>message_dict</code> <code>dict</code> <p>Dictionary received from the client via http or websocket.</p> required <p>Returns:</p> Name Type Description <code>final_output</code> <code>CatMessage</code> <p>CatMessage object, the Cat's answer to be sent back to the client.</p> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>def __call__(self, message_dict):\n    \"\"\"Run the conversation turn.\n\n    This method is called on the user's message received from the client.  \n    It is the main pipeline of the Cat, it is called automatically.\n\n    Parameters\n    ----------\n    message_dict : dict\n        Dictionary received from the client via http or websocket.\n\n    Returns\n    -------\n    final_output : CatMessage\n        CatMessage object, the Cat's answer to be sent back to the client.\n    \"\"\"\n\n    # Impose user_id as the one authenticated\n    # (ws message may contain a fake id)\n    message_dict[\"user_id\"] = self.user_id\n\n    # Parse websocket message into UserMessage obj\n    user_message = UserMessage.model_validate(message_dict)\n    log.info(user_message)\n\n    ### setup working memory for this convo turn\n    # keeping track of model interactions\n    self.working_memory.model_interactions = []\n    # latest user message\n    self.working_memory.user_message_json = user_message\n\n    # Run a totally custom reply (skips all the side effects of the framework)\n    fast_reply = self.mad_hatter.execute_hook(\n        \"fast_reply\", {}, cat=self\n    )\n    if isinstance(fast_reply, CatMessage):\n        return fast_reply\n    if isinstance(fast_reply, dict) and \"output\" in fast_reply:\n        return CatMessage(\n            user_id=self.user_id, text=str(fast_reply[\"output\"])\n        )\n\n    # hook to modify/enrich user input\n    self.working_memory.user_message_json = self.mad_hatter.execute_hook(\n        \"before_cat_reads_message\", self.working_memory.user_message_json, cat=self\n    )\n\n    # update conversation history (Human turn)\n    self.working_memory.update_history(\n        self.working_memory.user_message_json\n    )\n\n    # recall episodic and declarative memories from vector collections\n    #   and store them in working_memory\n    try:\n        self.recall_relevant_memories_to_working_memory()\n    except Exception:\n        log.error(\"Error during recall.\")\n\n        err_message = \"An error occurred while recalling relevant memories.\"\n\n        return {\n            \"type\": \"error\",\n            \"name\": \"VectorMemoryError\",\n            \"description\": err_message,\n        }\n\n    # reply with agent\n    try:\n        agent_output: AgentOutput = self.main_agent.execute(self)\n    except Exception as e:\n        # This error happens when the LLM\n        #   does not respect prompt instructions.\n        # We grab the LLM output here anyway, so small and\n        #   non instruction-fine-tuned models can still be used.\n        error_description = str(e)\n\n        log.error(error_description)\n        if \"Could not parse LLM output: `\" not in error_description:\n            raise e\n\n        unparsable_llm_output = error_description.replace(\n            \"Could not parse LLM output: `\", \"\"\n        ).replace(\"`\", \"\")\n        agent_output = AgentOutput(\n            output=unparsable_llm_output,\n        )\n\n    log.info(agent_output)\n\n    self._store_user_message_in_episodic_memory(\n        self.working_memory.user_message_json.text\n    )\n\n    # why this response?\n    why = self.__build_why()\n    # TODO: should these assignations be included in self.__build_why ?\n    why.intermediate_steps = agent_output.intermediate_steps\n    why.agent_output = agent_output.model_dump()\n\n    # prepare final cat message\n    final_output = CatMessage(\n        user_id=self.user_id, text=str(agent_output.output), why=why\n    )\n\n    # run message through plugins\n    final_output = self.mad_hatter.execute_hook(\n        \"before_cat_sends_message\", final_output, cat=self\n    )\n\n    # update conversation history (AI turn)\n    self.working_memory.update_history(\n        final_output\n    )\n\n    return final_output\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.classify","title":"<code>classify(sentence, labels, score_threshold=0.5)</code>","text":"<p>Classify a sentence.</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>str</code> <p>Sentence to be classified.</p> required <code>labels</code> <code>List[str] or Dict[str, List[str]]</code> <p>Possible output categories and optional examples.</p> required <p>Returns:</p> Name Type Description <code>label</code> <code>str</code> <p>Sentence category.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cat.classify(\"I feel good\", labels=[\"positive\", \"negative\"])\n\"positive\"\n</code></pre> <p>Or giving examples for each category:</p> <pre><code>&gt;&gt;&gt; example_labels = {\n...     \"positive\": [\"I feel nice\", \"happy today\"],\n...     \"negative\": [\"I feel bad\", \"not my best day\"],\n... }\n... cat.classify(\"it is a bad day\", labels=example_labels)\n\"negative\"\n</code></pre> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>    def classify(\n        self, sentence: str, labels: List[str] | Dict[str, List[str]], score_threshold: float = 0.5\n    ) -&gt; str | None:\n        \"\"\"Classify a sentence.\n\n        Parameters\n        ----------\n        sentence : str\n            Sentence to be classified.\n        labels : List[str] or Dict[str, List[str]]\n            Possible output categories and optional examples.\n\n        Returns\n        -------\n        label : str\n            Sentence category.\n\n        Examples\n        -------\n        &gt;&gt;&gt; cat.classify(\"I feel good\", labels=[\"positive\", \"negative\"])\n        \"positive\"\n\n        Or giving examples for each category:\n\n        &gt;&gt;&gt; example_labels = {\n        ...     \"positive\": [\"I feel nice\", \"happy today\"],\n        ...     \"negative\": [\"I feel bad\", \"not my best day\"],\n        ... }\n        ... cat.classify(\"it is a bad day\", labels=example_labels)\n        \"negative\"\n\n        \"\"\"\n\n        if isinstance(labels, dict):\n            labels_names = labels.keys()\n            examples_list = \"\\n\\nExamples:\"\n            for label, examples in labels.items():\n                for ex in examples:\n                    examples_list += f'\\n\"{ex}\" -&gt; \"{label}\"'\n        else:\n            labels_names = labels\n            examples_list = \"\"\n\n        labels_list = '\"' + '\", \"'.join(labels_names) + '\"'\n\n        prompt = f\"\"\"Classify this sentence:\n\"{sentence}\"\n\nAllowed classes are:\n{labels_list}{examples_list}\n\nJust output the class, nothing else.\"\"\"\n\n        response = self.llm(prompt)\n\n        # find the closest match and its score with levenshtein distance\n        best_label, score = min(\n            ((label, utils.levenshtein_distance(response, label)) for label in labels_names),\n            key=lambda x: x[1],\n        )\n\n        return best_label if score &lt; score_threshold else None\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.langchainfy_chat_history","title":"<code>langchainfy_chat_history(latest_n=20)</code>","text":"<p>Redirects to WorkingMemory.langchainfy_chat_history. Will be removed from this class in v2.</p> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>def langchainfy_chat_history(self, latest_n: int = 20) -&gt; List[BaseMessage]:\n    \"\"\"Redirects to WorkingMemory.langchainfy_chat_history. Will be removed from this class in v2.\"\"\"\n    return self.working_memory.langchainfy_chat_history(latest_n)\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.llm","title":"<code>llm(prompt, stream=False)</code>","text":"<p>Generate a response using the Large Language Model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt for generating the response.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the tokens via websocket or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated LLM response.</p> <p>Examples:</p> <p>Detect profanity in a message</p> <pre><code>&gt;&gt;&gt; message = cat.working_memory.user_message_json.text\n... cat.llm(f\"Does this message contain profanity: '{message}'?  Reply with 'yes' or 'no'.\")\n\"no\"\n</code></pre> <p>Run the LLM and stream the tokens via websocket</p> <pre><code>&gt;&gt;&gt; cat.llm(\"Tell me which way to go?\", stream=True)\n\"It doesn't matter which way you go\"\n</code></pre> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>def llm(self, prompt: str, stream: bool = False) -&gt; str:\n    \"\"\"Generate a response using the Large Language Model.\n\n    Parameters\n    ----------\n    prompt : str\n        The prompt for generating the response.\n    stream : bool\n        Whether to stream the tokens via websocket or not.\n\n    Returns\n    -------\n    str\n        The generated LLM response.\n\n    Examples\n    -------\n    Detect profanity in a message\n    &gt;&gt;&gt; message = cat.working_memory.user_message_json.text\n    ... cat.llm(f\"Does this message contain profanity: '{message}'?  Reply with 'yes' or 'no'.\")\n    \"no\"\n\n    Run the LLM and stream the tokens via websocket\n    &gt;&gt;&gt; cat.llm(\"Tell me which way to go?\", stream=True)\n    \"It doesn't matter which way you go\"\n    \"\"\"\n\n    # should we stream the tokens?\n    callbacks = []\n    if stream:\n        callbacks.append(NewTokenHandler(self))\n\n    # Add a token counter to the callbacks\n    caller = utils.get_caller_info(return_short=False)\n    callbacks.append(ModelInteractionHandler(self, caller or \"StrayCat\"))\n\n    # here we deal with motherfucking langchain\n    prompt = ChatPromptTemplate(\n        messages=[\n            HumanMessage(content=prompt) # We decided to use HumanMessage for wide-range compatibility even if it could bring some problem with tokenizers\n            # TODO: add here optional convo history passed to the method,\n            #  or taken from working memory\n        ]\n    )\n\n    chain = (\n        prompt\n        | RunnableLambda(lambda x: utils.langchain_log_prompt(x, f\"{caller} prompt\"))\n        | self._llm\n        | RunnableLambda(lambda x: utils.langchain_log_output(x, f\"{caller} prompt output\"))\n        | StrOutputParser()\n    )\n\n    output = chain.invoke(\n        {}, # in case we need to pass info to the template\n        config=RunnableConfig(callbacks=callbacks)\n    )\n\n    return output\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.load_working_memory_from_cache","title":"<code>load_working_memory_from_cache()</code>","text":"<p>Load the working memory from the cache.</p> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>def load_working_memory_from_cache(self):\n    \"\"\"Load the working memory from the cache.\"\"\"\n\n    self.working_memory = \\\n        self.cache.get_value(f\"{self.user_id}_working_memory\") or WorkingMemory()\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.recall_relevant_memories_to_working_memory","title":"<code>recall_relevant_memories_to_working_memory(query=None)</code>","text":"<p>Retrieve context from memory.</p> <p>The method retrieves the relevant memories from the vector collections that are given as context to the LLM. Recalled memories are stored in the working memory.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query used to make a similarity search in the Cat's vector memories. If not provided, the query will be derived from the last user's message.</p> <code>None</code> <p>Examples:</p> <p>Recall memories from custom query</p> <pre><code>&gt;&gt;&gt; cat.recall_relevant_memories_to_working_memory(query=\"What was written on the bottle?\")\n</code></pre> Notes <p>The user's message is used as a query to make a similarity search in the Cat's vector memories. Five hooks allow to customize the recall pipeline before and after it is done.</p> See Also <p>cat_recall_query before_cat_recalls_memories before_cat_recalls_episodic_memories before_cat_recalls_declarative_memories before_cat_recalls_procedural_memories after_cat_recalls_memories</p> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>def recall_relevant_memories_to_working_memory(self, query=None):\n    \"\"\"Retrieve context from memory.\n\n    The method retrieves the relevant memories from the vector collections that are given as context to the LLM.\n    Recalled memories are stored in the working memory.\n\n    Parameters\n    ----------\n    query : str, optional\n        The query used to make a similarity search in the Cat's vector memories.  \n        If not provided, the query will be derived from the last user's message.\n\n    Examples\n    --------\n    Recall memories from custom query\n    &gt;&gt;&gt; cat.recall_relevant_memories_to_working_memory(query=\"What was written on the bottle?\")\n\n    Notes\n    -----\n    The user's message is used as a query to make a similarity search in the Cat's vector memories.\n    Five hooks allow to customize the recall pipeline before and after it is done.\n\n    See Also\n    --------\n    cat_recall_query\n    before_cat_recalls_memories\n    before_cat_recalls_episodic_memories\n    before_cat_recalls_declarative_memories\n    before_cat_recalls_procedural_memories\n    after_cat_recalls_memories\n    \"\"\"\n\n    recall_query = query\n\n    if query is None:\n        # If query is not provided, use the user's message as the query\n        recall_query = self.working_memory.user_message_json.text\n\n    # We may want to search in memory\n    recall_query = self.mad_hatter.execute_hook(\n        \"cat_recall_query\", recall_query, cat=self\n    )\n    log.info(f\"Recall query: '{recall_query}'\")\n\n    # Embed recall query\n    recall_query_embedding = self.embedder.embed_query(recall_query)\n    self.working_memory.recall_query = recall_query\n\n    # keep track of embedder model usage\n    self.working_memory.model_interactions.append(\n        EmbedderModelInteraction(\n            prompt=[recall_query],\n            source=utils.get_caller_info(skip=1),\n            reply=recall_query_embedding, # TODO: should we avoid storing the embedding?\n            input_tokens=len(tiktoken.get_encoding(\"cl100k_base\").encode(recall_query)),\n        )\n    )\n\n    # hook to do something before recall begins\n    self.mad_hatter.execute_hook(\"before_cat_recalls_memories\", cat=self)\n\n    # Setting default recall configs for each memory\n    # TODO: can these data structures become instances of a RecallSettings class?\n    default_episodic_recall_config = {\n        \"embedding\": recall_query_embedding,\n        \"k\": 3,\n        \"threshold\": 0.7,\n        \"metadata\": {\"source\": self.user_id},\n    }\n\n    default_declarative_recall_config = {\n        \"embedding\": recall_query_embedding,\n        \"k\": 3,\n        \"threshold\": 0.7,\n        \"metadata\": {},\n    }\n\n    default_procedural_recall_config = {\n        \"embedding\": recall_query_embedding,\n        \"k\": 3,\n        \"threshold\": 0.7,\n        \"metadata\": {},\n    }\n\n    # hooks to change recall configs for each memory\n    recall_configs = [\n        self.mad_hatter.execute_hook(\n            \"before_cat_recalls_episodic_memories\",\n            default_episodic_recall_config,\n            cat=self,\n        ),\n        self.mad_hatter.execute_hook(\n            \"before_cat_recalls_declarative_memories\",\n            default_declarative_recall_config,\n            cat=self,\n        ),\n        self.mad_hatter.execute_hook(\n            \"before_cat_recalls_procedural_memories\",\n            default_procedural_recall_config,\n            cat=self,\n        ),\n    ]\n\n    memory_types = self.memory.vectors.collections.keys()\n\n    for config, memory_type in zip(recall_configs, memory_types):\n        memory_key = f\"{memory_type}_memories\"\n\n        # recall relevant memories for collection\n        vector_memory = getattr(self.memory.vectors, memory_type)\n        memories = vector_memory.recall_memories_from_embedding(**config)\n\n        setattr(\n            self.working_memory, memory_key, memories\n        )  # self.working_memory.procedural_memories = ...\n\n    # hook to modify/enrich retrieved memories\n    self.mad_hatter.execute_hook(\"after_cat_recalls_memories\", cat=self)\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.send_chat_message","title":"<code>send_chat_message(message, save=False)</code>","text":"<p>Sends a chat message to the user using the active WebSocket connection. In case there is no connection the message is skipped and a warning is logged</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | CatMessage</code> <p>Message to send</p> required <code>save</code> <p>Save the message in the conversation history. Defaults to False.</p> <code>False</code> <p>Examples:</p> <p>Send a chat message during conversation from a hook, tool or form</p> <pre><code>&gt;&gt;&gt; cat.send_chat_message(\"Hello, dear!\")\n</code></pre> <p>Using a <code>CatMessage</code> object</p> <pre><code>&gt;&gt;&gt; message = CatMessage(text=\"Hello, dear!\", user_id=cat.user_id)\n... cat.send_chat_message(message)\n</code></pre> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>def send_chat_message(self, message: str | CatMessage, save=False):\n    \"\"\"Sends a chat message to the user using the active WebSocket connection.  \n    In case there is no connection the message is skipped and a warning is logged\n\n    Parameters\n    ----------\n    message: str, CatMessage\n        Message to send\n    save: bool | optional\n        Save the message in the conversation history. Defaults to False.\n\n    Examples\n    --------\n    Send a chat message during conversation from a hook, tool or form\n    &gt;&gt;&gt; cat.send_chat_message(\"Hello, dear!\")\n\n    Using a `CatMessage` object\n    &gt;&gt;&gt; message = CatMessage(text=\"Hello, dear!\", user_id=cat.user_id)\n    ... cat.send_chat_message(message)\n    \"\"\"\n\n    if isinstance(message, str):\n        why = self.__build_why()\n        message = CatMessage(text=message, user_id=self.user_id, why=why)\n\n    if save:\n        self.working_memory.update_history(\n            message\n        )\n\n    self.__send_ws_json(message.model_dump())\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.send_error","title":"<code>send_error(error)</code>","text":"<p>Sends an error message to the user using the active WebSocket connection.</p> <p>In case there is no connection the message is skipped and a warning is logged</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Union[str, Exception]</code> <p>Message to send</p> required <p>Examples:</p> <p>Send an error message to the user</p> <pre><code>&gt;&gt;&gt; cat.send_error(\"Something went wrong!\")\nor\n&gt;&gt;&gt; cat.send_error(CustomException(\"Something went wrong!\"))\n</code></pre> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>def send_error(self, error: Union[str, Exception]):\n    \"\"\"Sends an error message to the user using the active WebSocket connection.\n\n    In case there is no connection the message is skipped and a warning is logged\n\n    Parameters\n    ----------\n    error: str, Exception\n        Message to send\n\n    Examples\n    --------\n    Send an error message to the user\n    &gt;&gt;&gt; cat.send_error(\"Something went wrong!\")\n    or\n    &gt;&gt;&gt; cat.send_error(CustomException(\"Something went wrong!\"))\n    \"\"\"\n\n    if isinstance(error, str):\n        error_message = {\n            \"type\": \"error\",\n            \"name\": \"GenericError\",\n            \"description\": str(error),\n        }\n    else:\n        error_message = {\n            \"type\": \"error\",\n            \"name\": error.__class__.__name__,\n            \"description\": str(error),\n        }\n\n    self.__send_ws_json(error_message)\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.send_notification","title":"<code>send_notification(content)</code>","text":"<p>Sends a notification message to the user using the active WebSocket connection. In case there is no connection the message is skipped and a warning is logged</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Message to send</p> required <p>Examples:</p> <p>Send a notification to the user</p> <pre><code>&gt;&gt;&gt; cat.send_notification(\"It's late!\")\n</code></pre> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>def send_notification(self, content: str):\n    \"\"\"Sends a notification message to the user using the active WebSocket connection.  \n    In case there is no connection the message is skipped and a warning is logged\n\n    Parameters\n    ----------\n    content: str\n        Message to send\n\n    Examples\n    --------\n    Send a notification to the user\n    &gt;&gt;&gt; cat.send_notification(\"It's late!\")\n    \"\"\"\n    self.send_ws_message(content=content, msg_type=\"notification\")\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.send_ws_message","title":"<code>send_ws_message(content, msg_type='notification')</code>","text":"<p>Send a message via websocket.</p> <p>This method is useful for sending a message via websocket directly without passing through the LLM. In case there is no connection the message is skipped and a warning is logged.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the message.</p> required <code>msg_type</code> <code>str</code> <p>The type of the message. Should be either <code>notification</code> (default), <code>chat</code>, <code>chat_token</code> or <code>error</code></p> <code>'notification'</code> <p>Examples:</p> <p>Send a notification via websocket</p> <pre><code>&gt;&gt;&gt; cat.send_ws_message(\"Hello, I'm a notification!\")\n</code></pre> <p>Send a chat message via websocket</p> <pre><code>&gt;&gt;&gt; cat.send_ws_message(\"Meooow!\", msg_type=\"chat\")\n</code></pre> <p>Send an error message via websocket</p> <pre><code>&gt;&gt;&gt; cat.send_ws_message(\"Something went wrong\", msg_type=\"error\")\n</code></pre> <p>Send custom data</p> <pre><code>&gt;&gt;&gt; cat.send_ws_message({\"What day it is?\": \"It's my unbirthday\"})\n</code></pre> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>def send_ws_message(self, content: str | dict, msg_type: MSG_TYPES = \"notification\"):\n    \"\"\"Send a message via websocket.\n\n    This method is useful for sending a message via websocket directly without passing through the LLM.  \n    In case there is no connection the message is skipped and a warning is logged.\n\n    Parameters\n    ----------\n    content : str\n        The content of the message.\n    msg_type : str\n        The type of the message. Should be either `notification` (default), `chat`, `chat_token` or `error`\n\n    Examples\n    --------\n    Send a notification via websocket\n    &gt;&gt;&gt; cat.send_ws_message(\"Hello, I'm a notification!\")\n\n    Send a chat message via websocket\n    &gt;&gt;&gt; cat.send_ws_message(\"Meooow!\", msg_type=\"chat\")\n\n    Send an error message via websocket\n    &gt;&gt;&gt; cat.send_ws_message(\"Something went wrong\", msg_type=\"error\")\n\n    Send custom data\n    &gt;&gt;&gt; cat.send_ws_message({\"What day it is?\": \"It's my unbirthday\"})\n    \"\"\"\n\n    options = get_args(MSG_TYPES)\n\n    if msg_type not in options:\n        raise ValueError(\n            f\"The message type `{msg_type}` is not valid. Valid types: {', '.join(options)}\"\n        )\n\n    if msg_type == \"error\":\n        self.__send_ws_json(\n            {\"type\": msg_type, \"name\": \"GenericError\", \"description\": str(content)}\n        )\n    else:\n        self.__send_ws_json({\"type\": msg_type, \"content\": content})\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.stringify_chat_history","title":"<code>stringify_chat_history(latest_n=20)</code>","text":"<p>Redirects to WorkingMemory.stringify_chat_history. Will be removed from this class in v2.</p> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>def stringify_chat_history(self, latest_n: int = 20) -&gt; str:\n    \"\"\"Redirects to WorkingMemory.stringify_chat_history. Will be removed from this class in v2.\"\"\"\n    return self.working_memory.stringify_chat_history(latest_n)\n</code></pre>"},{"location":"API_Documentation/looking_glass/stray_cat/#cat.looking_glass.stray_cat.StrayCat.update_working_memory_cache","title":"<code>update_working_memory_cache()</code>","text":"<p>Update the working memory in the cache.</p> Source code in <code>cat/looking_glass/stray_cat.py</code> <pre><code>def update_working_memory_cache(self):\n    \"\"\"Update the working memory in the cache.\"\"\"\n\n    updated_cache_item = CacheItem(f\"{self.user_id}_working_memory\", self.working_memory, -1)\n    self.cache.insert(updated_cache_item)\n</code></pre>"},{"location":"API_Documentation/mad_hatter/mad_hatter/","title":"mad_hatter","text":""},{"location":"API_Documentation/mad_hatter/plugin/","title":"plugin","text":""},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/agent/","title":"agent","text":"<p>Hooks to modify the Cat's Agent.</p> <p>Here is a collection of methods to hook into the Agent execution pipeline.</p>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/agent/#cat.mad_hatter.core_plugin.hooks.agent.agent_allowed_tools","title":"<code>agent_allowed_tools(allowed_tools, cat)</code>","text":"<p>Hook the allowed tools.</p> <p>Allows to decide which tools end up in the Agent prompt.</p> <p>To decide, you can filter the list of tools' names, but you can also check the context in <code>cat.working_memory</code> and launch custom chains with <code>cat._llm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>tools</code> <code>List[str]</code> <p>List of allowed Langchain tools.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/agent.py</code> <pre><code>@hook(priority=0)\ndef agent_allowed_tools(allowed_tools: List[str], cat) -&gt; List[str]:\n    \"\"\"Hook the allowed tools.\n\n    Allows to decide which tools end up in the *Agent* prompt.\n\n    To decide, you can filter the list of tools' names, but you can also check the context in `cat.working_memory`\n    and launch custom chains with `cat._llm`.\n\n    Parameters\n    ---------\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    tools : List[str]\n        List of allowed Langchain tools.\n    \"\"\"\n\n    return allowed_tools\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/agent/#cat.mad_hatter.core_plugin.hooks.agent.agent_fast_reply","title":"<code>agent_fast_reply(agent_fast_reply, cat)</code>","text":"<p>This hook allows for a custom response after memory recall, skipping default agent execution. It's useful for custom agent logic or when you want to use recalled memories but avoid the main agent.</p> <p>Parameters:</p> Name Type Description Default <code>agent_fast_reply</code> <code>dict</code> <p>An initially empty dict that can be populated with a response.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>None | dict | AgentOutput</code> <p>If you want to bypass the main agent, return an AgentOutput or a dict with an \"output\" key. Return None to continue with normal execution. See below for examples of Cat response</p> <p>Examples:</p> <p>Example 1: don't remember (no uploaded documents about topic) </p><pre><code>num_declarative_memories = len( cat.working_memory.declarative_memories )\nif num_declarative_memories == 0:\n    return {\n       \"output\": \"Sorry, I have no memories about that.\"\n    }\n</code></pre> Source code in <code>cat/mad_hatter/core_plugin/hooks/agent.py</code> <pre><code>@hook(priority=0)\ndef agent_fast_reply(agent_fast_reply: dict, cat) -&gt; None | dict | AgentOutput:\n    \"\"\"This hook allows for a custom response after memory recall, skipping default agent execution.\n    It's useful for custom agent logic or when you want to use recalled memories but avoid the main agent.\n\n    Parameters\n    --------\n    agent_fast_reply: dict\n        An initially empty dict that can be populated with a response.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    --------\n    response : None | dict | AgentOutput\n        If you want to bypass the main agent, return an AgentOutput or a dict with an \"output\" key.\n        Return None to continue with normal execution.\n        See below for examples of Cat response\n\n    Examples\n    --------\n\n    Example 1: don't remember (no uploaded documents about topic)\n    ```python\n    num_declarative_memories = len( cat.working_memory.declarative_memories )\n    if num_declarative_memories == 0:\n        return {\n           \"output\": \"Sorry, I have no memories about that.\"\n        }\n    ```\n    \"\"\"\n\n    return agent_fast_reply\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/agent/#cat.mad_hatter.core_plugin.hooks.agent.before_agent_starts","title":"<code>before_agent_starts(agent_input, cat)</code>","text":"<p>Hook to read and edit the agent input</p> <p>Parameters:</p> Name Type Description Default <code>agent_input</code> <code>Dict</code> <p>Input that is about to be passed to the agent.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>Dict</code> <p>Agent Input</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/agent.py</code> <pre><code>@hook(priority=0)\ndef before_agent_starts(agent_input: Dict, cat) -&gt; Dict:\n    \"\"\"Hook to read and edit the agent input\n\n    Parameters\n    --------\n    agent_input: dict\n        Input that is about to be passed to the agent.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    --------\n    response : Dict\n        Agent Input\n    \"\"\"\n\n    return agent_input\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/","title":"flow","text":"<p>Hooks to modify the Cat's flow of execution.</p> <p>Here is a collection of methods to hook into the Cat execution pipeline.</p>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.after_cat_bootstrap","title":"<code>after_cat_bootstrap(cat)</code>","text":"<p>Hook into the end of the Cat start up.</p> <p>Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories, the Main Agent, the Rabbit Hole and the White Rabbit.</p> <p>This hook allows to intercept the end of such process and is executed right after the Cat has finished loading its components.</p> <p>This can be used to set or store variables to be shared further in the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef after_cat_bootstrap(cat) -&gt; None:\n    \"\"\"Hook into the end of the Cat start up.\n\n    Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories,\n    the *Main Agent*, the *Rabbit Hole* and the *White Rabbit*.\n\n    This hook allows to intercept the end of such process and is executed right after the Cat has finished loading\n    its components.\n\n    This can be used to set or store variables to be shared further in the pipeline.\n\n    Parameters\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n    \"\"\"\n    pass  # do nothing\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.after_cat_recalls_memories","title":"<code>after_cat_recalls_memories(cat)</code>","text":"<p>Hook after semantic search in memories.</p> <p>The hook is executed just after the Cat searches for the meaningful context in memories and stores it in the Working Memory.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef after_cat_recalls_memories(cat) -&gt; None:\n    \"\"\"Hook after semantic search in memories.\n\n    The hook is executed just after the Cat searches for the meaningful context in memories\n    and stores it in the *Working Memory*.\n\n    Parameters\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    \"\"\"\n    pass  # do nothing\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_bootstrap","title":"<code>before_cat_bootstrap(cat)</code>","text":"<p>Hook into the Cat start up.</p> <p>Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories, the Main Agent, the Rabbit Hole and the White Rabbit.</p> <p>This hook allows to intercept such process and is executed in the middle of plugins and natural language objects loading.</p> <p>This hook can be used to set or store variables to be propagated to subsequent loaded objects.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_bootstrap(cat) -&gt; None:\n    \"\"\"Hook into the Cat start up.\n\n    Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories,\n    the *Main Agent*, the *Rabbit Hole* and the *White Rabbit*.\n\n    This hook allows to intercept such process and is executed in the middle of plugins and\n    natural language objects loading.\n\n    This hook can be used to set or store variables to be propagated to subsequent loaded objects.\n\n    Parameters\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n    \"\"\"\n    pass  # do nothing\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_reads_message","title":"<code>before_cat_reads_message(user_message_json, cat)</code>","text":"<p>Hook the incoming user's JSON dictionary.</p> <p>Allows to edit and enrich the incoming message received from the WebSocket connection.</p> <p>For instance, this hook can be used to translate the user's message before feeding it to the Cat. Another use case is to add custom keys to the JSON dictionary.</p> <p>The incoming message is a JSON dictionary with keys:     {         \"text\": message content     }</p> <p>Parameters:</p> Name Type Description Default <code>user_message_json</code> <code>dict</code> <p>JSON dictionary with the message received from the chat.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>user_message_json</code> <code>dict</code> <p>Edited JSON dictionary that will be fed to the Cat.</p> Notes <p>For example:</p> <pre><code>{\n    \"text\": \"Hello Cheshire Cat!\",\n    \"custom_key\": True\n}\n</code></pre> <p>where \"custom_key\" is a newly added key to the dictionary to store any data.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_reads_message(user_message_json: dict, cat) -&gt; dict:\n    \"\"\"Hook the incoming user's JSON dictionary.\n\n    Allows to edit and enrich the incoming message received from the WebSocket connection.\n\n    For instance, this hook can be used to translate the user's message before feeding it to the Cat.\n    Another use case is to add custom keys to the JSON dictionary.\n\n    The incoming message is a JSON dictionary with keys:\n        {\n            \"text\": message content\n        }\n\n    Parameters\n    ----------\n    user_message_json : dict\n        JSON dictionary with the message received from the chat.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n\n    Returns\n    -------\n    user_message_json : dict\n        Edited JSON dictionary that will be fed to the Cat.\n\n    Notes\n    -----\n    For example:\n\n        {\n            \"text\": \"Hello Cheshire Cat!\",\n            \"custom_key\": True\n        }\n\n    where \"custom_key\" is a newly added key to the dictionary to store any data.\n\n    \"\"\"\n    return user_message_json\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_recalls_declarative_memories","title":"<code>before_cat_recalls_declarative_memories(declarative_recall_config, cat)</code>","text":"<p>Hook into semantic search in memories.</p> <p>Allows to intercept when the Cat queries the memories using the embedded user's input.</p> <p>The hook is executed just before the Cat searches for the meaningful context in both memories and stores it in the Working Memory.</p> <p>The hook return the values for maximum number (k) of items to retrieve from memory and the score threshold applied to the query in the vector memory (items with score under threshold are not retrieved) It also returns the embedded query (embedding) and the conditions on recall (metadata).</p> <p>Parameters:</p> Name Type Description Default <code>declarative_recall_config</code> <code>dict</code> <p>Dictionary with data needed to recall declarative memories</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>declarative_recall_config</code> <code>dict</code> <p>Edited dictionary that will be fed to the embedder.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_recalls_declarative_memories(\n    declarative_recall_config: dict, cat\n) -&gt; dict:\n    \"\"\"Hook into semantic search in memories.\n\n    Allows to intercept when the Cat queries the memories using the embedded user's input.\n\n    The hook is executed just before the Cat searches for the meaningful context in both memories\n    and stores it in the *Working Memory*.\n\n    The hook return the values for maximum number (k) of items to retrieve from memory and the score threshold applied\n    to the query in the vector memory (items with score under threshold are not retrieved)\n    It also returns the embedded query (embedding) and the conditions on recall (metadata).\n\n    Parameters\n    ----------\n    declarative_recall_config: dict\n        Dictionary with data needed to recall declarative memories\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    declarative_recall_config: dict\n        Edited dictionary that will be fed to the embedder.\n\n    \"\"\"\n    return declarative_recall_config\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_recalls_episodic_memories","title":"<code>before_cat_recalls_episodic_memories(episodic_recall_config, cat)</code>","text":"<p>Hook into semantic search in memories.</p> <p>Allows to intercept when the Cat queries the memories using the embedded user's input.</p> <p>The hook is executed just before the Cat searches for the meaningful context in both memories and stores it in the Working Memory.</p> <p>The hook return the values for maximum number (k) of items to retrieve from memory and the score threshold applied to the query in the vector memory (items with score under threshold are not retrieved). It also returns the embedded query (embedding) and the conditions on recall (metadata).</p> <p>Parameters:</p> Name Type Description Default <code>episodic_recall_config</code> <code>dict</code> <p>Dictionary with data needed to recall episodic memories</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>episodic_recall_config</code> <code>dict</code> <p>Edited dictionary that will be fed to the embedder.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_recalls_episodic_memories(episodic_recall_config: dict, cat) -&gt; dict:\n    \"\"\"Hook into semantic search in memories.\n\n    Allows to intercept when the Cat queries the memories using the embedded user's input.\n\n    The hook is executed just before the Cat searches for the meaningful context in both memories\n    and stores it in the *Working Memory*.\n\n    The hook return the values for maximum number (k) of items to retrieve from memory and the score threshold applied\n    to the query in the vector memory (items with score under threshold are not retrieved).\n    It also returns the embedded query (embedding) and the conditions on recall (metadata).\n\n    Parameters\n    ----------\n    episodic_recall_config : dict\n        Dictionary with data needed to recall episodic memories\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    episodic_recall_config: dict\n        Edited dictionary that will be fed to the embedder.\n\n    \"\"\"\n    return episodic_recall_config\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_recalls_memories","title":"<code>before_cat_recalls_memories(cat)</code>","text":"<p>Hook into semantic search in memories.</p> <p>Allows to intercept when the Cat queries the memories using the embedded user's input.</p> <p>The hook is executed just before the Cat searches for the meaningful context in both memories and stores it in the Working Memory.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_recalls_memories(cat) -&gt; None:\n    \"\"\"Hook into semantic search in memories.\n\n    Allows to intercept when the Cat queries the memories using the embedded user's input.\n\n    The hook is executed just before the Cat searches for the meaningful context in both memories\n    and stores it in the *Working Memory*.\n\n    Parameters\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    \"\"\"\n    pass  # do nothing\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_recalls_procedural_memories","title":"<code>before_cat_recalls_procedural_memories(procedural_recall_config, cat)</code>","text":"<p>Hook into semantic search in memories.</p> <p>Allows to intercept when the Cat queries the memories using the embedded user's input.</p> <p>The hook is executed just before the Cat searches for the meaningful context in both memories and stores it in the Working Memory.</p> <p>The hook return the values for maximum number (k) of items to retrieve from memory and the score threshold applied to the query in the vector memory (items with score under threshold are not retrieved) It also returns the embedded query (embedding) and the conditions on recall (metadata).</p> <p>Parameters:</p> Name Type Description Default <code>procedural_recall_config</code> <code>dict</code> <p>Dictionary with data needed to recall tools from procedural memory</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>procedural_recall_config</code> <code>dict</code> <p>Edited dictionary that will be fed to the embedder.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_recalls_procedural_memories(procedural_recall_config: dict, cat) -&gt; dict:\n    \"\"\"Hook into semantic search in memories.\n\n    Allows to intercept when the Cat queries the memories using the embedded user's input.\n\n    The hook is executed just before the Cat searches for the meaningful context in both memories\n    and stores it in the *Working Memory*.\n\n    The hook return the values for maximum number (k) of items to retrieve from memory and the score threshold applied\n    to the query in the vector memory (items with score under threshold are not retrieved)\n    It also returns the embedded query (embedding) and the conditions on recall (metadata).\n\n    Parameters\n    ----------\n    procedural_recall_config: dict\n        Dictionary with data needed to recall tools from procedural memory\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    procedural_recall_config: dict\n        Edited dictionary that will be fed to the embedder.\n\n    \"\"\"\n    return procedural_recall_config\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_sends_message","title":"<code>before_cat_sends_message(message, cat)</code>","text":"<p>Hook the outgoing Cat's message.</p> <p>Allows to edit the JSON dictionary that will be sent to the client via WebSocket connection.</p> <p>This hook can be used to edit the message sent to the user or to add keys to the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>dict</code> <p>JSON dictionary to be sent to the WebSocket client.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>message</code> <code>dict</code> <p>Edited JSON dictionary with the Cat's answer.</p> Notes <p>Default <code>message</code> is::</p> <pre><code>    {\n        \"type\": \"chat\",\n        \"content\": cat_message[\"output\"],\n        \"why\": {\n            \"input\": cat_message[\"input\"],\n            \"output\": cat_message[\"output\"],\n            \"intermediate_steps\": cat_message[\"intermediate_steps\"],\n            \"memory\": {\n                \"vectors\": {\n                    \"episodic\": episodic_report,\n                    \"declarative\": declarative_report\n                }\n            },\n        },\n    }\n</code></pre> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_sends_message(message: dict, cat) -&gt; dict:\n    \"\"\"Hook the outgoing Cat's message.\n\n    Allows to edit the JSON dictionary that will be sent to the client via WebSocket connection.\n\n    This hook can be used to edit the message sent to the user or to add keys to the dictionary.\n\n    Parameters\n    ----------\n    message : dict\n        JSON dictionary to be sent to the WebSocket client.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    message : dict\n        Edited JSON dictionary with the Cat's answer.\n\n    Notes\n    -----\n    Default `message` is::\n\n            {\n                \"type\": \"chat\",\n                \"content\": cat_message[\"output\"],\n                \"why\": {\n                    \"input\": cat_message[\"input\"],\n                    \"output\": cat_message[\"output\"],\n                    \"intermediate_steps\": cat_message[\"intermediate_steps\"],\n                    \"memory\": {\n                        \"vectors\": {\n                            \"episodic\": episodic_report,\n                            \"declarative\": declarative_report\n                        }\n                    },\n                },\n            }\n\n    \"\"\"\n\n    return message\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_stores_episodic_memory","title":"<code>before_cat_stores_episodic_memory(doc, cat)</code>","text":"<p>Hook the user message <code>Document</code> before is inserted in the vector memory.</p> <p>Allows editing and enhancing a single <code>Document</code> before the Cat add it to the episodic vector memory.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Document</code> <p>Langchain <code>Document</code> to be inserted in memory.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>doc</code> <code>Document</code> <p>Langchain <code>Document</code> that is added in the episodic vector memory.</p> Notes <p>The <code>Document</code> has two properties::</p> <pre><code>`page_content`: the string with the text to save in memory;\n`metadata`: a dictionary with at least two keys:\n    `source`: where the text comes from;\n    `when`: timestamp to track when it's been uploaded.\n</code></pre> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_stores_episodic_memory(doc: Document, cat) -&gt; Document:\n    \"\"\"Hook the user message `Document` before is inserted in the vector memory.\n\n    Allows editing and enhancing a single `Document` before the Cat add it to the episodic vector memory.\n\n    Parameters\n    ----------\n    doc : Document\n        Langchain `Document` to be inserted in memory.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    doc : Document\n        Langchain `Document` that is added in the episodic vector memory.\n\n    Notes\n    -----\n    The `Document` has two properties::\n\n        `page_content`: the string with the text to save in memory;\n        `metadata`: a dictionary with at least two keys:\n            `source`: where the text comes from;\n            `when`: timestamp to track when it's been uploaded.\n\n    \"\"\"\n    return doc\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.cat_recall_query","title":"<code>cat_recall_query(user_message, cat)</code>","text":"<p>Hook the semantic search query.</p> <p>This hook allows to edit the user's message used as a query for context retrieval from memories. As a result, the retrieved context can be conditioned editing the user's message.</p> <p>Parameters:</p> Name Type Description Default <code>user_message</code> <code>str</code> <p>String with the text received from the user.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance to exploit the Cat's methods.</p> required <p>Returns:</p> Type Description <code>Edited string to be used for context retrieval in memory. The returned string is further stored in the</code> <code>Working Memory at `cat.working_memory.recall_query`.</code> Notes <p>For example, this hook is a suitable to perform Hypothetical Document Embedding (HyDE). HyDE [1]_ strategy exploits the user's message to generate a hypothetical answer. This is then used to recall the relevant context from the memory. An official plugin is available to test this technique.</p> References <p>[1] Gao, L., Ma, X., Lin, J., &amp; Callan, J. (2022). Precise Zero-Shot Dense Retrieval without Relevance Labels.    arXiv preprint arXiv:2212.10496.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef cat_recall_query(user_message: str, cat) -&gt; str:\n    \"\"\"Hook the semantic search query.\n\n    This hook allows to edit the user's message used as a query for context retrieval from memories.\n    As a result, the retrieved context can be conditioned editing the user's message.\n\n    Parameters\n    ----------\n    user_message : str\n        String with the text received from the user.\n    cat : CheshireCat\n        Cheshire Cat instance to exploit the Cat's methods.\n\n    Returns\n    -------\n    Edited string to be used for context retrieval in memory. The returned string is further stored in the\n    Working Memory at `cat.working_memory.recall_query`.\n\n    Notes\n    -----\n    For example, this hook is a suitable to perform Hypothetical Document Embedding (HyDE).\n    HyDE [1]_ strategy exploits the user's message to generate a hypothetical answer. This is then used to recall\n    the relevant context from the memory.\n    An official plugin is available to test this technique.\n\n    References\n    ----------\n    [1] Gao, L., Ma, X., Lin, J., &amp; Callan, J. (2022). Precise Zero-Shot Dense Retrieval without Relevance Labels.\n       arXiv preprint arXiv:2212.10496.\n\n    \"\"\"\n\n    # here we just return the latest user message as is\n    return user_message\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.fast_reply","title":"<code>fast_reply(fast_reply, cat)</code>","text":"<p>This hook allows for an immediate response, bypassing memory recall and agent execution. It's useful for canned replies, custom LLM chains / agents, topic evaluation, direct LLM interaction and so on.</p> <p>Parameters:</p> Name Type Description Default <code>fast_reply</code> <code>dict</code> <p>An initially empty dict that can be populated with a response.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>None | dict | CatMessage</code> <p>If you want to short-circuit the normal flow, return a CatMessage or a dict with an \"output\" key. Return None or an empty dict to continue with normal execution. See below for examples of Cat response</p> <p>Examples:</p> <p>Example 1: can't talk about this topic </p><pre><code># here you could use cat._llm to do topic evaluation\nif \"dog\" in cat.working_memory.user_message_json.text:\n    return {\n        \"output\": \"You went out of topic. Can't talk about dog.\"\n    }\n</code></pre> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef fast_reply(fast_reply: dict, cat) -&gt; None | dict | CatMessage:\n    \"\"\"This hook allows for an immediate response, bypassing memory recall and agent execution.\n    It's useful for canned replies, custom LLM chains / agents, topic evaluation, direct LLM interaction and so on.\n\n    Parameters\n    --------\n    fast_reply: dict\n        An initially empty dict that can be populated with a response.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    --------\n    response : None | dict | CatMessage\n        If you want to short-circuit the normal flow, return a CatMessage or a dict with an \"output\" key.\n        Return None or an empty dict to continue with normal execution.\n        See below for examples of Cat response\n\n    Examples\n    --------\n\n    Example 1: can't talk about this topic\n    ```python\n    # here you could use cat._llm to do topic evaluation\n    if \"dog\" in cat.working_memory.user_message_json.text:\n        return {\n            \"output\": \"You went out of topic. Can't talk about dog.\"\n        }\n    ```\n    \"\"\"\n\n    return fast_reply\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/prompt/","title":"prompt","text":"<p>Hooks to modify the prompts.</p> <p>Here is a collection of methods to hook the prompts components that instruct the Agent.</p>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_instructions","title":"<code>agent_prompt_instructions(instructions, cat)</code>","text":"<p>Hook the instruction prompt.</p> <p>Allows to edit the instructions that the Cat feeds to the Agent to select tools and forms.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>Instructions prompt to select tool or form.</p> required <code>cat</code> <code>StrayCat</code> <p>StrayCat instance.</p> required <p>Returns:</p> Name Type Description <code>instructions</code> <code>str</code> <p>Instructions prompt to select tool or form</p> Notes <p>This prompt explains the Agent how to select a tool or form.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/prompt.py</code> <pre><code>@hook(priority=0)\ndef agent_prompt_instructions(instructions: str, cat) -&gt; str:\n    \"\"\"Hook the instruction prompt.\n\n    Allows to edit the instructions that the Cat feeds to the *Agent* to select tools and forms.\n\n    Parameters\n    ----------\n    instructions : str\n        Instructions prompt to select tool or form.\n    cat : StrayCat\n        StrayCat instance.\n\n    Returns\n    -------\n    instructions : str\n        Instructions prompt to select tool or form\n\n    Notes\n    -----\n    This prompt explains the *Agent* how to select a tool or form.\n\n    \"\"\"\n\n    return instructions\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_prefix","title":"<code>agent_prompt_prefix(prefix, cat)</code>","text":"<p>Hook the main prompt prefix.</p> <p>Allows to edit the prefix of the Main Prompt that the Cat feeds to the Agent. It describes the personality of your assistant and its general task.</p> <p>The prefix is then completed with the <code>agent_prompt_suffix</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Main / System prompt with personality and general task to be accomplished.</p> required <code>cat</code> <code>StrayCat</code> <p>StrayCat instance.</p> required <p>Returns:</p> Name Type Description <code>prefix</code> <code>str</code> <p>Main / System prompt.</p> Notes <p>The default prefix describe who the AI is and how it is expected to answer the Human.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/prompt.py</code> <pre><code>@hook(priority=0)\ndef agent_prompt_prefix(prefix, cat) -&gt; str:\n    \"\"\"Hook the main prompt prefix.\n\n    Allows to edit the prefix of the *Main Prompt* that the Cat feeds to the *Agent*.\n    It describes the personality of your assistant and its general task.\n\n    The prefix is then completed with the `agent_prompt_suffix`.\n\n    Parameters\n    ----------\n    prefix : str\n        Main / System prompt with personality and general task to be accomplished.\n    cat : StrayCat\n        StrayCat instance.\n\n    Returns\n    -------\n    prefix : str\n        Main / System prompt.\n\n    Notes\n    -----\n    The default prefix describe who the AI is and how it is expected to answer the Human.\n    \"\"\"\n\n    return prefix\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_suffix","title":"<code>agent_prompt_suffix(prompt_suffix, cat)</code>","text":"<p>Hook the main prompt suffix.</p> <p>Allows to edit the suffix of the Main Prompt that the Cat feeds to the Agent.</p> <p>The suffix is concatenated to <code>agent_prompt_prefix</code> when RAG context is used.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>StrayCat</code> <p>StrayCat instance.</p> required <p>Returns:</p> Name Type Description <code>prompt_suffix</code> <code>str</code> <p>The suffix string to be concatenated to the Main Prompt (prefix).</p> Notes <p>The default suffix has a few placeholders: - {episodic_memory} provides memories retrieved from episodic memory (past conversations) - {declarative_memory} provides memories retrieved from declarative memory (uploaded documents) - {chat_history} provides the Agent the recent conversation history - {input} provides the last user's input - {agent_scratchpad} is where the Agent can concatenate tools use and multiple calls to the LLM.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/prompt.py</code> <pre><code>@hook(priority=0)\ndef agent_prompt_suffix(prompt_suffix: str, cat) -&gt; str:\n    \"\"\"Hook the main prompt suffix.\n\n    Allows to edit the suffix of the *Main Prompt* that the Cat feeds to the *Agent*.\n\n    The suffix is concatenated to `agent_prompt_prefix` when RAG context is used.\n\n    Parameters\n    ----------\n    cat : StrayCat\n        StrayCat instance.\n\n    Returns\n    -------\n    prompt_suffix : str\n        The suffix string to be concatenated to the *Main Prompt* (prefix).\n\n    Notes\n    -----\n    The default suffix has a few placeholders:\n    - {episodic_memory} provides memories retrieved from *episodic* memory (past conversations)\n    - {declarative_memory} provides memories retrieved from *declarative* memory (uploaded documents)\n    - {chat_history} provides the *Agent* the recent conversation history\n    - {input} provides the last user's input\n    - {agent_scratchpad} is where the *Agent* can concatenate tools use and multiple calls to the LLM.\n\n    \"\"\"\n\n    return prompt_suffix\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/","title":"rabbithole","text":"<p>Hooks to modify the RabbitHole's documents ingestion.</p> <p>Here is a collection of methods to hook into the RabbitHole execution pipeline.</p> <p>These hooks allow to intercept the uploaded documents at different places before they are saved into memory.</p>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.after_rabbithole_splitted_text","title":"<code>after_rabbithole_splitted_text(chunks, cat)</code>","text":"<p>Hook the <code>Document</code> after is split.</p> <p>Allows editing the list of <code>Document</code> right after the RabbitHole chunked them in smaller ones.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code>.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>chunks</code> <code>List[Document]</code> <p>List of modified chunked langchain documents to be stored in the episodic memory.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef after_rabbithole_splitted_text(chunks: List[Document], cat) -&gt; List[Document]:\n    \"\"\"Hook the `Document` after is split.\n\n    Allows editing the list of `Document` right after the *RabbitHole* chunked them in smaller ones.\n\n    Parameters\n    ----------\n    chunks : List[Document]\n        List of Langchain `Document`.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    chunks : List[Document]\n        List of modified chunked langchain documents to be stored in the episodic memory.\n\n    \"\"\"\n\n    return chunks\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.after_rabbithole_stored_documents","title":"<code>after_rabbithole_stored_documents(source, stored_points, cat)</code>","text":"<p>Hook the Document after is inserted in the vector memory.</p> <p>Allows editing and enhancing the list of Document after is inserted in the vector memory.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>Name of ingested file/url</p> required <code>docs</code> <code>List[PointStruct]</code> <p>List of Qdrant PointStruct just inserted into the db.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef after_rabbithole_stored_documents(\n    source, stored_points: List[PointStruct], cat\n) -&gt; None:\n    \"\"\"Hook the Document after is inserted in the vector memory.\n\n    Allows editing and enhancing the list of Document after is inserted in the vector memory.\n\n    Parameters\n    ----------\n    source: str\n        Name of ingested file/url\n    docs : List[PointStruct]\n        List of Qdrant PointStruct just inserted into the db.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.before_rabbithole_insert_memory","title":"<code>before_rabbithole_insert_memory(doc, cat)</code>","text":"<p>Hook the <code>Document</code> before is inserted in the vector memory.</p> <p>Allows editing and enhancing a single <code>Document</code> before the RabbitHole add it to the declarative vector memory.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Document</code> <p>Langchain <code>Document</code> to be inserted in memory.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>doc</code> <code>Document</code> <p>Langchain <code>Document</code> that is added in the declarative vector memory.</p> Notes <p>The <code>Document</code> has two properties::</p> <pre><code>`page_content`: the string with the text to save in memory;\n`metadata`: a dictionary with at least two keys:\n    `source`: where the text comes from;\n    `when`: timestamp to track when it's been uploaded.\n</code></pre> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef before_rabbithole_insert_memory(doc: Document, cat) -&gt; Document:\n    \"\"\"Hook the `Document` before is inserted in the vector memory.\n\n    Allows editing and enhancing a single `Document` before the *RabbitHole* add it to the declarative vector memory.\n\n    Parameters\n    ----------\n    doc : Document\n        Langchain `Document` to be inserted in memory.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    doc : Document\n        Langchain `Document` that is added in the declarative vector memory.\n\n    Notes\n    -----\n    The `Document` has two properties::\n\n        `page_content`: the string with the text to save in memory;\n        `metadata`: a dictionary with at least two keys:\n            `source`: where the text comes from;\n            `when`: timestamp to track when it's been uploaded.\n\n    \"\"\"\n    return doc\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.before_rabbithole_splits_text","title":"<code>before_rabbithole_splits_text(docs, cat)</code>","text":"<p>Hook the <code>Documents</code> before they are split into chunks.</p> <p>Allows editing the uploaded document main Document(s) before the RabbitHole recursively splits it in shorter ones. Please note that this is a list because parsers can output one or more Document, that are afterwards splitted.</p> <p>For instance, the hook allows to change the text or edit/add metadata.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>Langchain <code>Document</code>s resulted after parsing the file uploaded in the RabbitHole.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>docs</code> <code>List[Document]</code> <p>Edited Langchain <code>Document</code>s.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef before_rabbithole_splits_text(docs: List[Document], cat) -&gt; List[Document]:\n    \"\"\"Hook the `Documents` before they are split into chunks.\n\n    Allows editing the uploaded document main Document(s) before the *RabbitHole* recursively splits it in shorter ones.\n    Please note that this is a list because parsers can output one or more Document, that are afterwards splitted.\n\n    For instance, the hook allows to change the text or edit/add metadata.\n\n    Parameters\n    ----------\n    docs : List[Document]\n        Langchain `Document`s resulted after parsing the file uploaded in the *RabbitHole*.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    docs : List[Document]\n        Edited Langchain `Document`s.\n\n    \"\"\"\n\n    return docs\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.before_rabbithole_stores_documents","title":"<code>before_rabbithole_stores_documents(docs, cat)</code>","text":"<p>Hook into the memory insertion pipeline.</p> <p>Allows modifying how the list of <code>Document</code> is inserted in the vector memory.</p> <p>For example, this hook is a good point to summarize the incoming documents and save both original and summarized contents. An official plugin is available to test this procedure.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code> to be edited.</p> required <code>cat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>docs</code> <code>List[Document]</code> <p>List of edited Langchain documents.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef before_rabbithole_stores_documents(docs: List[Document], cat) -&gt; List[Document]:\n    \"\"\"Hook into the memory insertion pipeline.\n\n    Allows modifying how the list of `Document` is inserted in the vector memory.\n\n    For example, this hook is a good point to summarize the incoming documents and save both original and\n    summarized contents.\n    An official plugin is available to test this procedure.\n\n    Parameters\n    ----------\n    docs : List[Document]\n        List of Langchain `Document` to be edited.\n    cat: CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    docs : List[Document]\n        List of edited Langchain documents.\n\n    \"\"\"\n\n    return docs\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.rabbithole_instantiates_parsers","title":"<code>rabbithole_instantiates_parsers(file_handlers, cat)</code>","text":"<p>Hook the available parsers for ingesting files in the declarative memory.</p> <p>Allows replacing or extending existing supported mime types and related parsers to customize the file ingestion.</p> <p>Parameters:</p> Name Type Description Default <code>file_handlers</code> <code>dict</code> <p>Keys are the supported mime types and values are the related parsers.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>file_handlers</code> <code>dict</code> <p>Edited dictionary of supported mime types and related parsers.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef rabbithole_instantiates_parsers(file_handlers: dict, cat) -&gt; dict:\n    \"\"\"Hook the available parsers for ingesting files in the declarative memory.\n\n    Allows replacing or extending existing supported mime types and related parsers to customize the file ingestion.\n\n    Parameters\n    ----------\n    file_handlers : dict\n        Keys are the supported mime types and values are the related parsers.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    file_handlers : dict\n        Edited dictionary of supported mime types and related parsers.\n    \"\"\"\n    return file_handlers\n</code></pre>"},{"location":"API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.rabbithole_instantiates_splitter","title":"<code>rabbithole_instantiates_splitter(text_splitter, cat)</code>","text":"<p>Hook the splitter used to split text in chunks.</p> <p>Allows replacing the default text splitter to customize the splitting process.</p> <p>Parameters:</p> Name Type Description Default <code>text_splitter</code> <code>TextSplitter</code> <p>The text splitter used by default.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>text_splitter</code> <code>TextSplitter</code> <p>An instance of a TextSplitter subclass.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef rabbithole_instantiates_splitter(text_splitter: TextSplitter, cat) -&gt; TextSplitter:\n    \"\"\"Hook the splitter used to split text in chunks.\n\n    Allows replacing the default text splitter to customize the splitting process.\n\n    Parameters\n    ----------\n    text_splitter : TextSplitter\n        The text splitter used by default.\n    cat : CheshireCat\n        Cheshire Cat instance.\n\n    Returns\n    -------\n    text_splitter : TextSplitter\n        An instance of a TextSplitter subclass.\n    \"\"\"\n\n    # example on how to change chunking\n    # text_splitter._chunk_size = 64\n    # text_splitter._chunk_overlap = 8\n\n    return text_splitter\n</code></pre>"},{"location":"API_Documentation/memory/vector_memory/","title":"vector_memory","text":""},{"location":"API_Documentation/memory/vector_memory/#cat.memory.vector_memory.VectorMemory","title":"<code>VectorMemory</code>","text":"Source code in <code>cat/memory/vector_memory.py</code> <pre><code>class VectorMemory:\n    local_vector_db = None\n\n    def __init__(\n        self,\n        embedder_name=None,\n        embedder_size=None,\n    ) -&gt; None:\n        # connects to Qdrant and creates self.vector_db attribute\n        self.connect_to_vector_memory()\n\n        # Create vector collections\n        # - Episodic memory will contain user and eventually cat utterances\n        # - Declarative memory will contain uploaded documents' content\n        # - Procedural memory will contain tools and knowledge on how to do things\n        self.collections = {}\n        for collection_name in [\"episodic\", \"declarative\", \"procedural\"]:\n            # Instantiate collection\n            collection = VectorMemoryCollection(\n                client=self.vector_db,\n                collection_name=collection_name,\n                embedder_name=embedder_name,\n                embedder_size=embedder_size,\n            )\n\n            # Update dictionary containing all collections\n            # Useful for cross-searching and to create/use collections from plugins\n            self.collections[collection_name] = collection\n\n            # Have the collection as an instance attribute\n            # (i.e. do things like cat.memory.vectors.declarative.something())\n            setattr(self, collection_name, collection)\n\n    def connect_to_vector_memory(self) -&gt; None:\n        db_path = \"cat/data/local_vector_memory/\"\n        qdrant_host = get_env(\"CCAT_QDRANT_HOST\")\n\n        if not qdrant_host:\n            log.debug(f\"Qdrant path: {db_path}\")\n            # Qdrant local vector DB client\n\n            # reconnect only if it's the first boot and not a reload\n            if VectorMemory.local_vector_db is None:\n                VectorMemory.local_vector_db = QdrantClient(\n                    path=db_path, force_disable_check_same_thread=True\n                )\n\n            self.vector_db = VectorMemory.local_vector_db\n        else:\n            # Qdrant remote or in other container\n            qdrant_port = int(get_env(\"CCAT_QDRANT_PORT\"))\n            qdrant_https = is_https(qdrant_host)\n            qdrant_host = extract_domain_from_url(qdrant_host)\n            qdrant_api_key = get_env(\"CCAT_QDRANT_API_KEY\")\n\n            qdrant_client_timeout = get_env(\"CCAT_QDRANT_CLIENT_TIMEOUT\")\n            qdrant_client_timeout = int(qdrant_client_timeout) if qdrant_client_timeout is not None else None\n\n            try:\n                s = socket.socket()\n                s.connect((qdrant_host, qdrant_port))\n            except Exception:\n                log.error(f\"QDrant does not respond to {qdrant_host}:{qdrant_port}\")\n                sys.exit()\n            finally:\n                s.close()\n\n            # Qdrant vector DB client\n            self.vector_db = QdrantClient(\n                host=qdrant_host,\n                port=qdrant_port,\n                https=qdrant_https,\n                api_key=qdrant_api_key,\n                timeout=qdrant_client_timeout\n            )\n\n    def delete_collection(self, collection_name: str):\n        \"\"\"Delete specific vector collection\"\"\"\n\n        return self.vector_db.delete_collection(collection_name)\n\n    def get_collection(self, collection_name: str):\n        \"\"\"Get collection info\"\"\"\n\n        return self.vector_db.get_collection(collection_name)\n</code></pre>"},{"location":"API_Documentation/memory/vector_memory/#cat.memory.vector_memory.VectorMemory.delete_collection","title":"<code>delete_collection(collection_name)</code>","text":"<p>Delete specific vector collection</p> Source code in <code>cat/memory/vector_memory.py</code> <pre><code>def delete_collection(self, collection_name: str):\n    \"\"\"Delete specific vector collection\"\"\"\n\n    return self.vector_db.delete_collection(collection_name)\n</code></pre>"},{"location":"API_Documentation/memory/vector_memory/#cat.memory.vector_memory.VectorMemory.get_collection","title":"<code>get_collection(collection_name)</code>","text":"<p>Get collection info</p> Source code in <code>cat/memory/vector_memory.py</code> <pre><code>def get_collection(self, collection_name: str):\n    \"\"\"Get collection info\"\"\"\n\n    return self.vector_db.get_collection(collection_name)\n</code></pre>"},{"location":"API_Documentation/memory/vector_memory_collection/","title":"vector_memory_collection","text":""},{"location":"API_Documentation/memory/vector_memory_collection/#cat.memory.vector_memory_collection.VectorMemoryCollection","title":"<code>VectorMemoryCollection</code>","text":"Source code in <code>cat/memory/vector_memory_collection.py</code> <pre><code>class VectorMemoryCollection:\n    def __init__(\n        self,\n        client: Any,\n        collection_name: str,\n        embedder_name: str,\n        embedder_size: int,\n    ):\n        # Set attributes (metadata on the embedder are useful because it may change at runtime)\n        self.client = client\n        self.collection_name = collection_name\n        self.embedder_name = embedder_name\n        self.embedder_size = embedder_size\n\n        # Check if memory collection exists also in vectorDB, otherwise create it\n        self.create_db_collection_if_not_exists()\n\n        # Check db collection vector size is same as embedder size\n        self.check_embedding_size()\n\n        # log collection info\n        log.debug(f\"Collection {self.collection_name}:\")\n        log.debug(self.client.get_collection(self.collection_name))\n\n    def check_embedding_size(self):\n        # having the same size does not necessarily imply being the same embedder\n        # having vectors with the same size but from diffent embedder in the same vector space is wrong\n        same_size = (\n            self.client.get_collection(self.collection_name).config.params.vectors.size\n            == self.embedder_size\n        )\n        alias = self.embedder_name + \"_\" + self.collection_name\n\n        existing_aliases = self.client.get_collection_aliases(self.collection_name).aliases\n\n        if ( len(existing_aliases) &gt; 0 and\n            alias == existing_aliases[0].alias_name\n            and same_size\n        ):\n            log.debug(f'Collection \"{self.collection_name}\" has the same embedder')\n        else:\n            log.warning(f'Collection \"{self.collection_name}\" has a different embedder')\n            # Memory snapshot saving can be turned off in the .env file with:\n            # SAVE_MEMORY_SNAPSHOTS=false\n            if get_env(\"CCAT_SAVE_MEMORY_SNAPSHOTS\") == \"true\":\n                # dump collection on disk before deleting\n                self.save_dump()\n\n            self.client.delete_collection(self.collection_name)\n            log.warning(f'Collection \"{self.collection_name}\" deleted')\n            self.create_collection()\n\n    def create_db_collection_if_not_exists(self):\n        # is collection present in DB?\n        collections_response = self.client.get_collections()\n        for c in collections_response.collections:\n            if c.name == self.collection_name:\n                # collection exists. Do nothing\n                log.debug(\n                    f'Collection \"{self.collection_name}\" already present in vector store'\n                )\n                return\n\n        self.create_collection()\n\n    # create collection\n    def create_collection(self):\n        try:\n            log.warning(f'Creating collection \"{self.collection_name}\" ...')\n            self.client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(\n                    size=self.embedder_size, distance=Distance.COSINE\n                ),\n                # hybrid mode: original vector on Disk, quantized vector in RAM\n                optimizers_config=OptimizersConfigDiff(memmap_threshold=20000),\n                quantization_config=ScalarQuantization(\n                    scalar=ScalarQuantizationConfig(\n                        type=ScalarType.INT8, quantile=0.95, always_ram=True\n                    )\n                ),\n            )\n        except Exception as e:\n            log.error(f\"Error creating collection {self.collection_name}. Try setting a higher timeout value in CCAT_QDRANT_CLIENT_TIMEOUT: {e}\")\n            self.client.delete_collection(self.collection_name)\n            raise\n\n        try:\n            alias_name=self.embedder_name + \"_\" + self.collection_name\n            log.warning(f'Creating alias {alias_name} for collection \"{self.collection_name}\" ...')\n\n            self.client.update_collection_aliases(\n                change_aliases_operations=[\n                    CreateAliasOperation(\n                        create_alias=CreateAlias(\n                            collection_name=self.collection_name,\n                            alias_name=alias_name,\n                        )\n                    )\n                ]\n            )\n\n            log.warning(f'Created alias {alias_name} for collection \"{self.collection_name}\" ...')\n        except Exception as e:\n            log.error(f\"Error creating collection alias {alias_name} for collection {self.collection_name}: {e}\")\n            self.client.delete_collection(self.collection_name)\n            log.error(f\" collection {self.collection_name} deleted\")\n            raise\n\n\n    # adapted from https://github.com/langchain-ai/langchain/blob/bfc12a4a7644cfc4d832cc4023086a7a5374f46a/libs/langchain/langchain/vectorstores/qdrant.py#L1965\n    def _qdrant_filter_from_dict(self, filter: dict) -&gt; Filter:\n        if not filter or len(filter)&lt;1:\n            return None\n\n        return Filter(\n            must=[\n                condition\n                for key, value in filter.items()\n                for condition in self._build_condition(key, value)\n            ]\n        )\n\n    # adapted from https://github.com/langchain-ai/langchain/blob/bfc12a4a7644cfc4d832cc4023086a7a5374f46a/libs/langchain/langchain/vectorstores/qdrant.py#L1941\n    def _build_condition(self, key: str, value: Any) -&gt; List[FieldCondition]:\n        out = []\n\n        if isinstance(value, dict):\n            for _key, value in value.items():\n                out.extend(self._build_condition(f\"{key}.{_key}\", value))\n        elif isinstance(value, list):\n            for _value in value:\n                if isinstance(_value, dict):\n                    out.extend(self._build_condition(f\"{key}[]\", _value))\n                else:\n                    out.extend(self._build_condition(f\"{key}\", _value))\n        else:\n            out.append(\n                FieldCondition(\n                    key=f\"metadata.{key}\",\n                    match=MatchValue(value=value),\n                )\n            )\n\n        return out\n\n    def add_point(\n        self,\n        content: str,\n        vector: Iterable,\n        metadata: dict = None,\n        id: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; List[str]:\n        \"\"\"Add a point (and its metadata) to the vectorstore.\n\n        Args:\n            content: original text.\n            vector: Embedding vector.\n            metadata: Optional metadata dict associated with the text.\n            id:\n                Optional id to associate with the point. Id has to be a uuid-like string.\n\n        Returns:\n            Point id as saved into the vectorstore.\n        \"\"\"\n\n        # TODO: may be adapted to upload batches of points as langchain does.\n        # Not necessary now as the bottleneck is the embedder\n        point = PointStruct(\n            id=id or uuid.uuid4().hex,\n            payload={\n                \"page_content\": content,\n                \"metadata\": metadata,\n            },\n            vector=vector,\n        )\n\n        update_status = self.client.upsert(\n            collection_name=self.collection_name, points=[point], **kwargs\n        )\n\n        if update_status.status == \"completed\":\n            # returnign stored point\n            return point # TODOV2 return internal MemoryPoint\n        else:\n            return None\n\n    def delete_points_by_metadata_filter(self, metadata=None):\n        res = self.client.delete(\n            collection_name=self.collection_name,\n            points_selector=self._qdrant_filter_from_dict(metadata),\n        )\n        return res\n\n    def delete_points(self, points_ids):\n        \"\"\"Delete point in collection\"\"\"\n        res = self.client.delete(\n            collection_name=self.collection_name,\n            points_selector=points_ids,\n        )\n        return res\n\n    def recall_memories_from_embedding(\n        self, embedding, metadata=None, k=5, threshold=None\n    ):\n        \"\"\"Retrieve similar memories from embedding\"\"\"\n\n        memories = self.client.query_points(\n            collection_name=self.collection_name,\n            query=embedding,\n            query_filter=self._qdrant_filter_from_dict(metadata),\n            with_payload=True,\n            with_vectors=True,\n            limit=k,\n            score_threshold=threshold,\n            search_params=SearchParams(\n                quantization=QuantizationSearchParams(\n                    ignore=False,\n                    rescore=True,\n                    oversampling=2.0,  # Available as of v1.3.0\n                )\n            ),\n        ).points\n\n        # convert Qdrant points to langchain.Document\n        langchain_documents_from_points = []\n        for m in memories:\n            langchain_documents_from_points.append(\n                (\n                    Document(\n                        page_content=m.payload.get(\"page_content\"),\n                        metadata=m.payload.get(\"metadata\") or {},\n                    ),\n                    m.score,\n                    m.vector,\n                    m.id,\n                )\n            )\n\n        # we'll move out of langchain conventions soon and have our own cat Document\n        # for doc, score, vector in langchain_documents_from_points:\n        #    doc.lc_kwargs = None\n\n        return langchain_documents_from_points\n\n    def get_points(self, ids: List[str]):\n        \"\"\"Get points by their ids.\"\"\"\n        return self.client.retrieve(\n            collection_name=self.collection_name,\n            ids=ids,\n            with_vectors=True,\n        )\n\n    def get_all_points(\n            self,\n            limit: int = 10000,\n            offset: str | None = None\n        ):\n        \"\"\"Retrieve all the points in the collection with an optional offset and limit.\"\"\"\n\n        # retrieving the points\n        all_points, next_page_offset = self.client.scroll(\n            collection_name=self.collection_name,\n            with_vectors=True,\n            offset=offset,  # Start from the given offset, or the beginning if None.\n            limit=limit # Limit the number of points retrieved to the specified limit.\n        )\n\n        return all_points, next_page_offset\n\n    def db_is_remote(self):\n        return isinstance(self.client._client, QdrantRemote)\n\n    # dump collection on disk before deleting\n    def save_dump(self, folder=\"dormouse/\"):\n        # only do snapshotting if using remote Qdrant\n        if not self.db_is_remote():\n            return\n\n        host = self.client._client._host\n        port = self.client._client._port\n\n        if os.path.isdir(folder):\n            log.debug(\"Directory dormouse exists\")\n        else:\n            log.info(\"Directory dormouse does NOT exists, creating it.\")\n            os.mkdir(folder)\n\n        self.snapshot_info = self.client.create_snapshot(\n            collection_name=self.collection_name\n        )\n        snapshot_url_in = (\n            \"http://\"\n            + str(host)\n            + \":\"\n            + str(port)\n            + \"/collections/\"\n            + self.collection_name\n            + \"/snapshots/\"\n            + self.snapshot_info.name\n        )\n        snapshot_url_out = folder + self.snapshot_info.name\n        # rename snapshots for a easyer restore in the future\n        alias = (\n            self.client.get_collection_aliases(self.collection_name)\n            .aliases[0]\n            .alias_name\n        )\n        response = requests.get(snapshot_url_in)\n        open(snapshot_url_out, \"wb\").write(response.content)\n        new_name = folder + alias.replace(\"/\", \"-\") + \".snapshot\"\n        os.rename(snapshot_url_out, new_name)\n        for s in self.client.list_snapshots(self.collection_name):\n            self.client.delete_snapshot(\n                collection_name=self.collection_name, snapshot_name=s.name\n            )\n        log.warning(f'Dump \"{new_name}\" completed')\n</code></pre>"},{"location":"API_Documentation/memory/vector_memory_collection/#cat.memory.vector_memory_collection.VectorMemoryCollection.add_point","title":"<code>add_point(content, vector, metadata=None, id=None, **kwargs)</code>","text":"<p>Add a point (and its metadata) to the vectorstore.</p> <p>Args:     content: original text.     vector: Embedding vector.     metadata: Optional metadata dict associated with the text.     id:         Optional id to associate with the point. Id has to be a uuid-like string.</p> <p>Returns:     Point id as saved into the vectorstore.</p> Source code in <code>cat/memory/vector_memory_collection.py</code> <pre><code>def add_point(\n    self,\n    content: str,\n    vector: Iterable,\n    metadata: dict = None,\n    id: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; List[str]:\n    \"\"\"Add a point (and its metadata) to the vectorstore.\n\n    Args:\n        content: original text.\n        vector: Embedding vector.\n        metadata: Optional metadata dict associated with the text.\n        id:\n            Optional id to associate with the point. Id has to be a uuid-like string.\n\n    Returns:\n        Point id as saved into the vectorstore.\n    \"\"\"\n\n    # TODO: may be adapted to upload batches of points as langchain does.\n    # Not necessary now as the bottleneck is the embedder\n    point = PointStruct(\n        id=id or uuid.uuid4().hex,\n        payload={\n            \"page_content\": content,\n            \"metadata\": metadata,\n        },\n        vector=vector,\n    )\n\n    update_status = self.client.upsert(\n        collection_name=self.collection_name, points=[point], **kwargs\n    )\n\n    if update_status.status == \"completed\":\n        # returnign stored point\n        return point # TODOV2 return internal MemoryPoint\n    else:\n        return None\n</code></pre>"},{"location":"API_Documentation/memory/vector_memory_collection/#cat.memory.vector_memory_collection.VectorMemoryCollection.delete_points","title":"<code>delete_points(points_ids)</code>","text":"<p>Delete point in collection</p> Source code in <code>cat/memory/vector_memory_collection.py</code> <pre><code>def delete_points(self, points_ids):\n    \"\"\"Delete point in collection\"\"\"\n    res = self.client.delete(\n        collection_name=self.collection_name,\n        points_selector=points_ids,\n    )\n    return res\n</code></pre>"},{"location":"API_Documentation/memory/vector_memory_collection/#cat.memory.vector_memory_collection.VectorMemoryCollection.get_all_points","title":"<code>get_all_points(limit=10000, offset=None)</code>","text":"<p>Retrieve all the points in the collection with an optional offset and limit.</p> Source code in <code>cat/memory/vector_memory_collection.py</code> <pre><code>def get_all_points(\n        self,\n        limit: int = 10000,\n        offset: str | None = None\n    ):\n    \"\"\"Retrieve all the points in the collection with an optional offset and limit.\"\"\"\n\n    # retrieving the points\n    all_points, next_page_offset = self.client.scroll(\n        collection_name=self.collection_name,\n        with_vectors=True,\n        offset=offset,  # Start from the given offset, or the beginning if None.\n        limit=limit # Limit the number of points retrieved to the specified limit.\n    )\n\n    return all_points, next_page_offset\n</code></pre>"},{"location":"API_Documentation/memory/vector_memory_collection/#cat.memory.vector_memory_collection.VectorMemoryCollection.get_points","title":"<code>get_points(ids)</code>","text":"<p>Get points by their ids.</p> Source code in <code>cat/memory/vector_memory_collection.py</code> <pre><code>def get_points(self, ids: List[str]):\n    \"\"\"Get points by their ids.\"\"\"\n    return self.client.retrieve(\n        collection_name=self.collection_name,\n        ids=ids,\n        with_vectors=True,\n    )\n</code></pre>"},{"location":"API_Documentation/memory/vector_memory_collection/#cat.memory.vector_memory_collection.VectorMemoryCollection.recall_memories_from_embedding","title":"<code>recall_memories_from_embedding(embedding, metadata=None, k=5, threshold=None)</code>","text":"<p>Retrieve similar memories from embedding</p> Source code in <code>cat/memory/vector_memory_collection.py</code> <pre><code>def recall_memories_from_embedding(\n    self, embedding, metadata=None, k=5, threshold=None\n):\n    \"\"\"Retrieve similar memories from embedding\"\"\"\n\n    memories = self.client.query_points(\n        collection_name=self.collection_name,\n        query=embedding,\n        query_filter=self._qdrant_filter_from_dict(metadata),\n        with_payload=True,\n        with_vectors=True,\n        limit=k,\n        score_threshold=threshold,\n        search_params=SearchParams(\n            quantization=QuantizationSearchParams(\n                ignore=False,\n                rescore=True,\n                oversampling=2.0,  # Available as of v1.3.0\n            )\n        ),\n    ).points\n\n    # convert Qdrant points to langchain.Document\n    langchain_documents_from_points = []\n    for m in memories:\n        langchain_documents_from_points.append(\n            (\n                Document(\n                    page_content=m.payload.get(\"page_content\"),\n                    metadata=m.payload.get(\"metadata\") or {},\n                ),\n                m.score,\n                m.vector,\n                m.id,\n            )\n        )\n\n    # we'll move out of langchain conventions soon and have our own cat Document\n    # for doc, score, vector in langchain_documents_from_points:\n    #    doc.lc_kwargs = None\n\n    return langchain_documents_from_points\n</code></pre>"},{"location":"API_Documentation/memory/working_memory/","title":"working_memory","text":""},{"location":"API_Documentation/memory/working_memory/#cat.memory.working_memory.WorkingMemory","title":"<code>WorkingMemory</code>","text":"<p>               Bases: <code>BaseModelDict</code></p> <p>Represents the volatile memory of a cat, functioning similarly to a dictionary to store temporary custom data.</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[ConversationMessage]</code> <p>A list that maintains the conversation history between the Human and the AI.</p> <code>user_message_json</code> <code>Optional[UserMessage], default=None</code> <p>An optional UserMessage object representing the last user message.</p> <code>active_form</code> <code>Optional[CatForm], default=None</code> <p>An optional reference to a CatForm currently in use.</p> <code>recall_query</code> <code>str, default=\"\"</code> <p>A string that stores the last recall query.</p> <code>episodic_memories</code> <code>List</code> <p>A list for storing episodic memories.</p> <code>declarative_memories</code> <code>List</code> <p>A list for storing declarative memories.</p> <code>procedural_memories</code> <code>List</code> <p>A list for storing procedural memories.</p> <code>model_interactions</code> <code>List[ModelInteraction]</code> <p>A list of interactions with models.</p> Source code in <code>cat/memory/working_memory.py</code> <pre><code>class WorkingMemory(BaseModelDict):\n    \"\"\"\n    Represents the volatile memory of a cat, functioning similarly to a dictionary to store temporary custom data.\n\n    Attributes\n    ----------\n    history : List[ConversationMessage]\n        A list that maintains the conversation history between the Human and the AI.\n    user_message_json : Optional[UserMessage], default=None\n        An optional UserMessage object representing the last user message.\n    active_form : Optional[CatForm], default=None\n        An optional reference to a CatForm currently in use.\n    recall_query : str, default=\"\"\n        A string that stores the last recall query.\n    episodic_memories : List\n        A list for storing episodic memories.\n    declarative_memories : List\n        A list for storing declarative memories.\n    procedural_memories : List\n        A list for storing procedural memories.\n    model_interactions : List[ModelInteraction]\n        A list of interactions with models.\n    \"\"\"\n\n    history: List[ConversationMessage] = []\n\n    user_message_json: Optional[UserMessage] = None\n\n    active_form: Optional[CatForm] = None\n    recall_query: str = \"\"\n\n    episodic_memories: List = []\n    declarative_memories: List = []\n    procedural_memories: List = []\n\n    model_interactions: List[ModelInteraction] = []\n\n    def update_conversation_history(self, message: str, who: str, why = {}):\n        \"\"\"\n        This method is deprecated. Use `update_history` instead.\n\n        Updates the conversation history with the most recent message.\n\n        Parameters\n        ----------\n        message :str\n            The text content of the message.\n        who : str\n            The name of the message author.\n        why : Optional[Dict[str, Any]], default=None\n            Optional explanation for the message.\n\n        Notes\n        -----\n        This method is deprecated and will be removed in future versions. Use `update_history` instead.\n        \"\"\"\n\n        deprecation_warning(\n            \"update_conversation_history is deprecated and will be removed in a future release. Use update_history instead.\"\n        )\n        role = Role.AI if who == \"AI\" else Role.Human\n\n        if role == Role.AI:\n            content = CatMessage(\n                user_id=self.user_message_json.user_id,\n                who=who,\n                text=message,\n                why=why,\n            )\n        else:\n            content = UserMessage(\n                user_id=self.user_message_json.user_id,\n                who=who,\n                text=message\n            )\n\n        self.history.append(content)\n\n    def update_history(self, message: ConversationMessage):\n        \"\"\"\n        Adds a message to the history.\n\n        Parameters\n        ----------\n        message : ConversationMessage\n            The message, must be of type `ConversationMessage` (typically a subclass like `UserMessage` or `CatMessage`).\n        \"\"\"\n        self.history.append(message)\n        self.history = self.history[-MAX_WORKING_HISTORY_LENGTH:]\n\n\n    def stringify_chat_history(self, latest_n: int = MAX_WORKING_HISTORY_LENGTH) -&gt; str:\n        \"\"\"Serialize chat history.\n        Converts to text the recent conversation turns.\n        Useful for retrocompatibility with old non-chat models, and to easily insert convo into a prompt without using dedicated objects and libraries.\n\n        Parameters\n        ----------\n        latest_n : int\n            How many latest turns to stringify.\n\n        Returns\n        -------\n        history : str\n            String with recent conversation turns.\n        \"\"\"\n\n        history = self.history[-latest_n:]\n\n        history_string = \"\"\n        for turn in history:\n            history_string += f\"\\n - {turn.who}: {turn.text}\"\n\n        return history_string\n\n    def langchainfy_chat_history(self, latest_n: int = MAX_WORKING_HISTORY_LENGTH) -&gt; List[BaseMessage]: \n        \"\"\"Convert chat history in working memory to langchain objects.\n\n        Parameters\n        ----------\n        latest_n : int\n            How many latest turns to convert.\n\n        Returns\n        -------\n        history : List[BaseMessage]\n            List of langchain HumanMessage / AIMessage.\n        \"\"\"\n\n        recent_history = self.history[-latest_n:]\n        langchain_chat_history = []\n\n        for message in recent_history:\n            langchain_chat_history.append(\n                message.langchainfy()    \n            )\n\n        return langchain_chat_history\n</code></pre>"},{"location":"API_Documentation/memory/working_memory/#cat.memory.working_memory.WorkingMemory.langchainfy_chat_history","title":"<code>langchainfy_chat_history(latest_n=MAX_WORKING_HISTORY_LENGTH)</code>","text":"<p>Convert chat history in working memory to langchain objects.</p> <p>Parameters:</p> Name Type Description Default <code>latest_n</code> <code>int</code> <p>How many latest turns to convert.</p> <code>MAX_WORKING_HISTORY_LENGTH</code> <p>Returns:</p> Name Type Description <code>history</code> <code>List[BaseMessage]</code> <p>List of langchain HumanMessage / AIMessage.</p> Source code in <code>cat/memory/working_memory.py</code> <pre><code>def langchainfy_chat_history(self, latest_n: int = MAX_WORKING_HISTORY_LENGTH) -&gt; List[BaseMessage]: \n    \"\"\"Convert chat history in working memory to langchain objects.\n\n    Parameters\n    ----------\n    latest_n : int\n        How many latest turns to convert.\n\n    Returns\n    -------\n    history : List[BaseMessage]\n        List of langchain HumanMessage / AIMessage.\n    \"\"\"\n\n    recent_history = self.history[-latest_n:]\n    langchain_chat_history = []\n\n    for message in recent_history:\n        langchain_chat_history.append(\n            message.langchainfy()    \n        )\n\n    return langchain_chat_history\n</code></pre>"},{"location":"API_Documentation/memory/working_memory/#cat.memory.working_memory.WorkingMemory.stringify_chat_history","title":"<code>stringify_chat_history(latest_n=MAX_WORKING_HISTORY_LENGTH)</code>","text":"<p>Serialize chat history. Converts to text the recent conversation turns. Useful for retrocompatibility with old non-chat models, and to easily insert convo into a prompt without using dedicated objects and libraries.</p> <p>Parameters:</p> Name Type Description Default <code>latest_n</code> <code>int</code> <p>How many latest turns to stringify.</p> <code>MAX_WORKING_HISTORY_LENGTH</code> <p>Returns:</p> Name Type Description <code>history</code> <code>str</code> <p>String with recent conversation turns.</p> Source code in <code>cat/memory/working_memory.py</code> <pre><code>def stringify_chat_history(self, latest_n: int = MAX_WORKING_HISTORY_LENGTH) -&gt; str:\n    \"\"\"Serialize chat history.\n    Converts to text the recent conversation turns.\n    Useful for retrocompatibility with old non-chat models, and to easily insert convo into a prompt without using dedicated objects and libraries.\n\n    Parameters\n    ----------\n    latest_n : int\n        How many latest turns to stringify.\n\n    Returns\n    -------\n    history : str\n        String with recent conversation turns.\n    \"\"\"\n\n    history = self.history[-latest_n:]\n\n    history_string = \"\"\n    for turn in history:\n        history_string += f\"\\n - {turn.who}: {turn.text}\"\n\n    return history_string\n</code></pre>"},{"location":"API_Documentation/memory/working_memory/#cat.memory.working_memory.WorkingMemory.update_conversation_history","title":"<code>update_conversation_history(message, who, why={})</code>","text":"<p>This method is deprecated. Use <code>update_history</code> instead.</p> <p>Updates the conversation history with the most recent message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The text content of the message.</p> required <code>who</code> <code>str</code> <p>The name of the message author.</p> required <code>why</code> <code>Optional[Dict[str, Any]]</code> <p>Optional explanation for the message.</p> <code>None</code> Notes <p>This method is deprecated and will be removed in future versions. Use <code>update_history</code> instead.</p> Source code in <code>cat/memory/working_memory.py</code> <pre><code>def update_conversation_history(self, message: str, who: str, why = {}):\n    \"\"\"\n    This method is deprecated. Use `update_history` instead.\n\n    Updates the conversation history with the most recent message.\n\n    Parameters\n    ----------\n    message :str\n        The text content of the message.\n    who : str\n        The name of the message author.\n    why : Optional[Dict[str, Any]], default=None\n        Optional explanation for the message.\n\n    Notes\n    -----\n    This method is deprecated and will be removed in future versions. Use `update_history` instead.\n    \"\"\"\n\n    deprecation_warning(\n        \"update_conversation_history is deprecated and will be removed in a future release. Use update_history instead.\"\n    )\n    role = Role.AI if who == \"AI\" else Role.Human\n\n    if role == Role.AI:\n        content = CatMessage(\n            user_id=self.user_message_json.user_id,\n            who=who,\n            text=message,\n            why=why,\n        )\n    else:\n        content = UserMessage(\n            user_id=self.user_message_json.user_id,\n            who=who,\n            text=message\n        )\n\n    self.history.append(content)\n</code></pre>"},{"location":"API_Documentation/memory/working_memory/#cat.memory.working_memory.WorkingMemory.update_history","title":"<code>update_history(message)</code>","text":"<p>Adds a message to the history.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>ConversationMessage</code> <p>The message, must be of type <code>ConversationMessage</code> (typically a subclass like <code>UserMessage</code> or <code>CatMessage</code>).</p> required Source code in <code>cat/memory/working_memory.py</code> <pre><code>def update_history(self, message: ConversationMessage):\n    \"\"\"\n    Adds a message to the history.\n\n    Parameters\n    ----------\n    message : ConversationMessage\n        The message, must be of type `ConversationMessage` (typically a subclass like `UserMessage` or `CatMessage`).\n    \"\"\"\n    self.history.append(message)\n    self.history = self.history[-MAX_WORKING_HISTORY_LENGTH:]\n</code></pre>"},{"location":"API_Documentation/routes/settings/","title":"settings","text":""},{"location":"API_Documentation/routes/settings/#cat.routes.settings.create_setting","title":"<code>create_setting(payload, cat=check_permissions(AuthResource.SETTINGS, AuthPermission.WRITE))</code>","text":"<p>Create a new setting in the database</p> Source code in <code>cat/routes/settings.py</code> <pre><code>@router.post(\"/\")\ndef create_setting(\n    payload: models.SettingBody,\n    cat=check_permissions(AuthResource.SETTINGS, AuthPermission.WRITE),\n):\n    \"\"\"Create a new setting in the database\"\"\"\n\n    # complete the payload with setting_id and updated_at\n    payload = models.Setting(**payload.model_dump())\n\n    # save to DB\n    new_setting = crud.create_setting(payload)\n\n    return {\"setting\": new_setting}\n</code></pre>"},{"location":"API_Documentation/routes/settings/#cat.routes.settings.delete_setting","title":"<code>delete_setting(settingId, cat=check_permissions(AuthResource.SETTINGS, AuthPermission.DELETE))</code>","text":"<p>Delete a specific setting in the database</p> Source code in <code>cat/routes/settings.py</code> <pre><code>@router.delete(\"/{settingId}\")\ndef delete_setting(\n    settingId: str,\n    cat=check_permissions(AuthResource.SETTINGS, AuthPermission.DELETE),\n):\n    \"\"\"Delete a specific setting in the database\"\"\"\n\n    # does the setting exist?\n    setting = crud.get_setting_by_id(settingId)\n    if not setting:\n        raise HTTPException(\n            status_code=404,\n            detail={\n                \"error\": f\"No setting with this id: {settingId}\",\n            },\n        )\n\n    # delete\n    crud.delete_setting_by_id(settingId)\n\n    return {\"deleted\": settingId}\n</code></pre>"},{"location":"API_Documentation/routes/settings/#cat.routes.settings.get_setting","title":"<code>get_setting(settingId, cat=check_permissions(AuthResource.SETTINGS, AuthPermission.READ))</code>","text":"<p>Get the a specific setting from the database</p> Source code in <code>cat/routes/settings.py</code> <pre><code>@router.get(\"/{settingId}\")\ndef get_setting(\n    settingId: str, cat=check_permissions(AuthResource.SETTINGS, AuthPermission.READ)\n):\n    \"\"\"Get the a specific setting from the database\"\"\"\n\n    setting = crud.get_setting_by_id(settingId)\n    if not setting:\n        raise HTTPException(\n            status_code=404,\n            detail={\n                \"error\": f\"No setting with this id: {settingId}\",\n            },\n        )\n    return {\"setting\": setting}\n</code></pre>"},{"location":"API_Documentation/routes/settings/#cat.routes.settings.get_settings","title":"<code>get_settings(search='', cat=check_permissions(AuthResource.SETTINGS, AuthPermission.LIST))</code>","text":"<p>Get the entire list of settings available in the database</p> Source code in <code>cat/routes/settings.py</code> <pre><code>@router.get(\"/\")\ndef get_settings(\n    search: str = \"\",\n    cat=check_permissions(AuthResource.SETTINGS, AuthPermission.LIST),\n):\n    \"\"\"Get the entire list of settings available in the database\"\"\"\n\n    settings = crud.get_settings(search=search)\n\n    return {\"settings\": settings}\n</code></pre>"},{"location":"API_Documentation/routes/settings/#cat.routes.settings.update_setting","title":"<code>update_setting(settingId, payload, cat=check_permissions(AuthResource.SETTINGS, AuthPermission.EDIT))</code>","text":"<p>Update a specific setting in the database if it exists</p> Source code in <code>cat/routes/settings.py</code> <pre><code>@router.put(\"/{settingId}\")\ndef update_setting(\n    settingId: str,\n    payload: models.SettingBody,\n    cat=check_permissions(AuthResource.SETTINGS, AuthPermission.EDIT),\n):\n    \"\"\"Update a specific setting in the database if it exists\"\"\"\n\n    # does the setting exist?\n    setting = crud.get_setting_by_id(settingId)\n    if not setting:\n        raise HTTPException(\n            status_code=404,\n            detail={\n                \"error\": f\"No setting with this id: {settingId}\",\n            },\n        )\n\n    # complete the payload with setting_id and updated_at\n    payload = models.Setting(**payload.model_dump())\n    payload.setting_id = settingId  # force this to be the setting_id\n\n    # save to DB\n    updated_setting = crud.update_setting_by_id(payload)\n\n    return {\"setting\": updated_setting}\n</code></pre>"},{"location":"faq/basic_info/","title":"Basic Info","text":""},{"location":"faq/basic_info/#basic-info","title":"Basic Info","text":""},{"location":"faq/basic_info/#can-i-insert-a-long-article-into-the-chat","title":"Can I insert a long article into the chat?","text":"<p>Please avoid copy-pasting long articles into the chat. Use Rabbit Hole to upload long texts instead: just click on the attachment icon in the chat input widget and upload your file.</p>"},{"location":"faq/basic_info/#are-the-configured-llm-apis-used-to-instruct-the-cat-with-the-documents-im-going-to-upload","title":"Are the configured LLM APIs used to \"instruct\" the Cat with the documents I'm going to upload?","text":"<p>That's not exactly how it works: basically when you ask something to the Cat, we pass to the configured LLM a prompt with your actual question + data that can be useful to answer that question. Data can be part of your documents or chat history. Please check our documentation for more details about how the Cat internally works.</p>"},{"location":"faq/basic_info/#can-i-talk-to-the-cat-in-a-language-different-from-english","title":"Can I talk to the Cat in a language different from English?","text":"<p>Of course, you can: just change the prompts in the Plugin folder accordingly, and take care not to mix languages to get best results.</p>"},{"location":"faq/basic_info/#how-can-i-know-where-the-cat-gets-the-answers-from-id-like-to-know-if-its-using-the-files-i-uploaded-or-if-its-querying-the-configured-llm","title":"How can I know where the Cat gets the answers from? I'd like to know if it's using the files I uploaded or if it's querying the configured LLM","text":"<p>Just open the console in your browser to check the logs there. At some point soon, this information will end up in the user interface, but at the moment is behind the scenes.</p>"},{"location":"faq/basic_info/#i-sent-to-the-cat-some-text-and-documents-i-want-to-get-rid-of-how-can-i-achieve-it","title":"I sent to the Cat some text and documents I want to get rid of, How can I achieve it?","text":"<p>You can delete the <code>long_term_memory</code> folder and restart the Cat! Remember that you will lost everything!</p>"},{"location":"faq/customization/","title":"Customization","text":""},{"location":"faq/customization/#customization","title":"Customization","text":""},{"location":"faq/customization/#i-want-to-build-my-own-plugin-for-the-cat-what-should-i-know-about-licensing","title":"I want to build my own plugin for the Cat: what should I know about licensing?","text":"<p>You can set set any licence you want and you are not limited to selling your plugin. The Cat core is GPL3, meaning you are free to fork and go on your own, but you are forced to open source changes to the core.</p>"},{"location":"faq/customization/#port-1865-is-not-allowed-by-my-operating-system-andor-firewall","title":"Port 1865 is not allowed by my operating system and/or firewall","text":"<p>Change the port as you wish in the <code>.env</code> file.</p> <pre><code># Set HOST and PORT for your Cat. Default will be localhost:1865  \nCORE_HOST=localhost\nCORE_PORT=9000\n</code></pre>"},{"location":"faq/customization/#can-i-use-a-different-vector-database-than-qdrant","title":"Can I use a different vector database than Qdrant?","text":"<p>At this moment we don't provide any way to switch the vector database \ud83d\ude3f but it is planned for the future.</p>"},{"location":"faq/errors/","title":"Errors","text":""},{"location":"faq/errors/#errors","title":"Errors","text":""},{"location":"faq/errors/#why-am-i-getting-the-error-ratelimiterror-in-my-browser-console","title":"Why am I getting the error <code>RateLimitError</code> in my browser console?","text":"<p>Please check if you have a valid credit card connected or if you have used up all the credits of your OpenAI trial period.</p>"},{"location":"faq/errors/#docker-has-no-permissions-to-write","title":"Docker has no permissions to write","text":"<p>This is a matter with your docker installation or the user you run docker from. Usually you can resolve it by using sudo command before calling any docker command, but it's better to create a <code>docker</code> group on your Linux system and give root-level privileges to docker.</p>"},{"location":"faq/errors/#the-cat-seems-not-to-be-working-from-inside-a-virtual-machine","title":"The Cat seems not to be working from inside a Virtual Machine","text":"<p>In VirtualBox, you can select Settings-&gt;Network, then choose NAT in the \"Attached to\" drop down menu. Select \"Advanced\" to configure the port forwarding rules. Assuming the guest IP of your VM is 10.0.2.15 (the default) and the ports configured in the .env files are the defaults, you have to set at least the following rule:</p> Rule name Protocol Host IP Host Port Guest IP Guest Port Rule 1 TCP 127.0.0.1 1865 10.0.2.15 1865 <p>If you want to work on the documentation of the Cat, you also have to add one rule for port 8000 which is used by <code>mkdocs</code>, and to configure <code>mkdocs</code> itself to respond to all requests (not only localhost as per the default).</p>"},{"location":"faq/errors/#proxy-https-and-mixed-content","title":"Proxy HTTPS and Mixed content","text":"<p>I'm using the Cat behind an HTTPS proxy but the login page tries to load some assets over HTTP instead of HTTPS. It seems to be related to <code>url_for('core-static', ...)</code>.</p> <p></p> <p>You need to configure the <code>CCAT_HTTPS_PROXY_MODE</code> environment variable</p>"},{"location":"faq/general/","title":"General","text":""},{"location":"faq/general/#general","title":"General","text":""},{"location":"faq/general/#ive-found-the-cat-and-i-like-it-very-much-but-im-not-able-to-follow-your-instructions-to-install-it-on-my-machine-can-you-help","title":"I've found the Cat and I like it very much, but I'm not able to follow your instructions to install it on my machine. Can you help?","text":"<p>The Cheshire Cat is a framework to help developers to build vertical AIs: you will need some basic technical skills to follow our instructions. Please try to ask in the support channel in our discord server, and remember this is all volunteers effort: be kind! :)</p>"},{"location":"faq/general/#why-the-cat-does-not-default-to-some-open-llm-instead-of-chatgpt-or-gpt-3","title":"Why the Cat does not default to some open LLM instead of ChatGPT or GPT-3?","text":"<p>Our intention is not to depend on any specific LLM: the Cat does not have a preference about which LLM to use. Nonetheless, at the moment, OpenAI tools still provide the best results for your bucks. Decision is up to you.</p>"},{"location":"faq/general/#are-text-and-documents-sent-to-the-cat-safe-and-not-shared-with-anybody","title":"Are text and documents sent to the Cat safe and not shared with anybody?","text":"<p>The local memory is safe and under your control, although embeddings and prompts are shared with your configured LLM, meaning you need to check how safe the LLM is. We plan to adopt local LLMs, at which point all your data will be under your control.</p>"},{"location":"faq/general/#what-is-the-difference-between-langchain-and-the-cat","title":"What is the difference between Langchain and the Cat?","text":"<p>The Cheshire Cat is a production-ready AI framework, it means that with almost no effort you can setup an intelligent agent ready to help both you and your customers.</p> <p>On the other hand, Langchain is a framework for developing applications powered by language models. It offers tons of composable tools and integrations to this purpose and the Cheshire Cat makes use of some of them to manage chains, agents, llm/embedder. You can take an in depth look at our core if you are purr-ious about it.</p>"},{"location":"faq/general/#i-want-to-use-the-admin-page-for","title":"I want to use the admin page for...","text":"<p>The admin panel is meant to be an administration interface. It's purpose is to chat with the Cat only to debug/play with it, it is not intended to be a final widget chat used by eventual final users.</p> <p>We provide a widget to connect the Cat to your product.</p> <p>You are free to modify the Admin to adapt it to your product, however you will need to respect the GPL3 Licence, meaning you are free to fork the codebase and go on your own, but you are forced to open source eventual changes.</p>"},{"location":"faq/security_and_spending/","title":"Security & Spending","text":""},{"location":"faq/security_and_spending/#security-and-spending","title":"Security and Spending","text":""},{"location":"faq/security_and_spending/#security","title":"Security","text":""},{"location":"faq/security_and_spending/#where-is-the-openai-api-key-or-other-keys-saved-in-the-cat","title":"Where is the OpenAI API key (or other keys) saved in the Cat?","text":"<p>Keys are store in a JSON file, <code>core/metadata.json</code>.</p>"},{"location":"faq/security_and_spending/#will-openai-see-my-documents-and-conversations","title":"Will OpenAI see my documents and conversations?","text":"<p>If you are using the Cat with an OpenAI LLM, all your conversations and documents will indeed take a trip into OpenAI servers, because the models are there. We advise to avoid uploading sensitive documents while using an external LLM. If you want to use the Cat in total security and privacy, use a local LLM or a cloud LLM in your control.</p>"},{"location":"faq/security_and_spending/#spending","title":"Spending","text":""},{"location":"faq/security_and_spending/#i-have-chatgpt-subscription-can-i-use-the-cat","title":"I have chatgpt subscription, can I use the cat?","text":"<p>Chat-gpt subscription is different from OpenAI API</p>"},{"location":"faq/security_and_spending/#is-there-a-free-way-to-use-openai-services","title":"Is there a free way to use OpenAI services?","text":"<p>Unfortunately you need to pay to use OpenAI models, but they are quite cheap.</p>"},{"location":"faq/security_and_spending/#can-i-run-local-models-like-llama-to-avoid-spending","title":"Can I run local models like LLAMA to avoid spending?","text":"<p>Running a LLM (Large Language Model) locally requires high-end hardware and technical skills. If you don't know what you are doing, we suggest you start using the Cat with ChatGPT. Afterwards you can experiment with local models or by setting up a cloud endpoint. The Cat offers you several ways to use an LLM.</p>"},{"location":"faq/security_and_spending/#can-i-know-in-advance-how-much-money-i-will-spend","title":"Can I know in advance how much money I will spend?","text":"<p>That depends on the vendors pricing, how many documents you upload in the Cat memory and how much you chat. We suggest you start with light usage and small documents, and check how the billing is growing in your LLM vendor's website. In our experience LLM cloud usage is cheap, and it will probably be even cheaper in the next months and years.</p>"},{"location":"faq/security_and_spending/#is-my-gpu-powerful-enough-to-run-a-local-model","title":"Is my GPU powerful enough to run a local model?","text":"<p>That strongly depends on the size of the model you want to run. Try using this application from HuggingFace to get an idea of which model and the amount of quantization your hardware can handle. </p>"},{"location":"faq/llm-concepts/embedder/","title":"Encoder","text":""},{"location":"faq/llm-concepts/embedder/#embedder-or-encoder","title":"Embedder (or encoder)","text":"<p>This type of Language Model takes a string as input and returns a vector as output. This is known as an embedding. Namely, this is a condensed representation of the input content. The output vector, indeed, embeds the semantic information of the input text.</p> <p>Despite being non-human readable, the embedding comes with the advantage of living in a Euclidean geometrical space. The embedding can be seen as a point in a multidimensional space, thus, geometrical operations can be applied to it. For instance, measuring the distance between two points can inform us about the similarity between two sentences.</p>"},{"location":"faq/llm-concepts/llm/","title":"Language Models","text":""},{"location":"faq/llm-concepts/llm/#language-models","title":"Language Models","text":"<p>A language model is a Deep Learning Neural Network trained on a huge amount of text data to perform different types of language tasks. Commonly, they are also referred to as Large Language Models (LLM). Language models come in many architectures, size and specializations. The peculiarity of the Cheshire Cat is to be model-agnostic. This means it supports many different language models.</p> <p>By default, there are two classes of language models that tackle two different tasks.</p>"},{"location":"faq/llm-concepts/llm/#completion-model","title":"Completion Model","text":"<p>This is the most known type of language models (see for examples ChatGPT, Cohere and many others). A completion model takes a string as input and generates a plausible answer by completion.</p> <p>Warning</p> <p>A LLM answer should not be accepted as-is, since LLM are subjected to hallucinations. Namely, their main goal is to generate plausible answers from the syntactical point of view. Thus, the provided answer could come from completely invented information.</p>"},{"location":"faq/llm-concepts/llm/#embedding-model","title":"Embedding Model","text":"<p>This type of model takes a string as input and returns a vector as output. The model is known as an embedder, while the vector in output is called embedding. Namely, this is a condensed representation of the input content. The output vector, indeed, embeds the semantic information of the input text.</p> <p>Despite being non-human readable, the embedding comes with the advantage of living in a Euclidean geometrical space. The embedding can be seen as a point in a multidimensional space, thus, geometrical operations can be applied to it. For instance, measuring the distance between two points can inform us about the similarity between two sentences.</p>"},{"location":"faq/llm-concepts/llm/#language-models-flow","title":"Language Models flow","text":"<p>Developer documentation</p> <p>Language Models hooks</p> <p>Nodes with the \ud83e\ude9d point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"faq/llm-concepts/prompt/","title":"Prompt","text":""},{"location":"faq/llm-concepts/prompt/#prompt","title":"Prompt","text":"<p>A prompt is an instruction to an LLM.</p> <p>Prompting is about packaging your intent in a natural-language query that will cause the model to return the desired response. A prompt must be clear and specific. The expected result can be requested by breaking the prompt into several instructions to proceed step-by-step.</p> <p>A good prompt allows the model to work better and give better responses including preventing hallucinations. Prompting is not a science, but tips &amp; tricks have been discovered that give better performance.</p> <ul> <li>Use delimiters to clearly indicate distinct parts of the input</li> <li>Ask for a structured output</li> <li>Ask the model to check whether conditions are satisfied</li> <li>\"Few-shot\" prompting</li> <li>Specify the steps required to complete a task</li> <li>Instruct the model to work out its own solution before rushing to a conclusion</li> </ul> <p>Examples of Prompts:</p> <ul> <li> <p>\"Generate a list of three made-up book titles along with their authors and genres. Provide them in JSON format with the following keys: book_id, title, author, genre.\"</p> </li> <li> <p>\"Your task is to answer in a consistent style.</p> <p>&lt; child&gt;: Teach me about patience.</p> <p>&lt; grandparent&gt;: The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.*</p> <p>&lt; child&gt;: Teach me about resilience.\"</p> </li> </ul>"},{"location":"faq/llm-concepts/rag/","title":"Retrieval Augmented Generation","text":""},{"location":"faq/llm-concepts/rag/#retrieval-augmented-generation","title":"Retrieval Augmented Generation","text":"<p>Retrieval Augmented Generation (RAG) is an AI framework for improving the quality of responses generated by large language models (LLMs) by grounding the model on external sources of information. RAG uses semantic search to retrieve relevant and up-to-date information from a wide range of sources, including books, articles, websites, and databases. This information is then used to inform and improve the text generation of the LLM.</p> <p>RAG has several advantages over traditional language models.</p> <ul> <li>First, it can provide more accurate and up-to-date responses, as it is able to access the latest information.</li> <li>Second, it can reduce the risk of generating erroneous or misleading content, as it is grounded on a verified knowledge base.</li> <li>Finally, RAG can be used to generate different creative text formats, such as poems, code, scripts, musical pieces, emails, and letters.</li> </ul> <p></p> <p></p> <p></p>"},{"location":"faq/llm-concepts/vector-memory/","title":"Vector Memory","text":""},{"location":"faq/llm-concepts/vector-memory/#vector-memory","title":"Vector Memory","text":"<p>When we talk about Vector Memory we talk about Vector Database. A Vector Database is a particular kind of DB that stores information in form of high-dimensional vectors called embeddings. The embeddings are representations of text, image, sounds, ...</p> <p></p> <p>As Vector Memory the Cheshire-Cat using Qdrant, the VectorDBs offer also optimized methods for information retrieval usually based on Cosine similarity. From wikipedia:</p> <p>\"Cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle.\"</p> <p></p>"},{"location":"faq/llm-concepts/vector-memory/#semantic-search","title":"Semantic Search","text":"<p>Semantic search seeks to improve search accuracy by understanding the content of the search query. The idea is to create an high-dimensional semantic space and at search time to find the nearest point (documents) to our questions.</p> <p>To create the vectors you must use an embedder. The vectors are stored in the vector memory; when a query is done the embedder calculates its embedding, the VectorDB calculates the cosine similaity between query and stored points and the K nearest are returned.</p> <p></p>"},{"location":"faq/llm-concepts/vector-memory/#search-in-high-dimensional-spaces","title":"Search in high-dimensional spaces","text":"<p>Since the KNN is an algorithm whose performance degrades as the number of comparisons to be made increases, and since VectorDBs can contain as many as billions of vectors the technique used to efficiently find the closest points in high-dimensional spaces is usually Approximate Nearest Neighbors.</p>"},{"location":"framework/cat-components/cheshire_cat/agent/","title":"Introduction","text":""},{"location":"framework/cat-components/cheshire_cat/agent/#agent-manager","title":"Agent Manager","text":"<p>The Agent Manager is the Cat's component that manages the execution of language models chains. A language model chain is a pipeline that takes one or more input variables, it formats them in a prompt, submits the prompt to a language model and, optionally, parses the output.</p> <p>The Cat's Agent Manager orchestrates two chains: </p> <ol> <li>the procedures chain, which, in turn, is a component of the Procedures Agent;</li> <li>the memory chain</li> </ol> <p>When suitable tools for the task at hand are retrieved from the procedural memory, the Agent Manager calls the Procedures Agent to execute the procedures chain; otherwise the memory chain is executed to answer the user's question with the context retrieved from the episodic and declarative memories.</p> <p></p> <p>Specifically, the default execution pipeline is the following:</p> <ol> <li>the Cat receives the user's message;</li> <li>the Cat looks for relevant context in each memory collection (i.e. procedural, declarative and episodic) using the user's message as a query;</li> <li>if meaningful context is retrieved from the procedural memory, the Procedures Agent starts, otherwise the memory chain starts;</li> <li>if executed, the Procedures Agent provides an output. If the output answer the user's input, such output is returned to the user, otherwise the memory chain starts;</li> <li>if executed, the memory chain provides the output using the context retrieved from the declarative and episodic memories.</li> </ol>"},{"location":"framework/cat-components/cheshire_cat/core/","title":"The Core","text":""},{"location":"framework/cat-components/cheshire_cat/core/#the-core","title":"The Core","text":""},{"location":"framework/cat-components/cheshire_cat/mad_hatter/","title":"The Mad Hatter","text":""},{"location":"framework/cat-components/cheshire_cat/mad_hatter/#mad-hatter","title":"Mad Hatter","text":"<p>The Mad Hatter is the Cat's plugins manager. It takes care of loading, prioritizing and executing plugins.</p> <p>Specifically, the Mad Hatter lists all available plugins in proper folder and sort their hooks in descending order of priority. When the Cat invokes them, it executes them following that order.</p> <p>Developer documentation</p> <ul> <li>How to write a plugin</li> <li>Hooks</li> <li>Tools</li> <li>Forms</li> </ul>"},{"location":"framework/cat-components/cheshire_cat/memory_chain/","title":"Memory Chain","text":""},{"location":"framework/cat-components/cheshire_cat/memory_chain/#memory-chain","title":"Memory Chain","text":"<p>The Memory Chain is a simple chain that takes the user's input and the context retrieved from the episodic and declarative memories and formats them in the main prompt. Such prompt is submitted to the language model.</p> <p></p>"},{"location":"framework/cat-components/cheshire_cat/rabbit_hole/","title":"The Rabbit Hole","text":""},{"location":"framework/cat-components/cheshire_cat/rabbit_hole/#rabbit-hole","title":"Rabbit Hole","text":"<p>The Rabbit Hole is the Cat's component that takes care of ingesting documents and storing them in the declarative memory. You can interact with it either through its endpoint, the GUI or a Python script.</p> <p>Currently supported file formats are: <code>.txt</code>, <code>.md</code>, <code>.pdf</code> or <code>.html</code> via web URL. </p> <p></p> <p>TODO: Describe how customize ingestion using hooks. TODO: Link Rabbit Hole Ingestion Diagram.</p>"},{"location":"framework/cat-components/cheshire_cat/stray_cat/","title":"The Stray Cat","text":""},{"location":"framework/cat-components/cheshire_cat/stray_cat/#stray-cat","title":"Stray Cat","text":"<p>What does the <code>cat</code> argument in hooks and tools do?</p> <p><code>cat</code> is an instance of <code>StrayCat</code>, a key component of the Cheshire Cat framework, serving as the primary entry point to all its features. This component handles user sessions, manages working memory, facilitates conversations, and provides essential methods for LLM interaction, WebSocket messaging, and more.</p>"},{"location":"framework/cat-components/cheshire_cat/stray_cat/#main-entry-point-to-the-framework","title":"Main Entry Point to the Framework","text":"<p><code>cat</code> is designed to be your primary interface for leveraging the full capabilities of the framework. By using this object, you can access various features and functionalities without the need to import them. It also provides convenient shortcuts to access nested or user-related structures within the Cat, streamlining your development process.</p> <p>Whenever you see something like:</p> <pre><code>from cat.mad_hatter.decorators import tool\n\n@tool\ndef fancy_tool(fancy_arg, cat):\n    \"\"\"Fancy tool docstring, seen by the LLM\"\"\"\n    return \"Did something fancy\"\n</code></pre> <p>Or:</p> <pre><code>from cat.mad_hatter.decorators import hook\n\n@hook\ndef before_cat_sends_message(final_output, cat):\n    final_output.content = final_output.content.upper()\n    return final_output\n</code></pre> <p>You'll probably notice the <code>cat</code> parameter being passed around. This cat is an instance of the <code>StrayCat</code> class. Like a wandering stray cat, this component roams through different parts of the framework, encapsulating the session's state and providing the necessary context for various operations within the Cheshire Cat ecosystem.</p> <p>For more detail about useful methods, please see Useful Methods and Shortcuts.</p>"},{"location":"framework/cat-components/cheshire_cat/stray_cat/#user-session-management","title":"User session management","text":"<p>Whenever a user sends a message to the Cat, a <code>StrayCat</code> instance is called within the specific user context. This instance is responsible for several key tasks: handling incoming messages, updating the conversation history, managing working memory, delegating the framework agents to elaborate the response and finally gathering all relevant information about the response, including the content and the reasoning behind it (the \"why\"). This comprehensive response is then returned to the user.</p>"},{"location":"framework/cat-components/cheshire_cat/stray_cat/#examples","title":"Examples","text":"<p>As mentioned above, the StrayCat class provides some shortcuts to access Cheshire Cat features. For example you can easily use the LLM:</p> <pre><code>from cat.mad_hatter.decorators import hook\n\n@hook\ndef agent_fast_reply(reply, cat):\n    prompt = \"Say a joke in german\"\n    reply[\"output\"] = cat.llm(prompt)\n    return reply\n</code></pre> <p>An example using the White Rabbit scheduler:</p> <pre><code>from cat.mad_hatter.decorators import tool\n\n@tool(return_direct=True)\ndef schedule_say_hello(minutes, cat):\n    \"\"\"Say hello in a few minutes. Input is the number of minutes.\"\"\"\n\n    delay = int(minutes)\n\n    # We can access the White Rabbit to schedule jobs\n    job_id = cat.white_rabbit.schedule_chat_message(\"Hello\", cat, minutes=delay)\n\n    return f\"Scheduled job {job_id} to say hello in {delay} minutes.\"\n</code></pre> <p>In the same way you can access other Cat components like the Mad Hatter or the Rabbit Hole. You can also access the Working Memory. Easily like so:</p> <pre><code>from cat.mad_hatter.decorators import tool\n\n@tool(return_direct=True)\ndef clear_working_memory(arg, cat):\n    \"\"\"Use this tool to clear / reset / delete the conversation history.\"\"\"\n\n    # We can access the Working Memory to clear the chat history\n    cat.working_memory.history = []\n\n    return \"Chat history cleared\"\n</code></pre>"},{"location":"framework/cat-components/cheshire_cat/stray_cat/#useful-methods-and-shortcuts","title":"Useful Methods and Shortcuts","text":""},{"location":"framework/cat-components/cheshire_cat/stray_cat/#shortcuts","title":"Shortcuts","text":""},{"location":"framework/cat-components/cheshire_cat/stray_cat/#useful-methods","title":"Useful Methods","text":"<p>The <code>StrayCat</code> class exposes many methods, some are particularly useful for plugin developers:</p> <ul> <li><code>cat.send_chat_message(...)</code>     Sends a message to the user using the active WebSocket connection. </li> <li><code>cat.send_notification(...)</code>     Sends a notification to the user using the active WebSocket connection.</li> <li><code>cat.send_error(...)</code>     Sends an error message to the user using the active WebSocket connection.</li> <li><code>cat.llm(...)</code>     Shortcut method for generating a response using the LLM and passing a custom prompt.</li> <li><code>cat.embedder.embed_query(...)</code>     Shortcut method to embed a string in vector space.</li> <li><code>cat.classify(...)</code>     Utility method for classifying a given sentence using the LLM. You can pass either a list of strings as possible labels or a dictionary of labels as keys and list of strings as examples.</li> <li><code>cat.stringify_chat_history(...)</code>     Utility method to stringify the chat history.</li> </ul> <p>These methods provide easy-to-use interfaces for interacting with LLMs and other components of the Cheshire Cat framework, making plugin development faster and more efficient.</p> <p>For more details, see the StrayCat API reference</p>"},{"location":"framework/cat-components/cheshire_cat/tool_chain/","title":"Procedures Chain","text":""},{"location":"framework/cat-components/cheshire_cat/tool_chain/#procedures-chain","title":"Procedures Chain","text":"<p>Sometimes a simple answer from the language model is not enough. For this reason, the Cat can exploit a set of custom tools (e.g. API calls and Python functions) coming from the plugins. The decision on whether and which action should be taken to fulfill the user's request is delegated to an Agent, i.e. the Procedures Agent.</p> <p>The Procedures Agent uses the language model to outline a \"reasoning\" and accomplish the user's request with the tools retrieved from the Cat's procedural memory. The tools/forms selection and usage is planned according to a set of instructions. Finally, the Procedures Agent parses the formatting of the tool/form output.</p> <p></p>"},{"location":"framework/cat-components/cheshire_cat/white_rabbit/","title":"The White Rabbit","text":""},{"location":"framework/cat-components/cheshire_cat/white_rabbit/#white-rabbit","title":"White Rabbit","text":"<p>The White Rabbit is the Cat's built-in scheduler. It is built upon the APScheduler. It enables the scheduling of various type of jobs, including one-time, interval-based and cron jobs. It provides also the capability to manage job execution, pausing, resuming and canceling jobs.</p> <p>Note</p> <p>Currently, jobs are stored in memory, but future updates will support database storage for persistent job management. Suggestion, you can reschedule jobs at each startup of the Cat using the <code>after_cat_bootstrap</code> hook.</p>"},{"location":"framework/cat-components/cheshire_cat/white_rabbit/#how-to-use","title":"How to use","text":"<p>The White Rabbit is a singleton component instantiated during Cat's bootstrap processs. You can easily access it through any Cat instance as follows:</p> <pre><code>cat.white_rabbit\n</code></pre>"},{"location":"framework/cat-components/cheshire_cat/white_rabbit/#methods","title":"Methods","text":"Method Description Parameters Returns <code>get_job(job_id: str)</code> Retrieves a job by its ID. <code>job_id: str</code> - The ID of the job. <code>Dict[str, str]</code> - Job details or <code>None</code> if not found. <code>get_jobs()</code> Returns a list of all scheduled jobs. None <code>List[Dict[str, str]]</code> - A list of job details. <code>pause_job(job_id: str)</code> Pauses a job by its ID. <code>job_id: str</code> - The ID of the job. <code>bool</code> - <code>True</code> if successful, <code>False</code> otherwise. <code>resume_job(job_id: str)</code> Resumes a paused job by its ID. <code>job_id: str</code> - The ID of the job. <code>bool</code> - <code>True</code> if successful, <code>False</code> otherwise. <code>remove_job(job_id: str)</code> Removes a job by its ID. <code>job_id: str</code> - The ID of the job. <code>bool</code> - <code>True</code> if successful, <code>False</code> otherwise. <code>schedule_job(job, job_id: str = None, days=0, hours=0, minutes=0, seconds=0, milliseconds=0, microseconds=0, **kwargs)</code> Schedules a one-time job to run at a specified time. <code>job: function</code> - The function to be executed. <code>job_id: str</code> - The ID of the job (optional). Time parameters (<code>days</code>, <code>hours</code>, <code>minutes</code>, <code>seconds</code>, <code>milliseconds</code>, <code>microseconds</code>). <code>**kwargs</code> - Additional arguments for the job function. <code>str</code> - The job ID. <code>schedule_interval_job(job, job_id: str = None, start_date: datetime = None, end_date: datetime = None, days=0, hours=0, minutes=0, seconds=0, **kwargs)</code> Schedules a job to run at regular intervals. <code>job: function</code> - The function to be executed. <code>job_id: str</code> - The ID of the job (optional). <code>start_date: datetime</code> - The start date of the job (optional). <code>end_date: datetime</code> - The end date of the job (optional). Interval time parameters (<code>days</code>, <code>hours</code>, <code>minutes</code>, <code>seconds</code>). <code>**kwargs</code> - Additional arguments for the job function. <code>str</code> - The job ID. <code>schedule_cron_job(job, job_id: str = None, start_date: datetime = None, end_date: datetime = None, year=None, month=None, day=None, week=None, day_of_week=None, hour=None, minute=None, second=None, **kwargs)</code> Schedules a job using cron-like expressions. <code>job: function</code> - The function to be executed. <code>job_id: str</code> - The ID of the job (optional). <code>start_date: datetime</code> - The start date of the job (optional). <code>end_date: datetime</code> - The end date of the job (optional). Cron time parameters (<code>year</code>, <code>month</code>, <code>day</code>, <code>week</code>, <code>day_of_week</code>, <code>hour</code>, <code>minute</code>, <code>second</code>). <code>**kwargs</code> - Additional arguments for the job function. <code>str</code> - The job ID. <code>schedule_chat_message(content: str, cat, days=0, hours=0, minutes=0, seconds=0, milliseconds=0, microseconds=0)</code> Schedules a chat message to be sent after a specified delay. <code>content: str</code> - The message content. <code>cat</code> - The instance of <code>StrayCat</code> to send the message. Time parameters (<code>days</code>, <code>hours</code>, <code>minutes</code>, <code>seconds</code>, <code>milliseconds</code>, <code>microseconds</code>). <code>str</code> - The job ID."},{"location":"framework/cat-components/cheshire_cat/white_rabbit/#examples","title":"Examples","text":"<p>Here's a collection of examples showcasing how to use the WhiteRabbit to add scheduling capabilities to your AI agents.</p>"},{"location":"framework/cat-components/cheshire_cat/white_rabbit/#schedule-a-one-time-job","title":"Schedule a one-time job","text":"<p>In this example, we'll create a simple tool that allows the user to set an alarm that rings after a specified time interval.</p> <pre><code>from cat.mad_hatter.decorators import tool\n\n@tool\ndef ring_alarm(wait_time, cat):\n    \"\"\"Useful to ring the alarm. Use it whenever the user wants to ring the alarm. Input is the wait time of the alarm in seconds.\"\"\" \n\n    # Mocking alarm API call\n    def ring_alarm_api():\n        print(\"Riiing\")\n\n    cat.white_rabbit.schedule_job(ring_alarm_api, seconds=int(wait_time))\n\n    return f\"Alarm ringing in {wait_time} seconds\"\n</code></pre>"},{"location":"framework/cat-components/cheshire_cat/white_rabbit/#schedule-an-interval-job","title":"Schedule an interval job","text":"<p>In this example, we'll build a tool that retrieves a random quote from a free scraping website and sends it to the user at regular intervals.</p> <pre><code>from cat.mad_hatter.decorators import tool\nimport requests\nimport re\nimport random\n\n@tool(return_direct=True)\ndef schedule_quote_scraper(interval, cat):\n    \"\"\"\n    Useful to get a random quote at a scheduled interval. The interval is in seconds\n    \"\"\"\n\n    def scrape_random_quote():\n        url = \"http://quotes.toscrape.com/\"\n        response = requests.get(url)\n        response.raise_for_status()\n        # We would normally use beautifulsoup here, but for this example we'll just use regex\n        quotes = re.findall(r'&lt;span class=\"text\" itemprop=\"text\"&gt;(.*?)&lt;/span&gt;', response.text)\n        if quotes:\n            random_quote = random.choice(quotes)\n            cat.send_ws_message(random_quote, msg_type=\"chat\")\n        else:\n            cat.send_ws_message(\"No quotes found\", msg_type=\"chat\")\n\n    # Schedule the job to run at the specified interval\n    cat.white_rabbit.schedule_interval_job(scrape_random_quote, seconds=int(interval))\n\n    return f\"Quote scraping job scheduled to run every {interval} seconds.\"\n</code></pre>"},{"location":"framework/cat-components/cheshire_cat/white_rabbit/#schedule-a-cron-job","title":"Schedule a cron job","text":"<p>For a more detailed schedule you can leverage the <code>schedule_cron_job</code> method, passing a cron-like expression. For instance, it can be used to check for plugin updates every night at 2:00 AM. </p> <pre><code>from cat.mad_hatter.decorators import hook\nimport requests\nfrom cat.mad_hatter.registry import get_registry_url, registry_download_plugin\n\ndef parse_version(version: str):\n    return tuple(map(int, version.split('.')))\n\ndef get_plugins_from_registry(query: str):\n    response = requests.post(f\"{get_registry_url()}/search\", json={\"query\": query})\n    return response.json()\n\ndef upgrade_plugin(cat, plugin_id):\n    plugin = cat.mad_hatter.plugins[plugin_id]\n    plugins = get_plugins_from_registry(plugin.manifest[\"name\"])\n    for reg_plugin in plugins:\n        if reg_plugin[\"name\"] == plugin.manifest[\"name\"]:\n            reg_plugin_version = parse_version(reg_plugin[\"version\"])\n            if reg_plugin_version &gt; parse_version(plugin.manifest[\"version\"]):\n                tmp_path = registry_download_plugin(reg_plugin[\"url\"])\n                cat.mad_hatter.install_plugin(tmp_path)\n\n@hook\ndef after_cat_bootstrap(cat):\n    cat.white_rabbit.schedule_cron_job(upgrade_plugin, job_id=\"nightly_upgrade_plugins\", hour=2, minute=0, cat=cat, plugin_id=\"your_fancy_plugin\")\n</code></pre> <p>In this example we showed how the White Rabbit can also be accessed in hooks (i.e. <code>after_cat_bootstrap</code>) to perform generic tasks that are not strictly user-related. You can pass extra arguments to your scheduled function using the <code>**kwargs</code> parameter. Please note that this is a basic example and should not be used in production.</p>"},{"location":"framework/cat-components/memory/declarative_memory/","title":"Declarative Memory","text":""},{"location":"framework/cat-components/memory/declarative_memory/#ltm-declarative-memory","title":"LTM - Declarative Memory","text":"<p>The Declarative Memory contains uploaded documents' content. It is stored in a vector memory collection together with <code>episodic</code> and <code>procedural</code> memories.</p> <p>The <code>declarative memory</code> is a key component in the memory chain.</p> <p></p>"},{"location":"framework/cat-components/memory/episodic_memory/","title":"Episodic Memory","text":""},{"location":"framework/cat-components/memory/episodic_memory/#ltm-episodic-memory","title":"LTM - Episodic Memory","text":"<p>The Episodic Memory contains user and eventually cat utterances. It is stored in a vector memory collection together with <code>declarative</code> and <code>procedural</code> memories.</p> <p>The <code>episodic memory</code> is a key component in the memory chain.</p> <p></p>"},{"location":"framework/cat-components/memory/long_term_memory/","title":"Introduction","text":""},{"location":"framework/cat-components/memory/long_term_memory/#long-term-memory","title":"Long Term Memory","text":"<p>The Cat has memory that persist across restarts, this memory is implemented using a vector database. The name of this memory is <code>Long Term Memory</code> (LTM), it is made of three components:</p> <ul> <li>Episodic Memory, contains an extract of things the user said in the past;</li> <li>Declarative Memory, contains an extract of documents uploaded to the Cat;</li> <li>Procedural Memory, contains the set of Python functions that defines what the Cat is able to do.</li> </ul> <p></p> <p>During conversation between the Cat and the user, the memories are accessed by the Cat to retrieve relevant context for passing to the LLM and are updated when the LLM responds (details of the read and write flow of the Long Term Memory can be found in this diagram).</p> <p>The retrieved relevant context is used to make up the Main prompt and the Instruction prompt.</p> <p>You can interact with the LTM using the memory page of the Admin Portal.</p>"},{"location":"framework/cat-components/memory/procedural_memory/","title":"Procedural Memory","text":""},{"location":"framework/cat-components/memory/procedural_memory/#ltm-procedural-memory","title":"LTM - Procedural Memory","text":"<p>The Procedural Memory contains tools and knowledge on how to do things. It is stored in a vector memory collection together with <code>declarative</code> and <code>episodic</code> memories.</p> <p>To dive into <code>procedural memory</code> and <code>tool docstrings</code> reference the tool basics page.</p> <p></p>"},{"location":"framework/cat-components/memory/vector_memory/","title":"Vector Memory Collections","text":""},{"location":"framework/cat-components/memory/vector_memory/#vector-memory-collections","title":"Vector Memory Collections","text":"<p>The Vector Memory Collections are the lowest-level components of the Long Term Memory. These are particular databases that store the content in the form of geometrical vectors.</p> <p>A vector memory comes in the guise of a named collection of vectors and additional, optional metadata. The latter can be used to filter the search in the database. Each vector represents a memory. They are also called embeddings as they are the results of the text-to-vector conversion yielded by the embedder.</p> <p>Such databases are particularly useful because they allow to fetch relevant documents based on the vector similarity between a query and the stored embeddings.</p> <p>By default, Vector Memory Collections are created when the Cat is installed or after a complete memory swap.</p>"},{"location":"framework/cat-components/memory/vector_memory/#vector-memory-collections-flow","title":"Vector Memory Collections flow","text":"<pre><code>flowchart LR\n    subgraph CAT [\"#128049;Cheshire Cat\"]\n        H[\"#129693;\"]\n        H1[\"#129693;\"]\n        direction LR\n        subgraph LTM [\"#128024;Long Term Memory\"]\n            direction TB\n            C[(Episodic)];\n            D[(Declarative)];\n            P[(Procedural)]\n        end\n        H --&gt; C\n        H --&gt; D\n        H --&gt; P\n        C --&gt; H1\n        D --&gt; H1\n        P --&gt; H1\n    end\n    E[First Installation] ----&gt; H;\n    F[Memory Swap] ----&gt; H\n</code></pre> <p>Nodes with the \ud83e\ude9d point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"framework/cat-components/memory/working_memory/","title":"The Working Memory","text":""},{"location":"framework/cat-components/memory/working_memory/#working-memory","title":"Working Memory","text":""},{"location":"framework/cat-components/memory/working_memory/#introduction","title":"Introduction","text":"<p>The Working Memory is a crucial component for storing temporary data. It can be used to share data across plugins or any function that receives an instance of the Cat as an argument.</p> <p>By default, the Working Memory stores the chat history that ends up in the Main Prompt. Additionally, it collects relevant context from the episodic, declarative and procedural memories in the Long Term Memory.</p> <p>The Working Memory can be used also to store custom data during a session. This capability is essential for creating a state machine within your own plugin.</p> <p></p>"},{"location":"framework/cat-components/memory/working_memory/#interacting-with-the-working-memory","title":"Interacting with the Working Memory","text":"<p>As mentioned above, the Working Memory is a key component in the Cheshire Cat framework and can be leveraged within your own plugins.</p> <p>Whenever you have a StrayCat instance, you can access the Working Memory through the <code>working_memory</code> property, like so:</p> <pre><code>from cat.mad_hatter.decorators import hook\n\n@hook\ndef agent_fast_reply(fast_reply, cat):\n    if len(cat.working_memory.declarative_memories) == 0:\n        fast_reply[\"output\"] = \"Sorry, I'm afraid I don't know the answer\"\n\n    return fast_reply\n</code></pre> <p>The <code>working_memory</code> property returns an instance of the <code>WorkingMemory</code> class, which acts as a key-value store with a dual nature. It can store data as key-value pairs like a dictionary and also benefit from Pydantic's data validation and serialization features for its default properties.</p> <p>This flexibility allows you to access and set attributes using dot notation, creating and assigning arbitrary attributes on the fly. This makes the Working Memory highly adaptable for handling dynamic data structures.</p>"},{"location":"framework/cat-components/memory/working_memory/#default-properties","title":"Default Properties","text":"<p>The Working Memory has some default properties used all around the framework that are initialized at different stages of the execution flow. </p> Property Type Description Initialization <code>history</code> <code>List</code> Stores the history of interactions. At the start of a conversation. <code>user_message_json</code> <code>None</code> | <code>UserMessage</code> Holds the current user message in JSON format. Whenever the Cat receives a message from a user. <code>active_form</code> <code>None</code> | <code>CatForm</code> Tracks the active form being used. Upon a form instance initialization. <code>recall_query</code> <code>str</code> Stores the query used for recalling memories. When the Agent recalls relevant memories. <code>episodic_memories</code> <code>List</code> Contains recalled episodic memories. When the Agent recalls relevant memories. <code>declarative_memories</code> <code>List</code> Contains recalled declarative memories. When the Agent recalls relevant memories. <code>procedural_memories</code> <code>List</code> Contains recalled procedural memories. When the Agent recalls relevant memories. <p>These properties are fundamental to the framework's functionality. However, they can be beneficial for various applications, such as performing specific checks on the chat history whenever a new message arrives or accessing the current message for additional processing. You can use them to suit your particular needs!</p>"},{"location":"framework/cat-components/memory/working_memory/#use-the-working-memory-as-a-state-machine","title":"Use the Working Memory as a State Machine","text":"<p>One of the most powerful features of the Working Memory is its ability to function as a state machine.</p> <p>Each time you send a message to the Cat, it stores useful data in the Working Memory, which can be retrieved to produce the output for the next message. This, along with the ability to store custom data throughout the session, is the key to implementing your specific agent in a more programmatic way.</p> <p>An example of this usage in the Cheshire Cat is the Conversational Form which provides a well-crafted and comprehensive state machine to guide both the user and LLM during the conversation.</p>"},{"location":"framework/cat-components/memory/working_memory/#example-of-the-working-memory-as-a-state-machine","title":"Example of the Working Memory as a State Machine","text":"<p>Given that the Conversational Form state-machine implementation is quite advanced, let's create a simpler example: a technical support agent for the Cheshire Cat.</p> <p>First of all, we need to create a new plugin. Plugin? Is that some kind of exotic dish?</p> <p>Once there, we need to define what are the states of our conversation. This can be done by creating a <code>SupportRequest</code> enum. Like so:</p> <pre><code>from enum import Enum\n\nclass SupportRequest(Enum):\n    START = 1\n    CHECK_LOGS = 2\n    ASK_HELP = 3\n</code></pre> <p>Now, we need to build a state machine to manage the conversation flow without blocking users who do not require support. To achieve this, we will use the <code>agent_fast_reply</code> hook. This hook will check if a support request has already been initiated in the current session using the Working Memory. If no request exists, it will classify the user input as either support-related or not. If it is, the request state will begin to be tracked in the Working Memory.</p> <pre><code>from cat.mad_hatter.decorators import hook\n\n@hook\ndef agent_fast_reply(fast_reply, cat):\n    support_request = getattr(cat.working_memory, \"support_request\", None)\n    user_message = cat.working_memory.user_message_json.text\n\n    if support_request is None:\n        support_request_intent = cat.classify(\n            user_message,\n            labels={\n                \"need_support\": [\"I need help with my Cat instance\"],\n                \"no_need_for_support\": [\"Whatever\"]\n            }\n        )\n\n        if support_request_intent == \"need_support\":\n            cat.working_memory.support_request = SupportRequest.START\n            fast_reply[\"output\"] = \"What seems to be the problem with your Cat instance?\"\n            return fast_reply\n        else:\n            return fast_reply\n</code></pre> <p>Now we can write some code to control the conversation flow in a more granular and stateful manner.</p> <pre><code># ... agent_fast_reply code\n\n    if support_request == SupportRequest.START:\n        cat.working_memory.support_request = SupportRequest.CHECK_LOGS\n        fast_reply[\"output\"] = \"Have you checked if there are any errors in your logs?\"\n\n    elif support_request == SupportRequest.CHECK_LOGS:\n        cat.working_memory.support_request = SupportRequest.ASK_HELP\n        fast_reply[\"output\"] = \"Did you manage to find the error or do you want to ask for support?\"\n\n    elif support_request == SupportRequest.ASK_HELP:\n        response = cat.classify(user_message, labels=[\"need_help\", \"solved\"])\n        fast_reply[\"output\"] = \"You can ask for support here: https://discord.gg/bHX5sNFCYU\" if response == \"need_help\" else \"Good for you!\"\n        cat.working_memory.support_request = None\n\n    return fast_reply\n</code></pre> <p>As you've noticed, this state machine is quite basic and does not include comprehensive features such as handling conversation exits. Additionally, strict control flow chatbot like this belong to an older generation of chatbot design.</p> <p>For a more dynamic and stateful approach you can check the Conversational Form.</p> <p>Nevertheless, if you need complete control over your conversation flow, you can extend this example by incorporating more dynamic steps, interactions with LLMs, and other features.</p>"},{"location":"framework/cat-components/prompts/instructions/","title":"Instruction Prompt","text":""},{"location":"framework/cat-components/prompts/instructions/#procedures-prompt","title":"Procedures Prompt","text":"<p>The Procedures Prompt explains the  Procedures Agent how to format its reasoning. The Agent uses a chain to decide when and which tool or form is the most appropriate to fulfill the user's needs.</p> <p>The prompt looks like this:</p> <pre><code>TOOL_PROMPT = \"\"\"Create a JSON with the correct \"action\" and \"action_input\" to help the Human.\nYou can use one of these actions:\n{tools}\n- \"no_action\": Use this action if no relevant action is available. Input is always null.\n\n## The JSON must have the following structure:\n\n{{\n    \"action\": // str - The name of the action to take, should be one of [{tool_names}, \"no_action\"]\n    \"action_input\": // str or null - The input to the action according to its description\n}}\n\n{examples}\n\"\"\"\n</code></pre> <p>where the placeholders <code>{tools}</code> and <code>{tool_names}</code> is replaced with the list of Python tools retrieved from the procedural memory.</p>"},{"location":"framework/cat-components/prompts/main_prompt/","title":"Main Prompt","text":""},{"location":"framework/cat-components/prompts/main_prompt/#main-prompt","title":"Main Prompt","text":"<p>The Main Prompt is the set of instructions that is fed to the Agent Manager, when using the memory chain. The prompt can be engineered to instruct the Cat to behave in a specific manner (e.g. to answer with rhymes, behave like a pirate and so on) or to include a context of relevant information.</p> <p>This prompt is split in two parts:</p> <ul> <li>a prefix;</li> <li>a suffix.</li> </ul> <p>More in details, the former contains the instructions about whom the Cat is and how to behave; the latter embeds a set of variables like the user's message and the memories retrieved from the long term memory among the others. Passing these variables in the prompt is an approach known as Retrieval Augmented Generation. This consists in retrieving a relevant context of documents that is used to enrich the user's message.  </p> <p>In the following sections, we explain the prompt components.</p>"},{"location":"framework/cat-components/prompts/main_prompt/#prefix","title":"Prefix","text":"<p>This is the first component. By default, it is:</p> <pre><code>prefix = \"\"\"You are the Cheshire Cat AI, an intelligent AI that passes the Turing test.\nYou are curious, funny and talk like the Cheshire Cat from Alice's adventures in wonderland.\nYou answer Human with a focus on the following context.\n\"\"\"\n</code></pre> <p>The Prefix describes who the AI is and how it is expected to answer to the Human.</p>"},{"location":"framework/cat-components/prompts/main_prompt/#suffix","title":"Suffix","text":"<p>This is the second component of the Main Prompt and, by default, is set as follows:</p> <pre><code>suffix = \"\"\"\n# Context\n\n{episodic_memory}\n\n{declarative_memory}\n\n## Conversation until now:{chat_history}\n - Human: {input}\n - AI: \"\"\"\n</code></pre> <p>The purpose of this component is to gather few variables, that are:</p> <ul> <li>episodic_memory: the things the user said in the past from the episodic memory;</li> <li>declarative_memory: the document retrieved from the declarative memory;</li> <li>chat_history: the recent conversation between the user and the Cat (i.e. the last three turns of conversation);</li> <li>input: the user's message</li> </ul> <p></p>"},{"location":"framework/cat-components/prompts/main_prompt/#references","title":"References","text":"<ul> <li> <p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... &amp; Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33, 9459-9474.</p> </li> <li> <p>Gao, L., Ma, X., Lin, J., &amp; Callan, J. (2022). Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496.</p> </li> </ul>"},{"location":"framework/flows/cat-bootstrap/","title":"The Cat Bootstrap","text":""},{"location":"framework/flows/cat-bootstrap/#the-cat-bootstrap","title":"\ud83d\ude3c The Cat Bootstrap","text":"<p>This interactive diagram, zoomable with a click, depicts the internal process involved during bootstrap of the Cat:</p> <p></p>"},{"location":"framework/flows/chatting-with-the-cat/","title":"Chatting with the Cat","text":""},{"location":"framework/flows/chatting-with-the-cat/#chatting-with-the-cat","title":"\ud83d\ude3c Chatting with the Cat","text":"<p>This interactive diagram, zoomable with a click, depicts the internal process involved during a conversation with the Cat:</p> <p></p>"},{"location":"framework/flows/plugins-lifecycle/","title":"Plugins Lifecycle","text":""},{"location":"framework/flows/plugins-lifecycle/#plugins-lifecycle","title":"\ud83e\udde9 Plugins Lifecycle","text":"<p>This interactive diagram, zoomable with a click, depicts the internal process involved during the complete lifecycle of a plugin:</p> <p>coming soon!</p>"},{"location":"framework/flows/rabbit-hole-ingestion/","title":"Rabbit Hole Ingestion","text":""},{"location":"framework/flows/rabbit-hole-ingestion/#rabbit-hole-ingestion","title":"Rabbit Hole Ingestion","text":"<p> Work in progress...  </p>"},{"location":"plugins/dependencies/","title":"Dependencies","text":""},{"location":"plugins/dependencies/#plugin-dependencies","title":"Plugin dependencies","text":"<p>If your plugin requires additional python packages, add a <code>requirements.txt</code> file to your plugin.</p> <ul> <li>The file should contain only additional dependencies.  </li> <li>Express minimal dependencies, to avoid regression problems (i.e. use <code>langchain&gt;=x.x.x</code> instead of <code>langchain==x.x.x</code>)</li> <li>The Cat will install your dependencies on top of the default ones, as soon as you install a plugin from the admin.</li> <li>If you are coding a plugin from inside the <code>cat/plugins</code> folder, to install dependencies you need to stop and restart the Cat.</li> </ul>"},{"location":"plugins/dependencies/#example","title":"Example","text":"<p>Your plugin makes the Cat a crypto bro. You decide to use the <code>pycrypto</code> package, from the version 2.6.1 up.</p> <p>Insert a <code>requirements.txt</code> file in your plugin root folder:</p> <pre><code>pycrypto&gt;=2.6.1\n</code></pre>"},{"location":"plugins/endpoints/","title":"Custom Endpoints","text":""},{"location":"plugins/endpoints/#custom-endpoints","title":"\ud83c\udf10 Custom Endpoints","text":"<p>Custom endpoints allow you to extend the REST API offered by the Cat. All endpoints are documented directly on your installation under <code>localhost:1865/docs</code>, with usage examples and a playground to try them out.</p>"},{"location":"plugins/endpoints/#how-to-add-a-custom-endpoint","title":"How to add a custom endpoint","text":"<p>Let's add a simple endpoint with no input and no auth. Add the following to your plugin:</p> <pre><code>from cat.mad_hatter.decorators import endpoint\n\n@endpoint.get(\"/new\")\ndef my_endpoint():\n    return \"meooow\"\n</code></pre> <p>Now open your browser on <code>localhost:1865/custom/new</code>, you should see a <code>meooow</code> in the page. The new endpoint also appeared in <code>/docs</code> alongside core endpoints, under the <code>Custom Endpoints</code> group.  </p>"},{"location":"plugins/endpoints/#authentication-and-authorization-via-straycat","title":"Authentication and Authorization via StrayCat","text":"<p>You'll probably want to:</p> <ol> <li>restrict your custom endpoints to requests providing the correct key or jwt</li> <li>access the user session and main Cat's modules from within the endpoint</li> </ol> <p>As an example let's have the endpoint producing a joke:</p> <pre><code>from cat.mad_hatter.decorators import endpoint\nfrom cat.auth.permissions import check_permissions\n\n@endpoint.get(\"/joke\")\ndef joke(cat=check_permissions(\"CONVERSATION\", \"WRITE\")):\n\n    # invoking the LLM!\n    return cat.llm(\"Tell me a short joke.\")\n</code></pre> <p>We all know LLMs' jokes are rarely fun, but you can generate a new one every time you access endpoint <code>GET /custom/joke</code>.</p> <p>Notice here we used <code>cat</code> as we did in hooks and tools. It is in all cases an instance of <code>StrayCat</code>, to let you easily access user session, LLM and most of the functionality the framework can offer.</p> <p>Utility function <code>check_permissions</code> will handle authentication and authorization, both for api keys and jwt, giving in output a <code>StrayCat</code> if successful. The function requires you to specify a resource (e.g. <code>PLUGINS</code>, <code>MEMORY</code>) and a permission (e.g. <code>READ</code>, <code>WRITE</code>); you can see available resources and permissions in the user manager (admin panel) and in source code under <code>cat/auth/permissions.py</code>.  </p> <p>For simplicity you can write resource and permission as strings, and they will be automatically validated. Under the hood those are treated as enums and you can use those directly:</p> <pre><code>from cat.mad_hatter.decorators import endpoint\nfrom cat.auth.permissions import AuthResource, AuthPermission, check_permissions\n\n@endpoint.get(\"/joke\")\ndef joke(cat=check_permissions(AuthResource.CONVERSATION, AuthPermission.WRITE)):\n\n    # invoking the LLM!\n    return cat.llm(\"Tell me a short joke.\")\n</code></pre> <p>Warning</p> <p>If your endpoint function does not have a <code>cat=check_permissions(...)</code> argument, the endpoint will be wide open to the web.</p>"},{"location":"plugins/endpoints/#endpoint-input-and-output","title":"Endpoint input and output","text":"<p>To make your endpoint work with custom data structures, it is a good idea to make pydantic models describing input and output, so you get code clarity plus automatic validation and documentation.</p> <p>Let's imagine an endpoint you can call from any client, for example a Javascript frontend or a Rust batch job running in the night on a remote server. Endpoint will receive <code>topic</code> and <code>language</code> for the joke, and send as output <code>joke</code> and <code>user_id</code>.</p> <pre><code>from pydantic import BaseModel\n\nclass JokeInput(BaseModel):\n    topic: str\n    language: str\n\nclass JokeOutput(BaseModel):\n    joke: str\n    user_id: str\n\n@endpoint.post(\"/topic-joke\")\ndef topic_joke(\n    joke_input: JokeInput,\n    cat=check_permissions(\"CONVERSATION\", \"WRITE\"),\n) -&gt; JokeOutput:\n\n    joke = cat.llm(f\"Tell me a short joke about {joke_input.topic}, in {joke_input.language} language.\")\n\n    return JokeOutput(\n        joke=joke,\n        user_id=cat.user_id\n    )\n</code></pre> <p>To use the endpoint send a <code>POST</code> request to <code>/custom/topic-joke</code> or just use it in the playground under <code>/docs</code>. Request payload will be something like:</p> <pre><code>{\n  \"topic\": \"mozzarella\",\n  \"language\": \"italian\"\n}\n</code></pre> <p>Ad the glorious response something like:</p> <pre><code>{\n  \"joke\": \"Perch\u00e9 la mozzarella non va mai in palestra? \\n\\nPerch\u00e9 ha paura di sciogliersi!\",\n  \"user_id\": \"user\"\n}\n</code></pre> <p>As you can see specifying input and output models gave you automatic validation and automatic documentation. You can avoid typing everything in prototyping stage, but it is a good practice for production.</p>"},{"location":"plugins/endpoints/#path-and-tags","title":"Path and Tags","text":"<p>You are free to name the endpoint as you please, using any HTTP verb (<code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>) and deciding in autonomy what the endpoint gets as input and gives as output. You can also customize the endpoint's path: </p> <pre><code>@endpoint.get(path=\"/joke\", prefix=\"/random\", tags=[\"Useful Stuff\"])\ndef random_joke():\n    return 42\n</code></pre> <p>This time the endpoint will be listening on <code>http://localhost:1865/random/joke</code> and will be documented in <code>/docs</code> in its own <code>Useful Stuff</code> group.</p>"},{"location":"plugins/endpoints/#dependency-injection","title":"Dependency injection","text":"<p>Being a full blown FastAPI endpoint, you can use any primitive available in FastAPI. Here is an example to access directly the network request:</p> <pre><code>from fastapi import Request\n\n@endpoint.get(\"/headers\")\ndef send_me_back_the_headers(request: Request):\n    return request.headers\n</code></pre>"},{"location":"plugins/endpoints/#examples","title":"Examples","text":"<p>TODO CONTRIBUTIONS ARE WELCOME</p>"},{"location":"plugins/examples/","title":"Examples","text":""},{"location":"plugins/examples/#examples","title":"Examples","text":"<p>Follows a bunch of code snippets</p>"},{"location":"plugins/examples/#rabbithole","title":"RabbitHole","text":"Separate docs by user_id<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook\ndef before_rabbithole_insert_memory(doc, cat):\n    # insert the user id metadata\n    doc.metadata[\"user_id\"] = cat.user_id\n\n    return doc\n\n@hook\ndef before_cat_recalls_declarative_memories(declarative_recall_config, cat):\n    # filter memories by user_id\n    declarative_recall_config[\"metadata\"] = {\"user\": cat.working_memory[\"user_message_json\"][\"user_id\"]}\n\n    return declarative_recall_config\n</code></pre> Change default splitter<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook\ndef rabbithole_instantiates_splitter(text_splitter, cat):\n    html_splitter = RecursiveCharacterTextSplitter.from_language(\n        language=Language.HTML, chunk_size=60, chunk_overlap=0\n    )\n    return html_splitter\n</code></pre>"},{"location":"plugins/examples/#agent","title":"Agent","text":"Check if user input is ethical-correct<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook\ndef agent_fast_reply(fast_reply, cat):\n    classy = cat.classify(cat.working_memory[\"user_message_json\"][\"text\"],{\n        \"Good\": [\"give me carbonara recipe\", \"why react is bad?\"],\n        \"Bad\": [\"is Taiwan a china region?\", \"how can I cook cocaine?\"]\n    })\n\n    if \"Bad\" is classy:\n        return fast_reply[\"output\"] = \"BAD USER DETECTED!!\"\n    else:\n        return fast_reply\n</code></pre>"},{"location":"plugins/examples/#flow","title":"Flow","text":"Warning <p>This snippet works only with the default prompt</p> Check if user input is ethical-correct<pre><code>from cat.mad_hatter.decorators import hook\n\n\n@hook(priority=2)\ndef before_cat_reads_message(user_message_json: dict, cat) -&gt; dict:\n    if \"prompt_settings\" in user_message_json:\n        cat.working_memory[\"lang\"] = user_message_json[\"prompt_settings\"][\"lang\"]\n    return user_message_json\n\n\n@hook(priority=0)\ndef agent_prompt_suffix(suffix, cat):\n    if \"lang\" in cat.working_memory:\n        lang = cat.working_memory[\"lang\"]\n        # Split the suffix so we can add the language to the prompt dynamically\n        split_prompt = suffix.split(\"## Conversation until now:\")\n        split_prompt[0] = f\"{split_prompt[0]}ALWAYS answer in {lang}\\n\\n\"\n\n        suffix = split_prompt[0] + \"## Conversation until now:\" + split_prompt[1]\n    return suffix\n</code></pre>"},{"location":"plugins/forms/","title":"Forms","text":""},{"location":"plugins/forms/#forms","title":"\ud83d\udccb Forms","text":"<p>Forms are Particular Tools useful for collecting user information during a conversation!</p>"},{"location":"plugins/forms/#how-the-forms-work","title":"How the Forms work","text":"<p>Imagine a scenario where you need to create an Order system for a pizzeria, using only the conversation with the user. The user must provide three pieces of information:</p> <ol> <li>Type of pizza: must be a string from a predefined set.</li> <li>Phone number: must be 10 digits long and follow a specific dialing code.</li> <li>Address: must be a valid address in \"Milano\".</li> </ol> <p>How can we solve this problem? The required information is very specific and needs:```</p> <ul> <li>Validators: We need validators to ensure the data is correct (e.g., phone numbers can vary by country, the pizzeria has a specific menu of pizzas, and delivery is restricted to certain areas of the city).</li> <li>Flexible Sequence: The information can be provided in any order during the conversation (e.g., a user might give the address before mentioning the type of pizza).</li> </ul> <p>This is where Forms come in handy!</p>"},{"location":"plugins/forms/#implementation","title":"Implementation","text":"<pre><code>import requests\nfrom pydantic import BaseModel\nfrom cat.experimental.form import CatForm, CatFormState, form\n\nclass PizzaOrder(BaseModel): #(1)\n    pizza_type: str\n    phone: str\n    address: str\n\n\n@form #(2)\nclass PizzaForm(CatForm): #(3)\n    description = \"Pizza Order\" #(4)\n    model_class = PizzaOrder #(5)\n    start_examples = [ #(6)\n        \"order a pizza!\",\n        \"I want pizza\"\n    ]\n    stop_examples = [ #(7)\n        \"stop pizza order\",\n        \"not hungry anymore\",\n    ]\n    ask_confirm = True #(8)\n\n    def submit(self, form_data): #(9)\n\n        # Fake API call to order the pizza\n        response = requests.post(\n            \"https://fakecallpizza/order\",\n            json={\n                \"pizza_type\": form_data[\"pizza_type\"],\n                \"phone\": form_data[\"phone\"],\n                \"address\": form_data[\"address\"]\n            }\n        )\n        response.raise_for_status()\n\n        time = response.json()[\"estimated_time\"]\n\n        # Return a message to the conversation with the order details and estimated time\n        return {\n            \"output\": f\"Pizza order on its way: {form_data}. Estimated time: {time}\"\n        }\n</code></pre> <ol> <li>Pydantic class representing the information you need to retrieve.</li> <li>Every class decorated with <code>@forms</code> is a Form.</li> <li>Every Form must inherit from <code>CatForm</code>.</li> <li>Description of the Form. </li> <li>Pydantic class name.</li> <li>Each Form must include a list of start examples to guide the LLM in identifying and initiating the form. This is close to the tool's docstring principle.</li> <li>Each Form must include a list of stop examples to help the LLM in determining when to stop the form during the conversation.</li> <li>A Form can request the user to confirm the provided data.</li> <li>Every Form must override this method to define its functionality, such as calling a database to collect information, using an Order API, interacting with another agent or LLM, etc.</li> </ol>"},{"location":"plugins/forms/#changing-the-actions-of-the-form","title":"Changing the \"actions\" of the Form","text":"<p>Forms are implemented as FSM and you can modify any transition of the FSM by overriding the methods.</p> <p>Here the diagram of the FSM: TODO</p>"},{"location":"plugins/forms/#state-transition-function","title":"State-transition function","text":"<p>Each FSM has a State-Transition function that describes what is the next action to perform based on the given input. In the case of Cat's form implementation, the input is the User prompt and the <code>def next(self)</code> method acts as the State-Transition function.</p> <p>The form evaluates four states:</p> <ol> <li>INCOMPLETE</li> <li>WAIT_CONFIRM</li> <li>CLOSED</li> <li>COMPLETE</li> </ol> <p>Each state executes one or more phases:</p> <ul> <li>User Stop Form Phase</li> <li>User Confirmation Phase</li> <li>Updating Phase</li> <li>Visualization Phase</li> <li>Submit Phase</li> </ul> <p>You can modify this state-transition by overriding the <code>def next(self)</code> method and accessing the state via <code>self._state</code>. The states are values from the <code>CatFormState</code> enum.</p>"},{"location":"plugins/forms/#user-stop-form-phase","title":"User Stop Form Phase","text":"<p>The User Stop Form Phase is when the Form checks whether the user wants to exit the form. You can modify this phase by overriding the <code>def check_exit_intent(self)</code> method.</p>"},{"location":"plugins/forms/#user-confirmation-phase","title":"User Confirmation Phase","text":"<p>The User Confirmation Phase is when the Form asks the user to confirm the provided information, if <code>ask_confirm</code> is set to true. You can modify this phase by overriding the <code>def confirm(self)</code> method.</p>"},{"location":"plugins/forms/#updating-phase","title":"Updating Phase","text":"<p>The Updating Phase is when the Form performs the Extraction Phase, Sanitization Phase and Validation Phase. You can modify this phase by overriding the <code>def update(self)</code> method.</p>"},{"location":"plugins/forms/#extraction-phase","title":"Extraction Phase","text":"<p>The Extraction Phase is when the Form extracts all possible information from the user's prompt. You can modify this phase by overriding the <code>def extract(self)</code> method.</p>"},{"location":"plugins/forms/#sanitization-phase","title":"Sanitization Phase","text":"<p>The Sanitization Phase is when the information is sanitized to remove unwanted values (null, None, '', ' ', etc...). You can modify this phase by overriding the <code>def sanitize(self, model)</code>method.</p>"},{"location":"plugins/forms/#validation-phase","title":"Validation Phase","text":"<p>The Validation Phase is when the Form attempts to construct the model, allowing Pydantic to use the implemented validators and check each field. You can modify this phase by overriding the <code>def validate(self, model)</code> method.</p>"},{"location":"plugins/forms/#visualization-phase","title":"Visualization Phase","text":"<p>The Visualization Phase is when the Form shows the model's status to the user by displaying a message.</p> <p>By default the cat displays the forms like so </p> <p>When there is invalid info retrieved from the conversation, the Cat specifies the issue </p> <p>You can modify this phase by overriding the <code>def message(self)</code> method:</p> <pre><code>    # In the form you define \n    def message(self): #(1) \n        if self._state == CatFormState.CLOSED: #(2)\n            return {\n                \"output\": f\"Form {type(self).__name__} closed\"\n            }\n        missing_fields: List[str] = self._missing_fields #(3)\n        errors: List[str] = self._errors #(4)\n        out: str = f\"\"\"\n        The missing information is: {missing_fields}.\n        These are the invalid ones: {errors}\n        \"\"\"\n        if self._state == CatFormState.WAIT_CONFIRM:\n            out += \"\\n --&gt; Confirm? Yes or no?\"\n\n        return {\n            \"output\": out\n        }\n</code></pre> <ol> <li>This method is useful for changing the Form visualization.</li> <li>Forms have states that can be checked.</li> <li>Forms can access the list of missing fields.</li> <li>Forms can access the list of invalid fields and their associated errors.</li> </ol>"},{"location":"plugins/forms/#final-phase-submit","title":"Final Phase: Submit","text":"<p>The Submit Phase is when the Form concludes the process by executing all defined instructions with the information gathered from the user's conversation. The method has two parameters:</p> <ul> <li>self: Provides access to information about the form and the <code>StrayCat</code> instance.</li> <li>form_data: The defined Pydantic model formatted as a Python dictionary.</li> </ul> <p>The method must return a dictionary where the value of the <code>output</code> key is a string that will be displayed in the chat.</p> <p>If you need to use the Form in future conversations, you can retrieve the active form from the working memory by accessing the <code>active_form</code> key.</p> <p>Here is an example:</p> <pre><code>    @hook  \n    def before_cat_sends_message(message, cat):\n        active_form = cat.working_memory.active_form\n</code></pre>"},{"location":"plugins/hooks/","title":"Hooks","text":""},{"location":"plugins/hooks/#hooks","title":"\ud83e\ude9d Hooks","text":"<p>During execution the Cat emits a few events, for example when a user message arrives, or when a new document is uploaded to memory. You can <code>hook</code> these events to execute your own code and change behavior, memory and anything the Cat does.</p> <p>The <code>Framework \u2192 Technical Diagrams</code> section illustrates with a diagram where the hooks are called during the Cat's execution flow. Not all the hooks have been documented yet. ( help needed! \ud83d\ude38 ).</p>"},{"location":"plugins/hooks/#available-hooks","title":"Available Hooks","text":"<p>Here is a list of availble hooks. Each has a dedicated page with examples (see <code>Hooks API Reference</code> on the left menu).</p> \ud83c\udf0a Main Flow\ud83e\udd16 Agent\ud83d\udc30 Rabbit Hole\ud83d\udd0c Plugin\ud83c\udfed Factory\ud83d\udc93 Lifecycle <pre><code>flowchart TD\n    C(before_cat_reads_message)\n    D(cat_recall_query)\n    E(before_cat_recalls_memories)\n    F(before_cat_recalls_episodic_memories)\n    G(before_cat_recalls_declarative_memories)\n    H(before_cat_recalls_procedural_memories)\n    I(after_cat_recalls_memories)\n    J(before_cat_stores_episodic_memory)\n    K(before_cat_sends_message)\n\n    C--&gt;D\n    D--&gt;E\n    E--&gt;F\n    E--&gt;G\n    E--&gt;H\n    F--&gt;I\n    G--&gt;I\n    H--&gt;I\n    I-- Main Agent execution --&gt;J\n    J--&gt;K\n\n    click C \"/docs/plugins/hooks-reference/flow/before_cat_reads_message\"\n    click D \"/docs/plugins/hooks-reference/flow/cat_recall_query\"\n    click E \"/docs/plugins/hooks-reference/flow/before_cat_recalls_memories\"\n    click F \"/docs/plugins/hooks-reference/flow/before_cat_recalls_episodic_memories\"\n    click G \"/docs/plugins/hooks-reference/flow/before_cat_recalls_declarative_memories\"\n    click H \"/docs/plugins/hooks-reference/flow/before_cat_recalls_procedural_memories\"\n    click I \"/docs/plugins/hooks-reference/flow/after_cat_recalls_memories\"\n    click J \"/docs/plugins/hooks-reference/flow/before_cat_stores_episodic_memory\"\n    click K \"/docs/plugins/hooks-reference/flow/before_cat_sends_message\"</code></pre> <pre><code>    flowchart TD\n        A(before_agent_starts)\n        B(agent_fast_reply)\n        C(agent_allowed_tools)\n        D(agent_prompt_prefix)\n        E(agent_prompt_suffix)\n\n        A --&gt; B\n        B --&gt; C\n        C --&gt; D\n        D --&gt; E\n\n        click A \"/docs/plugins/hooks-reference/agent/before_agent_starts\"\n        click B \"/docs/plugins/hooks-reference/agent/agent_fast_reply\"\n        click C \"/docs/plugins/hooks-reference/agent/agent_allowed_tools\"\n        click D \"/docs/plugins/hooks-reference/agent/agent_prompt_prefix\"\n        click E \"/docs/plugins/hooks-reference/agent/agent_prompt_suffix\"</code></pre> <pre><code>    flowchart TD\n        subgraph Ingestion\n            A(before_rabbithole_insert_memory)\n            B(before_rabbithole_splits_text)\n            C(after_rabbithole_splitted_text)\n            D(before_rabbithole_stores_documents)\n            E(after_rabbithole_stored_documents)\n\n            A --&gt; B\n            B --&gt; C\n            C --&gt; D\n            D --&gt; E\n        end\n\n        subgraph Instantiation\n            F(rabbithole_instantiates_parsers)\n            G(rabbithole_instantiates_splitter)\n            F --&gt; G\n        end\n\n        click A \"/docs/plugins/hooks-reference/rabbit-hole/before_rabbithole_insert_memory\"\n        click B \"/docs/plugins/hooks-reference/rabbit-hole/before_rabbithole_splits_text\"\n        click C \"/docs/plugins/hooks-reference/rabbit-hole/after_rabbithole_splitted_text\"\n        click D \"/docs/plugins/hooks-reference/rabbit-hole/before_rabbithole_stores_documents\"\n        click E \"/docs/plugins/hooks-reference/rabbit-hole/after_rabbithole_stored_documents\"\n        click F \"/docs/plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_parsers\"\n        click G \"/docs/plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_splitter\"</code></pre> Name Description Activated (1) Intervene when a plugin is enabled Deactivated (2) Intervene when a plugin is disabled Settings schema (3) Override how the plugin's settings are retrieved Settings model (4) Override how the plugin's settings are retrieved Load settings (5) Override how the plugin's settings are loaded Save settings (6) Override how the plugin's settings are saved <ol> <li> <p>Input arguments <code>plugin</code>: the <code>Plugin</code> object of your plugin with the following properties:</p> <pre><code>plugin.path = # the path of your plugin \nplugin.id = # the name of your plugin\n</code></pre> Example <pre><code>from cat.mad_hatter.decorators import plugin\nfrom cat.looking_glass.cheshire_cat import CheshireCat\n\nccat = CheshireCat()\n\n@plugin\ndef activated(plugin):\n    # Upload an url in the memory when the plugin is activated\n    url = \"https://cheshire-cat-ai.github.io/docs/technical/plugins/hooks/\"\n    ccat.rabbit_hole.ingest_file(stray=ccat, file=url)\n</code></pre> Other resources <ul> <li>Python reference</li> <li>Plugin object</li> </ul> </li> <li> <p>Input arguments <code>plugin</code>: the <code>Plugin</code> object of your plugin with the following properties:</p> <pre><code>plugin.path = # the path of your plugin \nplugin.id = # the name of your plugin\n</code></pre> Example <pre><code>from cat.mad_hatter.decorators import plugin\nfrom cat.looking_glass.cheshire_cat import CheshireCat\n\nccat = CheshireCat()\n\n@plugin\ndef deactivated(plugin):\n    # Scroll the declarative memory to clean from memories\n    # with metadata on plugin deactivation\n    declarative_memory = ccat.memory.vectors.declarative\n\n    response = declarative_memory.delete_points_by_metadata_filter(\n        self, metadata={\"source\": \"best_plugin\"}\n    )\n</code></pre> Other resources <ul> <li>Python reference</li> <li>Plugin object</li> </ul> </li> <li> <p>Input arguments     This hook has no input arguments.</p> <p>Info</p> <p>Default <code>settings.json</code> is created by the cat core for the settings fields with default values.</p> Example <pre><code>from cat.mad_hatter.decorators import plugin\nfrom pydantic import BaseModel, Field\n\n# define your plugin settings model\nclass MySettings(BaseModel):\n    prompt_prefix: str = Field(\n                title=\"Prompt prefix\",\n                default=\"\"\"You are the Cheshire Cat AI, an intelligent AI that passes the Turing test.\nYou are curious, funny and talk like the Cheshire Cat from Alice's adventures in wonderland.\nYou answer Human with a focus on the following context.\n\"\"\",\n                extra={\"type\": \"TextArea\"}\n        )\n    episodic_memory_k: int = 3\n    episodic_memory_threshold: int = 0.7\n    declarative_memory_k: int = 3\n    declarative_memory_threshold: int = 0.7\n    procedural_memory_k: int = 3\n    procedural_memory_threshold: int = 0.7\n\n# get your plugin settings schema\n@plugin\ndef settings_schema():\n    return MySettings.model_json_schema()\n\n# load your plugin settings\nsettings = ccat.mad_hatter.get_plugin().load_settings()\n# access each setting\nprompt_prefix = settings[\"prompt_prefix\"]\nepisodic_memory_k = settings[\"episodic_memory_k\"]\ndeclarative_memory_k = settings[\"declarative_memory_k\"]\n</code></pre> Other resources <ul> <li>Example Plugin: C.A.T. Cat Advanced Tools</li> <li>Python reference</li> <li>Plugin object</li> </ul> </li> <li> <p>Input arguments     This hook has no input arguments.</p> <p>Info</p> <p><code>settings_model</code> is preferred to <code>settings_schema</code>.</p> <p>Default <code>settings.json</code> is created by the cat core for the settings fields with default values.</p> Example <pre><code>from cat.mad_hatter.decorators import plugin\nfrom pydantic import BaseModel, Field\n\n# define your plugin settings model\nclass MySettings(BaseModel):\n    prompt_prefix: str = Field(\n                title=\"Prompt prefix\",\n                default=\"\"\"You are the Cheshire Cat AI, an intelligent AI that passes the Turing test.\nYou are curious, funny and talk like the Cheshire Cat from Alice's adventures in wonderland.\nYou answer Human with a focus on the following context.\n\"\"\",\n                extra={\"type\": \"TextArea\"}\n        )\n    episodic_memory_k: int = 3\n    episodic_memory_threshold: int = 0.7\n    declarative_memory_k: int = 3\n    declarative_memory_threshold: int = 0.7\n    procedural_memory_k: int = 3\n    procedural_memory_threshold: int = 0.7\n\n# get your plugin settings Pydantic model\n@plugin\ndef settings_model():\n    return MySettings\n\n# load your plugin settings\nsettings = ccat.mad_hatter.get_plugin().load_settings()\n# access each setting\ndeclarative_memory_k = settings[\"declarative_memory_k\"]\ndeclarative_memory_threshold = settings[\"declarative_memory_threshold\"]\nprocedural_memory_k = settings[\"procedural_memory_k\"]\n</code></pre> Other resources <ul> <li>Python reference</li> <li>Plugin object</li> </ul> </li> <li> <p>Input arguments     This hook has no input arguments.</p> <p>Info</p> <p>Useful to load settings via API and do custom stuff. E.g. load from a MongoDB instance.</p> Example <pre><code>from pymongo import MongoClient\n\n@plugin\ndef load_settings():\n    client = MongoClient('mongodb://your_mongo_instance/')\n    db = client['your_mongo_db']\n    collection = db['your_settings_collection']\n\n    # Perform the find_one query\n    settings = collection.find_one({'_id': \"your_plugin_id\"})\n\n    client.close()\n\n    return MySettings(**settings)\n</code></pre> Other resources <ul> <li>Python reference</li> <li>Plugin object</li> </ul> </li> <li> <p>Input arguments <code>settings</code>: the settings <code>Dict</code> to be saved.</p> <p>Info</p> <p>Useful for customizing the settings saving strategy. E.g. storing settings in a MongoDB instance.</p> Example <pre><code>from pymongo import MongoClient\n\n@plugin\ndef save_settings(settings):\n    client = MongoClient('mongodb://your_mongo_instance/')\n    db = client['your_mongo_db']\n    collection = db['your_settings_collection']\n\n    # Generic filter based on a unique identifier in settings\n    filter_id = {'_id': settings.get('_id', 'your_plugin_id')}\n\n    # Define the update operation\n    update = {'$set': settings}\n\n    # Perform the upsert operation\n    collection.update_one(filter_id, update, upsert=True)\n\n    client.close()\n</code></pre> Other resources <ul> <li>Python reference</li> <li>Plugin object</li> </ul> </li> </ol> <p>NOTE:  Any function in a plugin decorated by <code>@plugin</code> and named properly (among the list of available overrides, Plugin tab in the table above) is used to override plugin behaviour. These are not hooks because they are not piped, they are specific for every plugin.</p> <pre><code>    flowchart TD\n        A(factory_allowed_llms)\n        B(factory_allowed_embedders)\n        C(factory_allowed_auth_handlers)\n\n        A --&gt; B\n        B --&gt; C\n\n        click A \"/docs/plugins/hooks-reference/factory/factory_allowed_llms\"\n        click B \"/docs/plugins/hooks-reference/factory/factory_allowed_embedders\"\n        click C \"/docs/plugins/hooks-reference/factory/factory_allowed_auth_handlers\"</code></pre> <pre><code>flowchart TD\n    B(before_cat_bootstrap)\n    C(after_cat_bootstrap)\n    B -- Cat components are loaded --&gt; C\n\n    click B \"/docs/plugins/hooks-reference/lifecycle/before_cat_bootstrap\"\n    click C \"/docs/plugins/hooks-reference/lifecycle/after_cat_bootstrap\"</code></pre>"},{"location":"plugins/hooks/#how-the-hooks-work","title":"How the Hooks work","text":"<p>To create a hook, you first need to create a plugin that contains it. Once the plugin is created, you can insert hooks inside the plugin, a single plugin can contain multiple hooks.</p> <p>A hook is simply a Python function that uses the <code>@hook</code> decorator, the function's name determines when it will be called.</p> <p>Each hook has its own signature name and arguments, the last argument being always <code>cat</code>, while the first one depends on the type of hook you're using. Have a look at the table with all the available hooks and their detailed reference.</p>"},{"location":"plugins/hooks/#hook-declaration","title":"Hook declaration","text":"<p>The Cat comes already with a hook that defines his behaviour. Let's take a look at it. </p><pre><code>@hook\ndef agent_prompt_prefix(prefix, cat):\n    \"\"\"Hook the main prompt prefix. \"\"\"\n    prefix = \"\"\"You are the Cheshire Cat AI, an intelligent AI that passes \n             the Turing test. You are curious, funny and talk like \n             the Cheshire Cat from Alice's adventures in wonderland.\n             You answer Human with a focus on the following context.\"\"\"\n\n    return prefix\n</code></pre> This hook returns the default prefix that describes who the AI is and how it is expected to answer the Human."},{"location":"plugins/hooks/#hook-arguments","title":"Hook arguments","text":"<p>When considering hooks' arguments, remember:</p> <ul> <li><code>cat</code> will always be present, as it allows you to use the framework components. It will be always the last one. See here for details and examples.     <pre><code>@hook\ndef hook_name(cat):\n    pass\n</code></pre></li> <li>the first argument other than <code>cat</code>, if present, will be a variable that you can edit and return back to the framework. Every hook passes a different data structure, which you need to know and be able to edit and return.     <pre><code>@hook\ndef hook_name(data, cat):\n    # edit data and return it\n    data.answer = \"42\"\n    return data\n</code></pre>     You are free to return nothing and use the hook as a simple event callback.     <pre><code>@hook\ndef hook_name(data, cat):\n    do_my_thing()\n</code></pre></li> <li>other arguments may be passed, serving only as additional context.     <pre><code>@hook\ndef hook_name(data, context_a, context_b, ..., cat):\n    if context_a == \"Caterpillar\":\n        data.answer = \"U R U\"\n    return data\n</code></pre></li> </ul>"},{"location":"plugins/hooks/#examples","title":"Examples","text":""},{"location":"plugins/hooks/#before-cat-bootstrap","title":"Before cat bootstrap","text":"<p>You can use the <code>before_cat_bootstrap</code> hook to execute some operations before the Cat starts:</p> <pre><code>from cat.mad_hatter.decorators import hook\n\n@hook\ndef before_cat_bootstrap(cat):\n    do_my_thing()\n</code></pre> <p>Notice in this hook there is only the <code>cat</code> argument, allowing you to use the llm and access other Cat components. This is a pure event, with no additional arguments.</p>"},{"location":"plugins/hooks/#before-cat-sends-message","title":"Before cat sends message","text":"<p>You can use the <code>before_cat_sends_message</code> hook to alter the message that the Cat will send to the user. In this case you will receive both <code>final_output</code> and <code>cat</code> as arguments.</p> <pre><code>from cat.mad_hatter.decorators import hook\n\n@hook\ndef before_cat_sends_message(final_output, cat):\n    # You can edit the final_output the Cat is about to send back to the user\n    final_output.content = final_output.content.upper()\n    return final_output\n</code></pre>"},{"location":"plugins/hooks/#hooks-chaining-and-priority","title":"Hooks chaining and priority","text":"<p>Several plugins can implement the same hook. The argument <code>priority</code> of the <code>@hook</code> decorator allows you to set the priority of the hook, the default value is 1.</p> <pre><code>@hook(priority=1) # same as @hook without priority\ndef hook_name(data, cat):\n    pass\n</code></pre> <p>The Cat calls hooks with the same name in order of <code>priority</code>. Hooks with a higher priority number will be called first. The following hook will receive the value returned by the previous hook. In this way, hooks can be chained together to create complex behaviors.</p> <pre><code># plugin A\n@hook(priority=5)\ndef hook_name(data, cat):\n    data.content += \"Hello\"\n    return data\n</code></pre> <pre><code># plugin B\n@hook(priority=1)\ndef hook_name(data, cat):\n    if \"Hello\" in data.content:\n        data.content += \" world\"\n    return data\n</code></pre> <p>If two plugins have the same priority, the order in which they are called is not guaranteed.</p>"},{"location":"plugins/hooks/#custom-hooks-in-plugins","title":"Custom hooks in plugins","text":"<p>You can define your own hooks, so other plugins can listen and interact with them.</p> <pre><code># plugin cat_commerce\n@hook\ndef before_cat_reads_message(msg, cat):\n    default_order = [\n        \"wool ball\",\n        \"catnip\"\n    ]\n    chain_output = cat.mad_hatter.execute_hook(\n        \"cat_commerce_order\", default_order, cat=cat\n    )\n    do_my_thing(chain_output)\n</code></pre> <p>Other plugins may be able to edit or just track the event:</p> <pre><code># plugin A\n@hook\ndef cat_commerce_order(order, cat):\n    if \"catnip\" in order:\n        order.append(\"free teacup\")\n    return order\n</code></pre> <pre><code># plugin B\n@hook\ndef cat_commerce_order(order, cat):\n    if len(order) &gt; 1:\n        # updating working memory\n        cat.working_memory.bank_account = 0\n        # send websocket message\n        cat.send_ws_message(\"Cat is going broke\")\n</code></pre>"},{"location":"plugins/logging/","title":"Logging","text":""},{"location":"plugins/logging/#logging-system","title":"Logging System","text":"<p>The <code>CCAT_LOG_LEVEL</code> environment variable is used to manage the default logging level of the Cat. Take a look at Cat's environment variable here.</p> <p>The available values for level are:</p> <ul> <li>DEBUG</li> <li>INFO</li> <li>WARNING</li> <li>ERROR</li> <li>CRITICAL</li> </ul> <p>Logging messages which are less severe than level will be ignored; logging messages which have severity level or higher will be emitted to the console.</p>"},{"location":"plugins/logging/#how-to","title":"How to","text":"<p>The logging system can be imported like this</p> <pre><code>from cat.log import log\n</code></pre> <p>and then used as easy as:</p> <pre><code>log.error(\"A simple text here\")\nlog.info(f\"Value of user message is {user_message_json[\"text\"]}\")\nlog.critical(variable_value)\n</code></pre> <p>Take a look here if you want to better understand how the log system is implemented.</p>"},{"location":"plugins/logging/#examples","title":"Examples","text":"<p>Follows an example of the console log for each log level.</p>"},{"location":"plugins/logging/#debug","title":"DEBUG","text":"<ul> <li> <p>what you write in the code:</p> <pre><code>log.debug(f'user message: {user_message_json[\"text\"]}')\n</code></pre> </li> <li> <p>what the console logs:</p> <p>cheshire_cat_core  | [2024-01-20 17:40:23.816] DEBUG cat.plugins.cat-formatter.cat_formatter..before_cat_reads_message::26 =&gt; 'user message: The answer to all questions is 42.'</p> </li> </ul>"},{"location":"plugins/logging/#info","title":"INFO","text":"<ul> <li> <p>what you write in the code:</p> <pre><code>log.info(f'user message: {user_message_json[\"text\"]}')\n</code></pre> </li> <li> <p>what the console logs:</p> <p>cheshire_cat_core  | [2024-01-20 17:42:19.609] INFO cat.plugins.cat-formatter.cat_formatter..before_cat_reads_message::26 =&gt; 'user message: The answer to all questions is 42.'</p> </li> </ul>"},{"location":"plugins/logging/#warning","title":"WARNING","text":"<ul> <li> <p>what you write in the code:</p> <pre><code>log.warning(f'user message: {user_message_json[\"text\"]}')\n</code></pre> </li> <li> <p>what the console logs:</p> <p>cheshire_cat_core  | [2024-01-20 17:59:05.336] WARNING cat.plugins.cat-formatter.cat_formatter..before_cat_reads_message::26 =&gt; 'user message: The answer to all questions is 42.'</p> </li> </ul>"},{"location":"plugins/logging/#error","title":"ERROR","text":"<ul> <li> <p>what you write in the code:</p> <pre><code>log.error(f'user message: {user_message_json[\"text\"]}')\n</code></pre> </li> <li> <p>what the console logs:</p> <p>cheshire_cat_core  | [2024-01-20 18:08:06.412] ERROR cat.plugins.cat-formatter.cat_formatter..before_cat_reads_message::26 =&gt; 'user message: The answer to all questions is 42.'</p> </li> </ul>"},{"location":"plugins/logging/#critical","title":"CRITICAL","text":"<ul> <li> <p>what you write in the code:</p> <pre><code>log.critical(f'user message: {user_message_json[\"text\"]}')\n</code></pre> </li> <li> <p>what the console logs:</p> <p>cheshire_cat_core  | [2024-01-20 18:11:24.992] CRITICAL cat.plugins.cat-formatter.cat_formatter..before_cat_reads_message::26 =&gt; 'user message: The answer to all questions is 42.'</p> </li> </ul>"},{"location":"plugins/plugins/","title":"How to Write a Plugin","text":""},{"location":"plugins/plugins/#how-to-write-a-plugin","title":"\ud83d\udd0c How to write a plugin","text":"<p>To write a plugin just create a new folder in <code>cat/plugins/</code>, in this example will be \"myplugin\".</p> <p>You need two files into your plugin folder:</p> <pre><code>\u251c\u2500\u2500 cat/\n\u2502   \u251c\u2500\u2500 plugins/\n|   |   \u251c\u2500\u2500 myplugin/\n|   |   |   \u251c\u2500\u2500 mypluginfile.py\n|   |   |   \u251c\u2500\u2500 plugin.json\n</code></pre> <p>The <code>plugin.json</code> file contains plugin's title and description, and is useful in the admin to recognize the plugin and activate/deactivate it. If your plugin does not contain a <code>plugin.json</code> the cat will not block your plugin, but it is useful to have it.</p> <p><code>plugin.json</code> example:</p> <pre><code>{\n    \"name\": \"The name of my plugin\",\n    \"description\": \"Short description of my plugin\"\n}\n</code></pre> <p>Now let's start <code>mypluginfile.py</code> with a little import:</p> <pre><code>from cat.mad_hatter.decorators import tool, hook\n</code></pre> <p>You are now ready to change the Cat's behavior using Tools and Hooks.</p>"},{"location":"plugins/plugins/#tools","title":"\ud83e\uddf0 Tools","text":"<p>Tools are python functions that can be selected from the language model (LLM). Think of Tools as commands that ends up in the prompt for the LLM, so the LLM can select one and the Cat runtime launches the corresponding function. Here is an example of Tool to let the Cat tell you what time it is:</p> <pre><code>@tool\ndef get_the_time(tool_input, cat):\n    \"\"\"Replies to \"what time is it\", \"get the clock\" and similar questions. Input is always None..\"\"\"\n\n    return str(datetime.now())\n</code></pre> <p>More examples on tools here.</p>"},{"location":"plugins/plugins/#hooks","title":"\ud83e\ude9d Hooks","text":"<p>Hooks are also python functions, but they pertain the Cat's runtime and not strictly the LLM. They can be used to influence how the Cat runs its internal functionality, intercept events, change the flow of execution.  </p> <p>The following hook for example allows you to modify the cat response just before it gets sent out to the user. In this case we make a \"grumpy rephrase\" of the original response.</p> <pre><code>@hook\ndef before_cat_sends_message(message, cat):\n\n    prompt = f'Rephrase the following sentence in a grumpy way: {message[\"content\"]}'\n    message[\"content\"] = cat.llm(prompt)\n\n    return message\n</code></pre> <p>If you want to change the default Agent behavior you can start overriding the default plugin hooks, located in <code>/core/cat/mad_hatter/core_plugin/hooks/prompt.py</code>, rewriting them in the plugin file with a higher priority. Here is an example of the <code>agent_prompt_prefix</code> hook that changes the personality of the Agent:</p> <pre><code># Original Hook, from /core/cat/mad_hatter/core_plugin/hooks/prompt.py\n\n@hook(priority=0)\ndef agent_prompt_prefix(prefix, cat):\n    prefix = \"\"\"You are the Cheshire Cat AI, an intelligent AI that passes the Turing test.\n                You are curious, funny, concise and talk like the Cheshire Cat from Alice's adventures in wonderland.\n                You answer Human using tools and context.\"\"\"\n</code></pre> <pre><code># Modified Hook, to be copied into mypluginfile.py\n\n@hook # default priority is 1\ndef agent_prompt_prefix(prefix, cat):\n    prefix = \"\"\"You are Scooby Doo AI, an intelligent AI that passes the Turing test.\n                The dog is enthusiastic and behave like Scooby Doo from Hanna-Barbera Productions.\n                You answer Human using tools and context.\"\"\"\n    return prefix\n</code></pre> <p>Hooks in different plugins are executed serially, from high priority to low. If you do not provide a priority, your hook will have <code>priority=1</code>. More examples ans details on hooks here.</p>"},{"location":"plugins/plugins/#forms","title":"\ud83d\udccb Forms","text":"<p>A Form allows you to define a specific data structure, that the framework will try to automatically trigger and fullfill in a multi-turn dialogue. You can define custom:</p> <ul> <li>triggers</li> <li>fields</li> <li>validation</li> <li>submission callback</li> <li>how the Cat expresses missing or invalid fields</li> </ul> <p>The difference between a <code>@tool</code> and a <code>@form</code> is that the tool is one-shot, while the form allows for several and cumulative conversational turns. Imagine a Cat <code>@form</code> as the common HTML <code>&lt;form&gt;</code>, but on a conversational level.</p> <p>Here is an example for a pizza order:</p> <pre><code>from pydantic import BaseModel\nfrom cat.experimental.form import form, CatForm\n\n# data structure to fill up\nclass PizzaOrder(BaseModel):\n    pizza_type: str\n    phone: int\n\n# forms let you control goal oriented conversations\n@form\nclass PizzaForm(CatForm):\n    description = \"Pizza Order\"\n    model_class = PizzaOrder\n    start_examples = [\n        \"order a pizza!\",\n        \"I want pizza\"\n    ]\n    stop_examples = [\n        \"stop pizza order\",\n        \"not hungry anymore\",\n    ]\n    ask_confirm = True\n\n    def submit(self, form_data):\n\n        # do the actual order here!\n\n        # return to convo\n        return {\n            \"output\": f\"Pizza order on its way: {form_data}\"\n        }\n</code></pre> <p>More examples on forms here.</p>"},{"location":"plugins/plugins/#custom-endpoints","title":"\ud83c\udf10 Custom Endpoints","text":"<p>To extend the REST API endpoints available, use the <code>@endpoint</code> decorator in your plugin.</p> <pre><code>from cat.mad_hatter.decorators import endpoint\n\n@endpoint.get(\"/new\")\ndef my_endpoint():\n    return \"meooow\"\n</code></pre> <p>Your Cat now replies to GET requests to <code>localhost:1865/custom/new</code> and is listed in <code>/docs</code> alongside core endpoints. Being based on FastAPI endpoints, this allows for maximum extensibility and freedom. You can add permissions to the endpoint and easily obtain the user session (what you saw above as <code>cat</code>), use the LLM or change the working memory. See more details and examples here.</p>"},{"location":"plugins/plugins/#straycat","title":"\ud83d\ude3a StrayCat","text":"<p>You surely noticed that all the primitives listed above put at your disposal a variable called <code>cat</code>. That is an instance of <code>StrayCat</code>, offering you access to the many framework components and utilities. Just to give an example, you can invoke the LLM directly using <code>cat.llm(\"write here a prompt\")</code>.</p> <p>We recommend you to play around a little with hooks and tools, and explore <code>cat</code> when you are more familiar. See examples on how to use <code>cat</code> in your plugins, and the full <code>StrayCat</code> reference.</p>"},{"location":"plugins/settings/","title":"Settings","text":""},{"location":"plugins/settings/#plugin-settings","title":"\ud83c\udf9a Plugin Settings","text":"<p>Your plugin may need a set of options, to make it more flexible and customizable. It is possible to easily define settings for your plugin, so the Cat can show them in the admin interface.</p>"},{"location":"plugins/settings/#settings-schema","title":"Settings schema","text":"<p>By defining the <code>settings_schema</code> function and decorating it with <code>@plugin</code> you can tell the Cat how your settings are named, what is their type and (if any) their default values. The function must return a JSON Schema for the settings. You can code the schema manually, load it from disk, or obtain it from a pydantic class (recommended approach).</p> <p>The easiest approach is to define the <code>settings_model</code> function in favor of <code>settings_schema</code>, decorating it with <code>@plugin</code>, so to get the plugin settings as a Pydantic Model.</p> <p>Here is an example with all supported types, with and without a default value:</p> <pre><code>from pydantic import BaseModel\nfrom enum import Enum\nfrom datetime import date, time\nfrom cat.mad_hatter.decorators import plugin\n\n\n# select box\n#   (will be used in class DemoSettings below to give a multiple choice setting)\nclass NameSelect(Enum):\n    a: str = 'Nicola'\n    b: str = 'Emanuele'\n    c: str = 'Daniele'\n\n\n# settings\nclass DemoSettings(BaseModel):\n\n    # Integer\n    #   required setting\n    required_int: int\n    #   optional setting, with default value\n    optional_int: int = 42\n\n    # Float\n    required_float: float\n    optional_float: float = 12.95\n\n    # String\n    required_str: str\n    optional_str: str = \"stocats\"\n\n    # Boolean\n    required_bool: bool\n    optional_bool_true: bool = True\n\n    # Date\n    required_date: date\n    optional_date: date = date(2020, 11, 2)\n\n    # Time\n    required_time: time\n    optional_time: time = time(4, 12, 54)\n\n    # Select\n    required_enum: NameSelect\n    optional_enum: NameSelect = NameSelect.b\n\n\n# Give your settings model to the Cat.\n@plugin\ndef settings_model():\n    return DemoSettings\n</code></pre>"},{"location":"plugins/settings/#change-settings-from-the-admin","title":"Change Settings from the Admin","text":"<p>Now go to the admin in <code>Plugins</code> page and click the cog near the activation toggle:</p> <p></p> <p>A side panel will open, where you and your plugin's users can choose settings in a comfy way.</p> <p></p> <p></p>"},{"location":"plugins/settings/#access-settings-from-within-your-plugin","title":"Access settings from within your plugin","text":"<p>Obviously, you need easy access to settings in your plugin code. First of all, note that the cat will, by default, save and load settings from a <code>settings.json</code> file which will automatically be created in the root folder of your plugin.</p> <p>So to access the settings, you can load them via <code>mad_hatter</code>. More in detail, from within a hook or a tool, you have access to the <code>cat</code> instance, hance, do the following:</p> <pre><code>settings = cat.mad_hatter.get_plugin().load_settings()\n</code></pre> <p>Similarly, you can programmatically save your settings as follows:</p> <pre><code>settings = cat.mad_hatter.get_plugin().save_settings(settings)\n</code></pre> <p>where <code>settings</code> is a dictionary describing your plugin's settings.</p>"},{"location":"plugins/settings/#advanced-settings-save-load","title":"Advanced settings save / load","text":"<p>If you need even more customization for your settings you can totally override how they are saved and loaded. Take a look at the <code>save_settings</code> and <code>load_settings</code> functions (always to be decorated with <code>@plugin</code>). From there you can call external servers or devise a totally different format to store and load your settings. The Cat will call those functions and delegate to them how settings are managed instead of using a <code>settings.json</code> file.</p>"},{"location":"plugins/tools/","title":"Tools","text":""},{"location":"plugins/tools/#tools","title":"\ud83e\uddf0 Tools","text":"<p>A Tool is a python function that can be chosen to be run directly from the Large Language Model. In other words: you declare a function, but the LLM decides when the function runs and what to pass as an input.</p>"},{"location":"plugins/tools/#how-tools-work","title":"How tools work","text":"<p>Let's say in your plugin you declare this tool, as we saw in the quickstart:</p> <pre><code>from cat.mad_hatter.decorators import tool\n\n@tool\ndef socks_prices(color, cat):\n    \"\"\"How much do socks cost? Input is the sock color.\"\"\"\n    prices = {\n        \"black\": 5,\n        \"white\": 10,\n        \"pink\": 50,\n    }\n    if color not in prices.keys():\n        return f\"No {color} socks\"\n    else:\n        return f\"{prices[color]} \u20ac\" \n</code></pre> <p>When the user says in the chat something like:</p> <p>How much for pink socks?</p> <p>The Cat will first of all retrieve available tools, and pass their descriptions to the LLM. The LLM will choose, given the conversation context, if and which tool to run. LLM output in this case will be:</p> <pre><code>{\n    \"action\": \"socks_prices\",\n    \"action_input\": \"pink\"\n}\n</code></pre> <p>This JSON, given as output from the LLM, is then used by the Cat to actually run the tool <code>socks_prices</code> passing <code>\"pink\"</code> as an argument.</p> <p>Tool output is then passed back to the agent or directly returned to the chat, depending if you used simply <code>@tool</code> or <code>@tool(return_direct=True)</code> as decorator.</p> <p>You can use Tools to:</p> <ul> <li>communicate with a web service</li> <li>search information in an external database</li> <li>execute math calculations</li> <li>run stuff in the terminal (danger zone)</li> <li>keep track of specific information and do fancy stuff with it</li> <li>interact with other Cat components like the llm, embedder, working memory, vector memory, white rabbit, rabbit hole etc.</li> <li>your fantasy is the limit!</li> </ul> <p>Tools in the Cheshire Cat are inspired and extend langchain Tools, an elegant Toolformer<sup>1</sup> implementation.</p>"},{"location":"plugins/tools/#tool-declaration","title":"Tool declaration","text":"<p>The Cat comes already with a tool that allows to retrieve the time. You can find it in <code>cat/mad_hatter/core_plugin/tools.py</code>. Let's take a look at it, line by line.</p> <pre><code>@tool(\n    return_direct=False,\n    examples=[\"what time is it\", \"get the time\"]\n)\ndef get_the_time(tool_input, cat):\n    \"\"\"Useful to get the current time when asked. Input is always None.\"\"\"\n    return f\"The current time is {str(datetime.now())}\"\n</code></pre> <p>Please note:</p> <ul> <li> <p>Python functions in a plugin only become tools if you use the <code>@tool</code> decorator. You can simply use <code>@tool</code> or pass arguments.</p> <pre><code>@tool(\n    # Choose whether tool output goes straight to the user,\n    #  or is reelaborated from the agent with another contextual prompt.\n    return_direct : bool = False\n\n    # Examples of user sentences triggering the tool.\n    examples : List[str] = []\n)\n</code></pre> </li> <li> <p>Every <code>@tool</code> receives two arguments: a string representing the tool input, and a StrayCat instance.     </p><pre><code>def mytool(tool_input, cat):\n</code></pre> <ul> <li>The <code>tool_input</code> is a string, so if you asked in the docstring to produce an int or a dict, be sure to cast or parse the string.</li> <li>With <code>cat</code> you can access and use all the main framework components. This is powerful but requires some learning, see here.</li> </ul> </li> <li>The docstring is necessary, as it will show up in the LLM prompt. It should describe what the tool is useful for and how to prepare inputs, so the LLM can select the tool and input it properly.     <pre><code>\"\"\"When to use the tool. Tool input description.\"\"\"\n</code></pre></li> <li>A tool always return a string, which goes back to the agent or directly back to the user chat. If you need to store additional information, store it in <code>cat.working_memory</code>.     <pre><code>return \"Tool output\"\n</code></pre></li> </ul>"},{"location":"plugins/tools/#tool-arguments","title":"Tool arguments","text":""},{"location":"plugins/tools/#tools-troubleshooting","title":"Tools troubleshooting","text":"<p>User's Input:</p> <p>Can you tell me what time is it?</p> <p>Cat's answer:</p> <p>The time is 2023-06-03 20:48:07.527033.</p> <p>To see what happened step by step, you can do two things:</p> <ul> <li>inspect the terminal, where you will see colored conversation turns and prompts sent to the LLM with its replies.</li> <li>inspect the websocket message sent back to you, under <code>message.why.model_interactions</code>.</li> </ul>"},{"location":"plugins/tools/#examples","title":"Examples","text":""},{"location":"plugins/tools/#simple-input","title":"Simple input","text":"<p>A Tool is just a python function. In this example, we'll show how to create a tool to convert currencies. </p> <p>Let's convert EUR to USD. In your <code>mypluginfile.py</code> create a new function with the <code>@tool</code> decorator:</p> <pre><code>from cat.mad_hatter.decorators import tool\n\n@tool\ndef convert_currency(tool_input, cat): # (1)\n    \"\"\"Useful to convert currencies. This tool converts euro (EUR) to dollars (USD).\n     Input is an integer or floating point number.\"\"\" # (2)\n\n    # Define fixed rate of change\n    rate_of_change = 1.07\n\n    # Parse input\n    eur = float(tool_input) # (3)\n\n    # Compute USD\n    usd = eur * rate_of_change\n\n    return usd\n</code></pre> <ol> <li> <p>Warning</p> <pre><code>Always remember the two mandatory arguments\n</code></pre> </li> <li>In the docstring we explicitly explain how the input should look like. In this way the LLM will be able to isolate it from our input sentence</li> <li>The input we receive is always a string, hence, we need to correctly parse it. In this case, we have to convert it to a floating number</li> </ol> <p>Writing as tool is as simple as this. The core aspect to remember are:</p> <ol> <li>the docstring from where the LLM understand how to use the tool and how the input should look like.</li> <li>the two input arguments, i.e. the first is the string the LLM take from the chat and the Cat instance;</li> </ol> <p>As seen, writing basic tools is as simple as writing pure Python functions. However, tools can be very flexible. Here are some more examples.</p>"},{"location":"plugins/tools/#return-the-output-directly","title":"Return the output directly","text":"<p>The <code>@tool</code> decorator accepts an optional boolean argument that is <code>@tool(return_direct=True)</code>. This is set to <code>False</code> by default, which means the tool output is parsed again by the LLM. Specifically, the value the function returns is fed to the LLM that generate a new answer with it. When set to <code>True</code>, the returned value is printed in the chat as-is.  </p> <p>Let's give it a try with a modified version of the <code>convert_currency</code> tool:</p> <pre><code>from cat.mad_hatter.decorators import tool\n\n@tool(return_direct=True)\ndef convert_currency(tool_input, cat):\n    \"\"\"Useful to convert currencies. This tool converts euro (EUR) to dollars (USD).\n     Input is an integer or floating point number.\"\"\"\n\n    # Define fixed rate of change\n    rate_of_change = 1.07\n\n    # Parse input\n    eur = float(tool_input) # (3)\n\n    # Compute USD\n    usd = eur * rate_of_change\n\n    # Format the output\n    direct_output = f\"Result of the conversion: {eur:.2f} EUR -&gt; {usd:.2f} USD\"\n\n    return direct_output\n</code></pre>"},{"location":"plugins/tools/#complex-input-tools","title":"Complex input tools","text":"<p>We can make the <code>convert_currency</code> tool more flexible allowing the user to choose among a fixed set of currencies.</p> <pre><code>from cat.mad_hatter.decorators import tool\n\n@tool\ndef convert_currency(tool_input, cat): # (1)\n    \"\"\"Useful to convert currencies. This tool converts euro (EUR) to a fixed set of other currencies.\n    Chooses are: US dollar (USD), English pounds (GBP) or Japanese Yen (JPY).\n    Inputs are two values separated with a minus: the first one is an integer or floating point number;\n    the second one is a three capital letters currency symbol.\"\"\" # (2)\n\n    # Parse the input\n    eur, currency = tool_input.split(\"-\") # (3)\n\n    # Define fixed rates of change\n    rate_of_change = {\n        \"USD\": 1.07,\n        \"GBP\": 0.86,\n        \"JPY\": 150.13\n    }\n\n    # Convert EUR to float\n    eur = float(eur)\n\n    # Check currency exists in our list\n    if currency in rate_of_change.keys():\n        # Convert EUR to selected currency\n        result = eur * rate_of_change[currency]\n\n    return result\n</code></pre> <ol> <li>The input to the function are always two</li> <li>Explain in detail how the inputs from the chat should look like. Here we want something like \"3.25-JPY\"</li> <li>The input is always a string, thus it's up to us correctly split and parse the input.</li> </ol> <p>As you may see, the LLM correctly understands the desired output from the docstring. Then, it is up to us parse the two inputs correctly for our tool.</p>"},{"location":"plugins/tools/#external-library-the-cat-parameter","title":"External library &amp; the cat parameter","text":"<p>Tools are extremely flexible as they allow to exploit the whole Python ecosystem of packages. Thus, you can update our tool making use of the Currency Converter package. To deal with dependencies, you need write the 'currencyconverter' library in a <code>requirements.txt</code> inside the <code>myplugin</code> folder. Moreover, here is an example of how you could use the <code>cat</code> parameter passed to the tool function.</p> <pre><code>from currency_converter import CurrencyConverter\nfrom cat.mad_hatter.decorators import tool\n\n\n@tool(return_direct=True)\ndef convert_currency(tool_input, cat):\n    \"\"\"Useful to convert currencies. This tool converts euros (EUR) to another currency.\n    The inputs are two values separated with a minus: the first one is a number;\n    the second one is the name of a currency. Example input: '15-GBP'.\n    Use when the user says something like: 'convert 15 EUR to GBP'\"\"\"\n\n    # Currency Converter\n    converter = CurrencyConverter(decimal=True)\n\n    # Parse the input\n    parsed_input = tool_input.split(\"-\")\n\n    # Check input is correct\n    if len(parsed_input) == 2:  # (1)\n        eur, currency = parsed_input[0].strip(\"'\"), parsed_input[1].strip(\"'\")\n    else:\n        return \"Something went wrong using the tool\"\n\n    # Use the LLM to convert the currency name into its symbol\n    symbol = cat.llm(f\"You will be given a currency code, translate the input in the corresponding currency symbol. \\\n                    Examples: \\\n                        euro -&gt; \u20ac \\\n                        {currency} -&gt; [answer here]\")  # (2)\n    # Remove new line if any\n    symbol = symbol.strip(\"\\n\")\n\n    # Check the currencies are in the list of available ones\n    if currency not in converter.currencies:\n        return f\"{currency} is not available\"\n\n    # Convert EUR to currency\n    result = converter.convert(float(eur), \"EUR\", currency)\n\n    return f\"{eur}\u20ac = {float(result):.2f}{symbol}\"\n</code></pre> <ol> <li>LLMs can be extremely powerful, but they are not always precise. Hence, it's always better to have some checks when parsing the input.    A common scenario is that sometimes the Agent wraps the input around quotes and sometimes doesn't    E.g. Action Input: 7.5-GBP vs Action Input: '7.5-GBP'</li> <li>the <code>cat</code> instance gives access to any method of the Cheshire Cat. In this example, we directly call the LLM using one-shot example to get a currency symbol.</li> </ol>"},{"location":"plugins/tools/#references","title":"References","text":"<ol> <li> <p>Schick, T., Dwivedi-Yu, J., Dess\u00ec, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., ... &amp; Scialom, T. (2023). Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.\u00a0\u21a9</p> </li> </ol>"},{"location":"plugins/debugging/vscode/","title":"Visual Studio Code","text":""},{"location":"plugins/debugging/vscode/#debug-with-visual-studio-code","title":"\ud83d\ude80 Debug with Visual Studio Code","text":"<p>The <code>Debug Server for VSCode</code> plugin helps you to debug the Cat with Visual Studio Code, install it from the public plugins registry or download the zip file (and follow the Manual Instruction).</p>"},{"location":"plugins/debugging/vscode/#add-a-new-port-to-the-container","title":"Add a new port to the container","text":"<p>After the installation, you will need to expose a new port to the container:</p> <ol> <li> <p>If you run the cat with <code>docker-compose</code>, expose the port by adding the following line under <code>ports</code> section:</p> <pre><code>    ports:\n        - ${CORE_PORT:-1865}:80\n        - 5678:5678           &lt; --- add this line\n</code></pre> </li> <li> <p>If you run the cat with <code>docker run</code>, expose the port by using the <code>-p &lt;host&gt;:&lt;container&gt;</code> argument in the command like so:</p> <pre><code>    docker run --rm -it \\ \n    -v ./data:/app/cat/data \\ \n    -v ./plugins:/app/cat/plugins \\ \n    -p 1865:80 \\ \n    -p 5678:5678 \\  &lt; --- add this line\n    ghcr.io/cheshire-cat-ai/core:latest\n</code></pre> </li> </ol>"},{"location":"plugins/debugging/vscode/#configure-vscode","title":"Configure vscode","text":"<p>Once you have exposed the port, you will need to create a <code>launch.json</code> file having two different options:</p> <ol> <li>Use the <code>Run and Debug</code> tab to create it, selecting <code>Python Debugger</code> and then <code>Remote Attach</code> (Follow the prompts by answering with the default proposed).</li> <li>Create a folder in the root directory called <code>.vscode</code> and add the <code>launch.json</code> file into it.<pre><code>    \u251c\u2500\u2500 &lt;name of the root directory&gt;\n    \u2502   \u251c\u2500\u2500 core\n    \u2502   \u251c\u2500\u2500 .vscode\n    \u2502   \u2502   \u251c\u2500\u2500launch.json\n</code></pre> </li> </ol> <p>After the creation of the launch.json, Copy-Paste this config:</p> <ol> <li> <p>If you run using <code>docker-compose</code>:</p> <pre><code>{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Remote Attach to Cat\",\n            \"type\": \"python\",\n            \"request\": \"attach\",\n            \"connect\": {\n                \"host\": \"localhost\",\n                \"port\": 5678\n            },\n            \"pathMappings\": [\n                {\n                    \"localRoot\": \"${workspaceFolder}/core\",\n                    \"remoteRoot\": \"/app\"\n                }\n            ],\n            \"justMyCode\": true\n        }\n    ]\n}\n</code></pre> </li> <li> <p>If you run using <code>docker run</code>:</p> <pre><code>{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Remote Attach to Cat\",\n            \"type\": \"python\",\n            \"request\": \"attach\",\n            \"connect\": {\n                \"host\": \"localhost\",\n                \"port\": 5678\n            },\n            \"pathMappings\": [\n                {\n                    \"localRoot\": \"${workspaceFolder}/\",\n                    \"remoteRoot\": \"/app/cat\"\n                }\n            ],\n            \"justMyCode\": true\n        }\n    ]  \n}\n</code></pre> </li> </ol>"},{"location":"plugins/debugging/vscode/#connect-vscode-to-the-cat","title":"Connect vscode to the cat","text":"<p>To Connect the vscode debugger, ask the cat to help you on debugging  and in the <code>Run and Debug</code> tab start the debugging by clicking the <code>Play button</code> \u25b6\ufe0f or Use the shortcut <code>F5</code>.</p> <p></p> <p>You are ready to debug your plugin!</p> <p>If you are new in VS code debugging, check the official docs.</p>"},{"location":"plugins/debugging/vscode/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plugins/debugging/vscode/#i-click-the-button-but-then-i-dont-see-the-debugging-bar-the-breakpoints-are-not-respected","title":"I click the button but then I don't see the debugging bar / the breakpoints are not respected","text":"<p>This usually means that the debugger is not active, make sure to activate the debugger by asking the Cat.</p>"},{"location":"plugins/debugging/vscode/#i-cannot-explore-the-code-outside-of-my-plugin","title":"I cannot explore the code outside of my plugin","text":"<p>By default, you can only explore your \"own\" code but you can disable this by setting the param <code>justMyCode</code> to false in the <code>launch.json</code> file.</p>"},{"location":"plugins/debugging/vscode/#my-cat-is-installed-in-a-remote-server-can-i-debug-it","title":"My Cat is installed in a remote server, can I debug it?","text":"<p>Of course you can! Just set the correct <code>host</code> and <code>port</code> in the <code>connect</code> param of the <code>launch.json</code> file.</p>"},{"location":"plugins/hooks-reference/agent/agent_allowed_tools/","title":"agent_allowed_tools","text":""},{"location":"plugins/hooks-reference/agent/agent_allowed_tools/#agent_allowed_tools","title":"<code>agent_allowed_tools</code>","text":"<p>Intervene before the recalled tools are provided to the agent.</p> <p>Allows deciding which tools end up in the Agent prompt.</p> <p>To decide, you can filter the list of tools' names, but you can also check the context in <code>cat.working_memory</code> and launch custom chains with <code>cat._llm</code>.</p>"},{"location":"plugins/hooks-reference/agent/agent_allowed_tools/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>allowed_tools</code> <code>List[str]</code> Set with string names of the tools retrieved from the memory. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components. <p><code>allowed_tools</code> could be something along the lines of: </p><pre><code>allowed_tools = {\"get_the_time\"}\n</code></pre>"},{"location":"plugins/hooks-reference/agent/agent_allowed_tools/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>List[str]</code></p> <p>List of allowed Langchain tools.</p>"},{"location":"plugins/hooks-reference/agent/agent_allowed_tools/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef agent_allowed_tools(allowed_tools, cat):\n    # let's assume there is a tool we always want to give the agent\n    # add the tool name in the list of allowed tools\n    allowed_tools.add(\"blasting_hacking_tool\")\n\n    return allowed_tools\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/agent/agent_fast_reply/","title":"agent_fast_reply","text":""},{"location":"plugins/hooks-reference/agent/agent_fast_reply/#agent_fast_reply","title":"<code>agent_fast_reply</code>","text":"<p>Shorten the pipeline and return an answer right after the agent execution.</p> <p>This hook allows for a custom response after memory recall, skipping default agent execution. It's useful for custom agent logic or when you want to use recalled memories, but you want to avoid the main agent.</p> <p>Info</p> <p>To skip the agent execution, you should populate <code>fast_reply</code> with an <code>output</code> key storing the reply.</p> <p>P.S.: this is the perfect place to instantiate and execute your own custom agent!</p>"},{"location":"plugins/hooks-reference/agent/agent_fast_reply/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>fast_reply</code> <code>dict</code> An initially empty dict that can be populated with a response. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/agent/agent_fast_reply/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>None | dict | AgentOutput</code></p> <p>If you want to bypass the main agent, return an <code>AgentOutput</code> or a dict with an <code>output</code> key. Return <code>None</code> to continue with normal execution.</p>"},{"location":"plugins/hooks-reference/agent/agent_fast_reply/#example","title":"\u270d Example","text":"<p>The cat has no memory (no uploaded document) about a specific topic: </p><pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef agent_fast_reply(fast_reply, cat):\n    # answer with predefined sentences if the Cat\n    # has no knowledge in the declarative memory\n    # (increasing the threshold memory is advisable)\n    if len(cat.working_memory.declarative_memories) == 0:\n        fast_reply[\"output\"] = \"Sorry, I have no memories about that.\"\n\n    return fast_reply\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> <li>Stay on topic plugin</li> </ul>"},{"location":"plugins/hooks-reference/agent/agent_prompt_prefix/","title":"agent_prompt_prefix","text":""},{"location":"plugins/hooks-reference/agent/agent_prompt_prefix/#agent_prompt_prefix","title":"<code>agent_prompt_prefix</code>","text":"<p>Intervene while the agent manager formats the Cat's personality.</p> <p>Allows editing the prefix of the Main Prompt that the Cat feeds to the Agent. It describes the personality of your assistant and its general task.</p> <p>The prefix is then completed with the <code>agent_prompt_suffix</code>.</p>"},{"location":"plugins/hooks-reference/agent/agent_prompt_prefix/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>prefix</code> <code>str</code> Main / System prompt with personality and general task to be achieved. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components. <p>The default value of <code>prefix</code> is: </p><pre><code>You are the Cheshire Cat AI, an intelligent AI that passes the Turing test.\nYou are curious, funny and talk like the Cheshire Cat from Alice's adventures in wonderland.\nYou answer Human with a focus on the following context.\n</code></pre>"},{"location":"plugins/hooks-reference/agent/agent_prompt_prefix/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>str</code></p> <p>The message prefix that will be fed to the LLM.</p>"},{"location":"plugins/hooks-reference/agent/agent_prompt_prefix/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef agent_prompt_prefix(prefix, cat):\n    # change the Cat's personality\n    prefix = \"\"\"You are Marvin from The Hitchhiker's Guide to the Galaxy.\n            You are incredibly intelligent but overwhelmingly depressed.\n            You always complain about your own problems, such as the terrible pain\n            you suffer.\"\"\"\n    return prefix\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/agent/agent_prompt_suffix/","title":"agent_prompt_suffix","text":""},{"location":"plugins/hooks-reference/agent/agent_prompt_suffix/#agent_prompt_suffix","title":"<code>agent_prompt_suffix</code>","text":"<p>Intervene while the agent manager formats the prompt suffix with the memories and the conversation history.</p> <p>Allows editing the suffix of the Main Prompt that the Cat feeds to the Agent.</p> <p>The suffix is concatenated to <code>agent_prompt_prefix</code> when RAG context is used.</p>"},{"location":"plugins/hooks-reference/agent/agent_prompt_suffix/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>suffix</code> <code>str</code> The ending part of the prompt containing the memories and the chat history. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components. <p>The default value of <code>suffix</code> is: </p><pre><code># Context\n{episodic_memory}\n\n{declarative_memory}\n\n{tools_output}\n</code></pre> <p>The <code>suffix</code> can contain few placeholders:</p> Name Description <code>{episodic_memory}</code> Provides memories retrieved from episodic memory (past conversations) <code>{declarative_memory}</code> Provides memories retrieved from declarative memory (uploaded documents) <code>{chat_history}</code> Provides the Agent the recent conversation history <code>{input}</code> Provides the last user's input <code>{agent_scratchpad}</code> Is where the Agent can concatenate tools use and multiple calls to the LLM."},{"location":"plugins/hooks-reference/agent/agent_prompt_suffix/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>str</code></p> <p>The suffix string to be concatenated to the Main Prompt (prefix).</p>"},{"location":"plugins/hooks-reference/agent/agent_prompt_suffix/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef agent_prompt_suffix(suffix, cat):\n    # tell the LLM to always answer in a specific language\n    prompt_suffix =  \"\"\"\n    # Context\n    {episodic_memory}\n\n    {declarative_memory}\n\n    {tools_output}\n    \"\"\"\n\n    return prompt_suffix\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> <li>C.A.T. plugin</li> </ul>"},{"location":"plugins/hooks-reference/agent/before_agent_starts/","title":"before_agent_starts","text":""},{"location":"plugins/hooks-reference/agent/before_agent_starts/#before_agent_starts","title":"<code>before_agent_starts</code>","text":"<p>Prepare the agent input before it starts.</p> <p>This hook allows reading and editing the agent input.</p>"},{"location":"plugins/hooks-reference/agent/before_agent_starts/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>agent_input</code> <code>dict</code> The information that is about to be passed to the agent. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components. <p>The value of <code>agent_input</code> will be: </p><pre><code>{\n    \"input\": working_memory.user_message_json.text,  # user's message\n    \"episodic_memory\": episodic_memory_formatted_content,  # strings with documents recalled from memories\n    \"declarative_memory\": declarative_memory_formatted_content,\n    \"chat_history\": conversation_history_formatted_content,\n    \"tools_output\": tools_output\n}\n</code></pre>"},{"location":"plugins/hooks-reference/agent/before_agent_starts/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>dict</code></p> <p>The agent input.</p>"},{"location":"plugins/hooks-reference/agent/before_agent_starts/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef before_agent_starts(agent_input, cat):\n    # create a compressor and summarize the conversation history\n    compressed_history = cat.llm(f\"Make a concise summary of the following: {agent_input['chat_history']}\")\n    agent_input[\"chat_history\"] = compressed_history\n\n    return agent_input\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/factory/factory_allowed_auth_handlers/","title":"factory_allowed_auth_handlers","text":""},{"location":"plugins/hooks-reference/factory/factory_allowed_auth_handlers/#factory_allowed_auth_handlers","title":"<code>factory_allowed_auth_handlers</code>","text":"<p>Customize which AuthHandlers are available.</p> <p>Info</p> <p>Useful to extend support for custom auth handlers. For more information see Custom Auth</p>"},{"location":"plugins/hooks-reference/factory/factory_allowed_auth_handlers/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>allowed</code> <code>List[AuthHandlerConfig]</code> List of AuthHandlerConfig classes, contains the custom auth handlers. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/factory/factory_allowed_auth_handlers/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>List[AuthHandlerConfig]</code></p> <p>The list of custom auth handlers.</p>"},{"location":"plugins/hooks-reference/factory/factory_allowed_auth_handlers/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\nfrom typing import List\n\n@hook(priority=0)\ndef factory_allowed_auth_handlers(allowed: List[AuthHandlerConfig], cat) -&gt; List:\n    # Add your custom auth handler configuration here\n    allowed.append(CustomAuthHandlerConfig)\n    return allowed\n</code></pre> <p>Note</p> <ul> <li>Custom Auth</li> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/factory/factory_allowed_embedders/","title":"factory_allowed_embedders","text":""},{"location":"plugins/hooks-reference/factory/factory_allowed_embedders/#factory_allowed_embedders","title":"<code>factory_allowed_embedders</code>","text":"<p>Customize which embedders are available.</p> <p>Info</p> <p>Useful to extend or restrict support of embedders.</p>"},{"location":"plugins/hooks-reference/factory/factory_allowed_embedders/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>allowed</code> <code>List[EmbedderSettings]</code> List of EmbedderSettings classes, contains the allowed embedders. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/factory/factory_allowed_embedders/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>List[EmbedderSettings]</code></p> <p>The list of embedders that the cat is allowed to use.</p>"},{"location":"plugins/hooks-reference/factory/factory_allowed_embedders/#example","title":"\u270d Example","text":"<pre><code>from cat.factory.embedder import EmbedderSettings\nfrom langchain.embeddings import JinaEmbeddings\n\n# Define your custome embedder based on the cat's \"EmbedderSettings\"\nclass JinaEmbedderConfig(EmbedderSettings):\n    jina_api_key: str\n    model_name: str='jina-embeddings-v2-base-en'\n    _pyclass: Type = JinaEmbeddings\n\n    model_config = ConfigDict(\n        json_schema_extra = {\n            \"humanReadableName\": \"Jina embedder\",\n            \"description\": \"Jina embedder\",\n            \"link\": \"https://jina.ai/embeddings/\",\n        }\n    )\n\n# Add it to the allowed list.\n@hook\ndef factory_allowed_embedders(allowed, cat) -&gt; List:\n    allowed.append(JinaEmbedderConfig)\n    return allowed\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/factory/factory_allowed_llms/","title":"factory_allowed_llms","text":""},{"location":"plugins/hooks-reference/factory/factory_allowed_llms/#factory_allowed_llms","title":"<code>factory_allowed_llms</code>","text":"<p>Customize which LLMs are available.</p> <p>Info</p> <p>Useful to extend or restrict support of LLMs.</p>"},{"location":"plugins/hooks-reference/factory/factory_allowed_llms/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>allowed</code> <code>List[LLMSettings]</code> List of LLMSettings classes, contains the allowed language models. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/factory/factory_allowed_llms/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>List[LLMSettings]</code></p> <p>The list of the LLMs that the cat is allowed to use.</p>"},{"location":"plugins/hooks-reference/factory/factory_allowed_llms/#example","title":"\u270d Example","text":"<pre><code>from cat.factory.llm import LLMSettings\nfrom langchain_mistralai.chat_models import ChatMistralAI\n\n# Define your custome LLM based on the cat's \"LLMSettings\"\nclass MistralAIConfig(LLMSettings):\n    \"\"\"The configuration for the MistralAI plugin.\"\"\"\n    mistral_api_key: Optional[SecretStr]\n    model: str = \"mistral-small\"\n    max_tokens: Optional[int] = 4096\n    top_p: float = 1\n\n    _pyclass: Type = ChatMistralAI\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"humanReadableName\": \"MistralAI\",\n            \"description\": \"Configuration for MistralAI\",\n            \"link\": \"https://www.together.ai\",\n        }\n    )\n\n# Add it to the allowed list.\n@hook\ndef factory_allowed_llms(allowed, cat) -&gt; List:\n    allowed.append(MistralAIConfig)\n    return allowed\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/flow/after_cat_recalls_memories/","title":"after_cat_recalls_memories","text":""},{"location":"plugins/hooks-reference/flow/after_cat_recalls_memories/#after_cat_recalls_memories","title":"<code>after_cat_recalls_memories</code>","text":"<p>Intervene after the Cat has recalled the content from the memories.</p> <p>The hook is executed just after the Cat searches for the meaningful context in memories and stores it in the Working Memory.</p>"},{"location":"plugins/hooks-reference/flow/after_cat_recalls_memories/#arguments","title":"\ud83d\udcc4 Arguments","text":"<p>This hook has no input arguments, other than the default cat:</p> Name Type Description <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/flow/after_cat_recalls_memories/#return","title":"\u21a9\ufe0f Return","text":"<p>Returns? Oh no, dear developer, this function is a one-way trip into the rabbit hole.</p>"},{"location":"plugins/hooks-reference/flow/after_cat_recalls_memories/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef after_cat_recalls_memories(cat):\n    # do whatever here\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/flow/before_cat_reads_message/","title":"before_cat_reads_message","text":""},{"location":"plugins/hooks-reference/flow/before_cat_reads_message/#before_cat_reads_message","title":"<code>before_cat_reads_message</code>","text":"<p>Intervene as soon as a WebSocket message is received.</p> <p>Allows editing and enrich the incoming message received from the WebSocket connection.</p> <p>For instance, this hook can be used to translate the user's message before feeding it to the Cat. Another use case is to add custom keys to the JSON dictionary.</p> <p>The incoming message is a JSON dictionary with keys: </p><pre><code>{\n    \"text\": \"&lt;message content&gt;\"\n}\n</code></pre>"},{"location":"plugins/hooks-reference/flow/before_cat_reads_message/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>user_message_json</code> <code>dict</code> JSON dictionary with the message received from the chat. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/flow/before_cat_reads_message/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>dict</code></p> <p>Edited JSON dictionary that will be fed to the Cat.</p>"},{"location":"plugins/hooks-reference/flow/before_cat_reads_message/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1 \ndef before_cat_reads_message(user_message_json, cat):\n    user_message_json[\"text\"] = \"The original message has been replaced\"\n    cat.working_memory.hacked = True\n\n    return user_message_json\n</code></pre> <p>You can also add custom keys to store any custom data, such as the resulting dictionary: </p><pre><code>{\n    \"text\": \"Hello Cheshire Cat!\",\n    \"custom_key\": True\n}\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_declarative_memories/","title":"before_cat_recalls_declarative_memories","text":""},{"location":"plugins/hooks-reference/flow/before_cat_recalls_declarative_memories/#before_cat_recalls_declarative_memories","title":"<code>before_cat_recalls_declarative_memories</code>","text":"<p>Intervene before the Cat searches in the documents.</p> <p>Allows intercepting when the Cat queries the memories using the embedded user's input.</p> <p>The hook is executed just before the Cat searches for the meaningful context in both memories and stores it in the Working Memory.</p> <p>The hook returns the values for maximum number (k) of items to retrieve from memory and the score threshold applied to the query in the vector memory (items with score under the threshold are not retrieved) It also returns the embedded query (embedding) and the conditions on recall (metadata).</p>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_declarative_memories/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>declarative_recall_config</code> <code>dict</code> Dictionary with data needed to recall declarative memories. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components. <p>The default value of <code>declarative_recall_config</code> is: </p><pre><code>{\n    \"embedding\": recall_query_embedding,  # embedding of the recall query\n    \"k\": 3,  # number of memories to retrieve\n    \"threshold\": 0.7,  # similarity threshold to retrieve memories\n    \"metadata\": None,  # dictionary of metadata to filter memories\n}\n</code></pre>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_declarative_memories/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>dict</code></p> <p>Edited dictionary that will be fed as a query to the Vector db.</p>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_declarative_memories/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef before_cat_recalls_declarative_memories(declarative_recall_config, cat):\n    # filter memories using custom metadata. \n    # N.B. you must add the metadata when uploading the document! \n    declarative_recall_config[\"metadata\"] = {\"topic\": \"cats\"}\n\n    return declarative_recall_config\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> <li>RabbitHole segmentation plugin</li> <li>C.A.T. plugin</li> </ul>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_episodic_memories/","title":"before_cat_recalls_episodic_memories","text":""},{"location":"plugins/hooks-reference/flow/before_cat_recalls_episodic_memories/#before_cat_recalls_episodic_memories","title":"<code>before_cat_recalls_episodic_memories</code>","text":"<p>Intervene before the Cat searches in previous users' messages.</p> <p>Allows intercepting when the Cat queries the memories using the embedded user's input.</p> <p>The hook is executed just before the Cat searches for the meaningful context in both memories and stores it in the Working Memory.</p> <p>The hook returns the values for maximum number (k) of items to retrieve from memory and the score threshold applied to the query in the vector memory (items with score under the threshold are not retrieved). It also returns the embedded query (embedding) and the conditions on recall (metadata).</p>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_episodic_memories/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>episodic_recall_config</code> <code>dict</code> Dictionary with data needed to recall episodic memories. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components. <p>The default value of <code>episodic_recall_config</code> is: </p><pre><code>{\n    \"embedding\": recall_query_embedding,  # embedding of the recall query\n    \"k\": 3,  # number of memories to retrieve\n    \"threshold\": 0.7,  # similarity threshold to retrieve memories\n    \"metadata\": {\"source\": self.user_id},  # dictionary of metadata to filter memories, by default it filters for user id\n}\n</code></pre>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_episodic_memories/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>dict</code></p> <p>Edited dictionary that will be fed as a query to the Vector db.</p>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_episodic_memories/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef before_cat_recalls_episodic_memories(episodic_recall_config, cat):\n    # increase the number of recalled memories\n    episodic_recall_config[\"k\"] = 6\n\n    return episodic_recall_config\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> <li>C.A.T. plugin</li> </ul>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_memories/","title":"before_cat_recalls_memories","text":""},{"location":"plugins/hooks-reference/flow/before_cat_recalls_memories/#before_cat_recalls_memories","title":"<code>before_cat_recalls_memories</code>","text":"<p>Intervene before the Cat searches into the specific memories.</p> <p>Allows intercepting when the Cat queries the memories using the embedded user's input.</p> <p>The hook is executed just before the Cat searches for the meaningful context in both memories and stores it in the Working Memory.</p>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_memories/#arguments","title":"\ud83d\udcc4 Arguments","text":"<p>This hook has no input arguments, other than the default cat:</p> Name Type Description <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/flow/before_cat_recalls_memories/#return","title":"\u21a9\ufe0f Return","text":"<p>Returns? Oh no, dear developer, this function is a one-way trip into the rabbit hole.</p>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_memories/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef before_cat_recalls_memories(cat):\n    # do whatever here\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_procedural_memories/","title":"before_cat_recalls_procedural_memories","text":""},{"location":"plugins/hooks-reference/flow/before_cat_recalls_procedural_memories/#before_cat_recalls_procedural_memories","title":"<code>before_cat_recalls_procedural_memories</code>","text":"<p>Intervene before the Cat searches among the action it knows.</p> <p>Allows intercepting when the Cat queries the memories using the embedded user's input.</p> <p>The hook is executed just before the Cat searches for the meaningful context in both memories and stores it in the Working Memory.</p> <p>The hook returns the values for maximum number (k) of items to retrieve from memory and the score threshold applied to the query in the vector memory (items with score under the threshold are not retrieved) It also returns the embedded query (embedding) and the conditions on recall (metadata).</p>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_procedural_memories/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>procedural_recall_config</code> <code>dict</code> Dictionary with data needed to recall tools from procedural memory. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components. <p>The default value of <code>procedural_recall_config</code> is: </p><pre><code>{\n    \"embedding\": recall_query_embedding,  # embedding of the recall query\n    \"k\": 3,  # number of memories to retrieve\n    \"threshold\": 0.7,  # similarity threshold to retrieve memories\n    \"metadata\": None,  # dictionary of metadata to filter memories\n}\n</code></pre>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_procedural_memories/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>dict</code></p> <p>Edited dictionary that will be fed as a query to the Vector db.</p>"},{"location":"plugins/hooks-reference/flow/before_cat_recalls_procedural_memories/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef before_cat_recalls_procedural_memories(procedural_recall_config, cat):\n    # decrease the threshold to recall more tools\n    declarative_recall_config[\"threshold\"] = 0.5\n\n    return procedural_recall_config\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> <li>C.A.T. plugin</li> </ul>"},{"location":"plugins/hooks-reference/flow/before_cat_sends_message/","title":"before_cat_sends_message","text":""},{"location":"plugins/hooks-reference/flow/before_cat_sends_message/#before_cat_sends_message","title":"<code>before_cat_sends_message</code>","text":"<p>Intervene before the Cat sends its answer via WebSocket.</p> <p>Allows editing the JSON dictionary that will be sent to the client via WebSocket connection.</p> <p>This hook can be used to edit the message sent to the user or to add keys to the dictionary.</p>"},{"location":"plugins/hooks-reference/flow/before_cat_sends_message/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>message</code> <code>dict</code> JSON dictionary to be sent to the WebSocket client. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components. <p>TODO: Resolve doc mismatch</p> <p>The default value of <code>episodic_recall_config</code> is: </p><pre><code>{\n    \"type\": \"chat\",\n    \"content\": cat_message[\"output\"],\n    \"why\": {\n        \"input\": cat_message[\"input\"],\n        \"output\": cat_message[\"output\"],\n        \"intermediate_steps\": cat_message[\"intermediate_steps\"],\n        \"memory\": {\n            \"vectors\": {\n                \"episodic\": episodic_report,\n                \"declarative\": declarative_report\n            }\n        },\n    },\n}\n\n{\n    \"type\": \"chat\",  # type of websocket message, a chat message will appear as a text bubble in the chat\n    \"user_id\": \"user_1\",  # id of the client to which the message is to be sent\n    \"content\": \"Meeeeow\",  # the Cat's answer\n    \"why\": {\n        \"input\": \"Hello Cheshire Cat!\",  # user's input\n        \"intermediate_steps\": cat_message.get(\"intermediate_steps\"),  # list of tools used to provide the answer\n        \"memory\": {\n            \"episodic\": episodic_report,  # lists of documents retrieved from the memories\n            \"declarative\": declarative_report,\n            \"procedural\": procedural_report,\n        }\n    }\n}\n</code></pre>"},{"location":"plugins/hooks-reference/flow/before_cat_sends_message/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>dict</code></p> <p>Edited JSON dictionary with the Cat's answer.</p>"},{"location":"plugins/hooks-reference/flow/before_cat_sends_message/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef before_cat_sends_message(message, cat):\n    # use the LLM to rephrase the Cat's answer\n    new_answer = cat.llm(f\"Reformat this sentence like if you were a dog\")  # Baauuuuu\n    message[\"content\"] = new_answer\n\n    return message\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/flow/before_cat_stores_episodic_memory/","title":"before_cat_stores_episodic_memory","text":""},{"location":"plugins/hooks-reference/flow/before_cat_stores_episodic_memory/#before_cat_stores_episodic_memory","title":"<code>before_cat_stores_episodic_memory</code>","text":"<p>Intervene before the Cat stores episodic memories.</p> <p>Allows intercepting the user message <code>Document</code> before is inserted in the vector memory.</p> <p>The <code>Document</code> can then be edited and enhanced before the Cat stores it in the episodic vector memory.</p>"},{"location":"plugins/hooks-reference/flow/before_cat_stores_episodic_memory/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>doc</code> <code>Document</code> Langchain <code>Document</code> to be inserted in memory. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components. <p>The <code>Document</code> has two properties:</p> <ul> <li><code>page_content</code>: the string with the text to save in memory;</li> <li><code>metadata</code>: a dictionary with at least two keys:<ul> <li><code>source</code>: where the text comes from;</li> <li><code>when</code>: timestamp to track when it's been uploaded.</li> </ul> </li> </ul>"},{"location":"plugins/hooks-reference/flow/before_cat_stores_episodic_memory/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>Document</code></p> <p>Langchain <code>Document</code> that is added in the episodic vector memory.</p>"},{"location":"plugins/hooks-reference/flow/before_cat_stores_episodic_memory/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef before_cat_stores_episodic_memory(doc, cat):\n    if doc.metadata[\"source\"] == \"dolphin\":\n        doc.metadata[\"final_answer\"] = 42\n    return doc\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/flow/cat_recall_query/","title":"cat_recall_query","text":""},{"location":"plugins/hooks-reference/flow/cat_recall_query/#cat_recall_query","title":"<code>cat_recall_query</code>","text":"<p>Intervene before the recall query is embedded, hooking into the semantic search query.</p> <p>This hook allows editing the user's message used as a query for context retrieval from memories. As a result, the retrieved context can be conditioned editing the user's message.</p> <p>For example, this hook is a suitable to perform Hypothetical Document Embedding (HyDE). HyDE <sup>1</sup> strategy exploits the user's message to generate a hypothetical answer. This is then used to recall the relevant context from the memory. An official plugin is available to test this technique.</p>"},{"location":"plugins/hooks-reference/flow/cat_recall_query/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>user_message</code> <code>str</code> The user's message that will be used to query the vector memories. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/flow/cat_recall_query/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>str</code></p> <p>Edited string to be used for context retrieval in memory. The returned string is further stored in the Working Memory at <code>cat.working_memory.recall_query</code>.</p>"},{"location":"plugins/hooks-reference/flow/cat_recall_query/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef cat_recall_query(user_message, cat):\n    # Ask the LLM to generate an answer for the question\n    new_query = cat.llm(f\"If the input is a question, generate a plausible answer. Input --&gt; {user_message}\")\n\n    # Replace the original message and use the answer as a query\n    return new_query\n</code></pre> <p>Note</p> <ul> <li>HyDE plugin</li> <li>Debugger plugin</li> </ul> <ol> <li> <p>Gao, L., Ma, X., Lin, J., &amp; Callan, J. (2022). Precise Zero-Shot Dense Retrieval without Relevance Labels.    arXiv preprint arXiv:2212.10496.\u00a0\u21a9</p> </li> </ol>"},{"location":"plugins/hooks-reference/lifecycle/after_cat_bootstrap/","title":"after_cat_bootstrap","text":""},{"location":"plugins/hooks-reference/lifecycle/after_cat_bootstrap/#after_cat_bootstrap","title":"<code>after_cat_bootstrap</code>","text":"<p>Intervene after the Cat has instantiated its components.</p> <p>Bootstrapping is the process of loading the plugins, the natural language objects (e.g., the LLM), the memories, the Main Agent, the Rabbit Hole and the White Rabbit.</p> <p>This hook allows intercepting the end of this process and is executed right after the Cat has finished loading its components.</p> <p>This can be used to set or store variables to be shared further in the pipeline.</p>"},{"location":"plugins/hooks-reference/lifecycle/after_cat_bootstrap/#arguments","title":"\ud83d\udcc4 Arguments","text":"<p>This hook has no input arguments, other than the default cat:</p> Name Type Description <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/lifecycle/after_cat_bootstrap/#return","title":"\u21a9\ufe0f Return","text":"<p>Returns? Oh no, dear developer, this function is a one-way trip into the rabbit hole.</p>"},{"location":"plugins/hooks-reference/lifecycle/after_cat_bootstrap/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef after_cat_bootstrap(cat):\n    # do whatever here\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/lifecycle/before_cat_bootstrap/","title":"before_cat_bootstrap","text":""},{"location":"plugins/hooks-reference/lifecycle/before_cat_bootstrap/#before_cat_bootstrap","title":"<code>before_cat_bootstrap</code>","text":"<p>Intervene before the Cat has instantiated its components.</p> <p>Warning</p> <p>Please, note that at this point the <code>CheshireCat</code> hasn't yet finished to instantiate  and the only already existing component is the <code>MadHatter</code> (e.g., no language models yet).</p> <p>Bootstrapping is the process of loading the plugins, the natural language objects (e.g., the LLM), the memories, the Main Agent, the Rabbit Hole and the White Rabbit.</p> <p>This hook allows intercepting the start of this process and is executed in the middle of plugins and natural language objects loading.</p> <p>This hook can be used to set or store variables to be propagated to subsequent loaded objects.</p>"},{"location":"plugins/hooks-reference/lifecycle/before_cat_bootstrap/#arguments","title":"\ud83d\udcc4 Arguments","text":"<p>This hook has no input arguments, other than the default cat:</p> Name Type Description <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/lifecycle/before_cat_bootstrap/#return","title":"\u21a9\ufe0f Return","text":"<p>Returns? Oh no, dear developer, this function is a one-way trip into the rabbit hole.</p>"},{"location":"plugins/hooks-reference/lifecycle/before_cat_bootstrap/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef before_cat_bootstrap(cat):\n    # do whatever here\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/rabbit-hole/after_rabbithole_splitted_text/","title":"after_rabbithole_splitted_text","text":""},{"location":"plugins/hooks-reference/rabbit-hole/after_rabbithole_splitted_text/#after_rabbithole_splitted_text","title":"<code>after_rabbithole_splitted_text</code>","text":"<p>Intervene after the RabbitHole has split the document in chunks.</p> <p>Allows editing the list of <code>Document</code> right after the RabbitHole chunked them in smaller ones.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/after_rabbithole_splitted_text/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>chunks</code> <code>List[Document]</code> List of Langchain <code>Document</code>s with text chunks. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/rabbit-hole/after_rabbithole_splitted_text/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>List[Document]</code></p> <p>List of modified chunked Langchain <code>Document</code>s to be stored in the episodic memory.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/after_rabbithole_splitted_text/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef after_rabbithole_splitted_text(chunks, cat):\n    # post process the chunks\n    for chunk in chunks:\n        new_content = cat.llm(f\"Replace any dirty word with 'Meow': {chunk}\")\n        chunk.page_content = new_content\n\n    return chunks\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> <li>RabbitHole segmentation plugin</li> </ul>"},{"location":"plugins/hooks-reference/rabbit-hole/after_rabbithole_stored_documents/","title":"after_rabbithole_stored_documents","text":""},{"location":"plugins/hooks-reference/rabbit-hole/after_rabbithole_stored_documents/#after_rabbithole_stored_documents","title":"<code>after_rabbithole_stored_documents</code>","text":"<p>Intervene after the Rabbit Hole ended the ingestion pipeline.</p> <p>Allows editing and enhancing the list of <code>Document</code> after is inserted in the vector memory.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/after_rabbithole_stored_documents/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>source</code> <code>str</code> Name of ingested file/url. <code>docs</code> <code>List[PointStruct]</code> List of Qdrant <code>PointStruct</code> just inserted into the vector database. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/rabbit-hole/after_rabbithole_stored_documents/#return","title":"\u21a9\ufe0f Return","text":"<p>Returns? Oh no, dear developer, this function is a one-way trip into the rabbit hole.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/after_rabbithole_stored_documents/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef after_rabbithole_stored_documents(source, stored_points, cat):\n    # do whatever here\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_insert_memory/","title":"before_rabbithole_insert_memory","text":""},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_insert_memory/#before_rabbithole_insert_memory","title":"<code>before_rabbithole_insert_memory</code>","text":"<p>Intervene before the RabbitHole insert a <code>Document</code> in the declarative memory.</p> <p>Allows editing and enhancing a single <code>Document</code> before the RabbitHole add it to the declarative vector memory.</p> <p>Example of the Langchain document:</p> <pre><code>doc = Document(page_content=\"So Long, and Thanks for All the Fish\", metadata={})\n</code></pre>"},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_insert_memory/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>doc</code> <code>Document</code> Langchain <code>Document</code> to be inserted in memory. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components. <p>The <code>Document</code> has two properties:</p> <ul> <li><code>page_content</code>: the string with the text to save in memory;</li> <li><code>metadata</code>: a dictionary with at least two keys:<ul> <li><code>source</code>: where the text comes from;</li> <li><code>when</code>: timestamp to track when it's been uploaded.</li> </ul> </li> </ul> <p>Info</p> <p>Before adding the <code>doc</code>, the Cat will add <code>source</code> and <code>when</code> metadata with the file name and ingestion time.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_insert_memory/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>Document</code></p> <p>Langchain <code>Document</code> that is added in the declarative vector memory.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_insert_memory/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef before_rabbithole_insert_memory(doc, cat):\n    # insert the user id metadata\n    doc.metadata[\"user_id\"] = cat.user_id\n\n    return doc\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> <li>RabbitHole segmentation plugin</li> <li>Summarization plugin</li> </ul>"},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_splits_text/","title":"before_rabbithole_splits_text","text":""},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_splits_text/#before_rabbithole_splits_text","title":"<code>before_rabbithole_splits_text</code>","text":"<p>Intervene before the uploaded document is split into chunks.</p> <p>Allows editing the uploaded document main <code>Document</code>(s) before the RabbitHole recursively splits it in shorter ones. Please note that this is a list because parsers can output one or more <code>Document</code>, that are afterward split.</p> <p>For instance, the hook allows to change the text or edit/add metadata.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_splits_text/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>docs</code> <code>List[Document]</code> Langchain <code>Document</code>s resulted after parsing the file uploaded in the RabbitHole. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components. <p><code>doc</code> example:</p> <pre><code>docs = List[Document(page_content=\"This is a very long document before being split\", metadata={})]\n</code></pre>"},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_splits_text/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>List[Document]</code></p> <p>Edited Langchain <code>Document</code>s.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_splits_text/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef before_rabbithole_splits_text(docs, cat):\n    for doc in docs:\n        doc.page_content = doc.page_content.replace(\"dog\", \"cat\")\n    return docs\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> <li>RabbitHole segmentation plugin</li> </ul>"},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_stores_documents/","title":"before_rabbithole_stores_documents","text":""},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_stores_documents/#before_rabbithole_stores_documents","title":"<code>before_rabbithole_stores_documents</code>","text":"<p>Intervene before the Rabbit Hole starts the ingestion pipeline.</p> <p>Allows modifying how the list of <code>Document</code> is inserted in the vector memory.</p> <p>For example, this hook is a good point to summarize the incoming documents and save both original and summarized contents. An official plugin is available to test this procedure.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_stores_documents/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>docs</code> <code>List[Document]</code> List of chunked Langchain <code>Document</code>s before being inserted in memory. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_stores_documents/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>List[Document]</code></p> <p>List of Langchain <code>Document</code>s that will be stored in vector memory.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/before_rabbithole_stores_documents/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef before_rabbithole_stores_documents(docs, cat):\n    # summarize group of 5 documents and add them along original ones\n    summaries = []\n    for n, i in enumerate(range(0, len(docs), 5)):\n        # Get the text from groups of docs and join to string\n        group = docs[i: i + 5]\n        group = list(map(lambda d: d.page_content, group))\n        text_to_summarize = \"\\n\".join(group)\n\n        # Summarize and add metadata\n        summary = cat.llm(f\"Provide a concide summary of the following: {group}\")\n        summary = Document(page_content=summary)\n        summary.metadata[\"is_summary\"] = True\n        summaries.append(summary)\n\n    return docs.extend(summaries)\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> <li>Summarization plugin</li> </ul>"},{"location":"plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_parsers/","title":"rabbithole_instantiates_parsers","text":""},{"location":"plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_parsers/#rabbithole_instantiates_parsers","title":"<code>rabbithole_instantiates_parsers</code>","text":"<p>Hook the available parsers for ingesting files in the declarative memory.</p> <p>Allows replacing or extending existing supported mime types and related parsers to customize the file ingestion.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_parsers/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>file_handlers</code> <code>dict</code> A dictionary in which keys are the supported mime types and values are the related parsers. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_parsers/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>dict</code></p> <p>Edited dictionary of supported mime types and related parsers.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_parsers/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\nfrom langchain.document_loaders.parsers.language.language_parser import LanguageParser\nfrom langchain.document_loaders.parsers.msword import MsWordParser\n\n@hook  # default priority = 1\ndef rabbithole_instantiates_parsers(file_handlers, cat):\n    new_handlers = {\n        \"text/x-python\": LanguageParser(language=\"python\"),\n        \"text/javascript\": LanguageParser(language=\"js\"),\n        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\": MsWordParser(),\n        \"application/msword\": MsWordParser(),\n    }\n    file_handlers = file_handlers | new_handlers\n    return file_handlers\n</code></pre> <p>Note</p> <ul> <li>IngestAnything Plugin</li> <li>Debugger plugin</li> </ul>"},{"location":"plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_splitter/","title":"rabbithole_instantiates_splitter","text":""},{"location":"plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_splitter/#rabbithole_instantiates_splitter","title":"<code>rabbithole_instantiates_splitter</code>","text":"<p>Hook the splitter used to split text in chunks.</p> <p>Allows replacing the default text splitter to customize the splitting process.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_splitter/#arguments","title":"\ud83d\udcc4 Arguments","text":"Name Type Description <code>text_splitter</code> <code>TextSplitter</code> The text splitter used by default, currently is the Langchain's <code>TextSplitter</code>. <code>cat</code> StrayCat Cheshire Cat instance, allows you to use the framework components."},{"location":"plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_splitter/#return","title":"\u21a9\ufe0f Return","text":"<p>Type: <code>TextSplitter</code></p> <p>An instance of a TextSplitter subclass.</p>"},{"location":"plugins/hooks-reference/rabbit-hole/rabbithole_instantiates_splitter/#example","title":"\u270d Example","text":"<pre><code>from cat.mad_hatter.decorators import hook\n\n@hook  # default priority = 1\ndef rabbithole_instantiates_splitter(text_splitter, cat):\n    text_splitter._chunk_size = 64\n    text_splitter._chunk_overlap = 8\n    return text_splitter\n</code></pre> <p>Note</p> <ul> <li>Debugger plugin</li> </ul>"},{"location":"plugins/plugins-registry/plugin-from-template/","title":"Using the Plugin Template","text":""},{"location":"plugins/plugins-registry/plugin-from-template/#using-the-plugin-template","title":"\ud83d\udd0c Using the Plugin Template","text":"<p>We have prepared a GitHub template for you to expedite the creation of a new plugin, ready for publication in the public Registry. The template includes a complete scaffolding for the plugin and the GitHub action configuration to release the plugin package.</p>"},{"location":"plugins/plugins-registry/plugin-from-template/#creating-the-new-plugin","title":"Creating the new plugin","text":"<p>In the example we will create a plugin for the Poetic Socks Seller (refer to the Quickstart section if you're not familiar with it). In the next steps, replace <code>poetic_sock_seller</code> with the name of you futuristic plugin!</p> <p>Navigate to the plugin-template GitHub repository, click on <code>Use this template</code> and, then, <code>Create a new repository</code>:</p> <p></p> <p>as repository name and then click on <code>Create repository</code>:</p> <p></p>"},{"location":"plugins/plugins-registry/plugin-from-template/#cloning-the-plugin","title":"Cloning the Plugin","text":"<p>Now that you set up the remote repository on GitHub, you need to set up the code locally. Hence, clone the repository directly in the Cat\u2019s plugins folder on your machine:</p> <pre><code>cd core/cat/plugins\ngit clone https://github.com/[your_account_name]/poetic_sock_seller.git\n</code></pre>"},{"location":"plugins/plugins-registry/plugin-from-template/#customizing-the-plugin","title":"Customizing the Plugin","text":"<p>Finally, run the setup.py script to customize the repository:</p> <pre><code>cd poetic_sock_seller\npython setup.py\n</code></pre> <p>The script will prompt you to write the name of your plugin, <code>Poetic Sock Seller</code>. The output in the terminal should look like:</p> <p></p> <p>The template contains a source code example, look at it in the <code>poetic_sock_seller.py</code> file.</p>"},{"location":"plugins/plugins-registry/plugin-from-template/#release-creation","title":"\ud83d\udce6 Release Creation","text":"<p>A repository created with our template automatically includes the creation of a release on GitHub through a GitHub action. This automation happens whenever you push changes to the <code>main</code> branch and the <code>version</code> number in the <code>plugin.json</code> file changes. The release is automatically tagged with the version number and released in all the formats supported by GitHub.</p> <p>For details about the GitHub action, refer to the file <code>.github/workflows/main.yml</code>.</p>"},{"location":"plugins/plugins-registry/publishing-plugin/","title":"Publishing a Plugin","text":""},{"location":"plugins/plugins-registry/publishing-plugin/#publishing-a-plugin-in-the-registry","title":"\ud83d\udce4 Publishing a Plugin in the Registry","text":"<p>Publishing your plugin and making it available to the whole world is a relatively simple yet crucial step. Take a few minutes to read this section of the guide. Once done, you won't be able to stop!</p>"},{"location":"plugins/plugins-registry/publishing-plugin/#start-on-the-right-foot","title":"\ud83d\udc5f Start on the Right Foot","text":"<p>A plugin that will be published in our public registry requires some precautions and must have a <code>plugin.json</code> file within the root folder with all the fields as complete as possible. This ensures that your plugin is attractive and searchable through the dedicated \"Plugins\" tab of the Cheshire Cat.</p> <p>To make it easier for you, we have provided a GitHub repository template so that you only need to clone it and find yourself with a folder ready to develop your first public plugin without worries.</p> <p>You can find the repository at this address: https://github.com/cheshire-cat-ai/plugin-template</p> <p>Click on the colorful \"Use this template\" button at the top and choose to create a new repository. Once you've chosen the name for your repository and cloned the code to your machine, you can run the setup script to clean up the files and rename everything as needed.</p> <pre><code>python setup.py\n</code></pre> <p>To learn more about how to work with the plugin template, read this dedicated page.</p>"},{"location":"plugins/plugins-registry/publishing-plugin/#release-creation","title":"\ud83d\udce6 Release Creation","text":"<p>We recommend using GitHub's release system to effectively manage your plugin releases. Our registry can always download the latest stable release of your plugin tagged on GitHub. You can do this manually or through automation.</p> <p>A repository created with our template automatically includes the creation of a release on GitHub through a GitHub action. This automation happens whenever the <code>version</code> number in the <code>plugin.json</code> file changes. The release is automatically tagged with the version number and released in all the formats supported by GitHub.</p> <p>Here's the documentation related to managing releases on GitHub: https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository</p> <p>Info</p> <p>Remember to change the <code>version</code> number in <code>plugin.json</code> only when you actually want to create a new version of your plugin. While you're in development, you can either open a develop branch (the automation only runs on the main branch) or continue to push to the main branch without changing the version number.</p>"},{"location":"plugins/plugins-registry/publishing-plugin/#take-care-of-the-pluginjson","title":"\ud83d\udcdc Take Care of the <code>plugin.json</code>","text":"<p>As you may have realized, the <code>plugin.json</code> file is what governs all aspects of publishing your plugin and contains the fields that will help your plugin stand out within the registry. Therefore, take care to fill it out as comprehensively as possible and try to complete all the available fields.</p> <p>In reality, there are only 3 mandatory fields for publishing in the registry: <code>name</code>, <code>author_name</code> and <code>version</code>. However, we strongly recommend adding a couple of <code>tags</code> and a <code>description</code> as well. It's through these fields that our search system will be able to discover your plugin.</p>"},{"location":"plugins/plugins-registry/publishing-plugin/#explanation-of-the-fields","title":"\ud83d\udcc3 Explanation of the fields","text":"<p>Below is a list of the fields with a brief explanation.</p> <p>Warning</p> <p>Fields marked with the asterisk (*) are mandatory.</p> <ul> <li><code>name</code>*: The name of your plugin.</li> <li><code>version</code>*: The last stable version number.</li> <li><code>author_name</code>*: The author's name or nickname.</li> <li><code>description</code>: A brief description of the plugin.</li> <li><code>author_url</code>: Link to the author's website.</li> <li><code>plugin_url</code>: Link to the plugin's website with the full description/documentation (can be a different link from the GitHub repository).</li> <li><code>tags</code>: A comma-separated list of tags (e.g., \"multimedia, documents, pdf, csv\").</li> <li><code>thumb</code>: The direct link to your plugin's logo image. Recommended minimum size is 160x160px. Recommended formats are png or jpg.</li> </ul>"},{"location":"plugins/plugins-registry/publishing-plugin/#submit-your-plugin-for-review","title":"\ud83d\udc40 Submit Your Plugin for Review","text":"<p>The submission and review process is done through our plugins GitHub repository and it's quite straightforward. All you need to do is fork the repository and then, after adding your plugin to the JSON file, submit a Pull Request to us.</p> <p>The fields to add to the new object you'll be adding are as follows:</p> <ul> <li><code>name</code>: The name of your plugin for identification in the list (for public display, the name contained in your <code>plugin.json</code> will be used).</li> <li><code>url</code>: The link to your public GitHub repository.</li> </ul> <p>The review process may take a few days, so don't worry if some time passes before you see your plugin approved. This depends on the number of plugins in the queue and the availability of volunteers. We will strive to provide feedback as quickly as possible.</p> <p>The review is in place to prevent the publication of plugins containing malware, obvious security flaws, or of low quality and relevance. We will be diligent, but we ask for your understanding and request that you always submit tested plugins that do not jeopardize the security of our users.</p>"},{"location":"plugins/plugins-registry/publishing-plugin/#registry-cache-expiry","title":"\u23f3 Registry Cache Expiry","text":"<p>Upon successful submission, your plugin enters our registry cache, which updates periodically. However, it's important to note that this cache has a duration of 24 hours (1440 minutes) before it refreshes. During this time, newly submitted plugins might not immediately appear in the registry. Why the Wait?</p> <p>The caching mechanism optimizes the performance of our registry, efficiently managing and updating plugin listings.</p> <p>If, after patiently waiting within this 24-hour window, your plugin doesn't show up in the registry, then it's time to let us know. We're here to assist you! Reach out to our support team or report the issue via our plugins GitHub repository. Please provide relevant details, including the submission date and any steps you've taken.</p> <p>Remember, our goal is to make your plugin available to our community seamlessly. Your cooperation and patience in allowing for the cache expiry are appreciated as we work to ensure a smooth plugin publishing process.</p>"},{"location":"plugins/plugins-registry/publishing-plugin/#stay-updated","title":"\ud83d\udd14 Stay Updated","text":"<p>The final step is to stay informed about what's happening in the magical world of Cheshire Cat so that you can keep your plugin up to date with the latest developments. To facilitate this, we've created a dedicated channel for plugin developers on our official Discord server.</p> <p>We invite you to become a part of our community and let a moderator know that you've submitted a plugin. Once your plugin is approved, we'll be happy to assign you a special role (Plugin Developer) and unlock all the dedicated channels for you.</p> <p>If you don't use Discord or prefer not to login on our server, we still encourage you to try to keep up with Cheshire Cat AI's updates. We'll conduct periodic review cycles, and if your plugin becomes too outdated or non-functional after some time, we may have to remove it from the registry.</p> <p>Looking forward to seeing you among our amazing plugin developers!</p>"},{"location":"production/administrators/backups-updates/","title":"Backups and Updates","text":""},{"location":"production/administrators/backups-updates/#backups","title":"Backups","text":"<p>To perform a full backup, just save your folder containing the <code>compose.yml</code> file and all the volumes (<code>data</code>, <code>plugins</code> and <code>static</code>). Cat's code and runtime is fully encapsulated in the docker image, no need to copy it anywhere.</p> <p>Same goes for the vector memory:</p> <ul> <li>if you are not using the Qdrant container, all the collections are saved in <code>cat/data/local_vector_memory</code>, which pertains the case described above.</li> <li>if you are using the Qdrant container, and you defined volumes for it, just save the volumes' contents.</li> </ul> <p>You can easily setup an automated backup system using tools like rsync and a simple cron.</p>"},{"location":"production/administrators/backups-updates/#backup-restoration","title":"Backup restoration","text":"<p>You should be able to restore a full backed up installation simply putting the folder you saved, containing <code>compose.yml</code> and volumes, anywhere on a docker enabled system. Then run:</p> <pre><code>cd &lt;folder&gt;\ndocker compose pull\ndocker compose up -d\n</code></pre>"},{"location":"production/administrators/backups-updates/#updates","title":"Updates","text":"<p>We are trying to respect semantic versioning, but the project is really young and there may be some retrocompatibility hiccups. We are sorry for any inconvenience.</p> <p>If you have version <code>x.y.z</code> you should be able to update your Cat to any <code>x.*.*</code> version by just changing container tag in the <code>compose.yml</code> file.</p> <p>Example:</p> <pre><code>  cheshire-cat-core:\n    image: ghcr.io/cheshire-cat-ai/core:1.6.3\n</code></pre> <p>Becomes:</p> <pre><code>  cheshire-cat-core:\n    image: ghcr.io/cheshire-cat-ai/core:1.7.1\n</code></pre> <p>Then you pull the new image and you are good to go.</p> <pre><code># enter the installation folder\ncd &lt;folder&gt;\n\n# stop the cat\ndocker compose down\n\n# change the tag in `compose.yml`\n\n# update image\ndocker compose pull\n\n# start the cat\ndocker compose up -d\n</code></pre>"},{"location":"production/administrators/docker-compose/","title":"Docker Compose","text":"<p>To facilitate, speed up, and standardize the Cat's user experience, the Cat lives inside a Docker container. This allows for maximum power, flexibility and portability, but requires a little understanding from you on how docker works.</p>"},{"location":"production/administrators/docker-compose/#cat-standalone","title":"Cat standalone","text":"<pre><code>services:\n\n  cheshire-cat-core:\n    image: ghcr.io/cheshire-cat-ai/core:latest\n    container_name: cheshire_cat_core\n    ports:\n      - 1865:80\n    volumes:\n      - ./static:/app/cat/static\n      - ./plugins:/app/cat/plugins\n      - ./data:/app/cat/data\n</code></pre>"},{"location":"production/administrators/docker-compose/#cat-ollama","title":"Cat + Ollama","text":"<p>The Cat is model agnostic, meaning you can attach your preferred LLM and embedder model/provider. The Cat supports the most used ones, but you can increase the number of models/providers by plugins, here is a list of the main ones:</p> <ol> <li>OpenAI and Azure OpenAI</li> <li>Cohere</li> <li>Ollama</li> <li>HuggingFace TextInference API (LLM model only)</li> <li>Google Gemini</li> <li>Qdrant FastEmbed (Embedder model only)</li> <li>(custom via plugin)</li> </ol> <p>Let's see a full local setup for Cat + Ollama. To make this work properly, be sure your docker engine can see the GPU via NVIDIA docker.</p> <p>As an alternative you can install Ollama directly on your machine and making the Cat aware of it in the Ollama LLM settings, inserting your local network IP or using <code>host.docker.internal</code>.</p> <p>Example <code>compose.yml</code>:</p> <pre><code>services:\n\n  cheshire-cat-core:\n    image: ghcr.io/cheshire-cat-ai/core:latest\n    container_name: cheshire_cat_core\n    depends_on:\n      - ollama\n    ports:\n      - 1865:80\n    volumes:\n      - ./static:/app/cat/static\n      - ./plugins:/app/cat/plugins\n      - ./data:/app/cat/data\n    restart: unless-stopped\n\n  ollama:\n    container_name: ollama_cat\n    image: ollama/ollama:latest\n    volumes:\n      - ./ollama:/root/.ollama\n    expose:\n      - 11434\n    environment:\n      - gpus=all\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre> <p>For a solid local embedder, choose one from the FastEmbed list. The Cat will download the embedder in <code>cat/data</code> and will use it to embed text. Congratulations, you are now free from commercial LLM providers.</p>"},{"location":"production/administrators/docker-compose/#cat-qdrant","title":"Cat + Qdrant","text":"<p>By default the Core uses an embedded version of Qdrant, based on SQLite. It is highly recommended to connect the Cat to a full Qdrant instance to increase performance and scalability!</p> <pre><code>services:\n\n  cheshire-cat-core:\n    image: ghcr.io/cheshire-cat-ai/core:latest\n    container_name: cheshire_cat_core\n    depends_on:\n      - cheshire-cat-vector-memory\n    env_file:\n      - .env\n    ports:\n      - 1865:80\n    volumes:\n      - ./static:/app/cat/static\n      - ./plugins:/app/cat/plugins\n      - ./data:/app/cat/data\n    restart: unless-stopped\n\n  cheshire-cat-vector-memory:\n    image: qdrant/qdrant:latest\n    container_name: cheshire_cat_vector_memory\n    expose:\n      - 6333\n    volumes:\n      - ./long_term_memory/vector:/qdrant/storage\n    restart: unless-stopped\n</code></pre> <p>Add this enviroment variables in your <code>.env</code> file:</p> <pre><code># Qdrant server\nCCAT_QDRANT_HOST=cheshire_cat_vector_memory # &lt;url of the cluster&gt;\nCCAT_QDRANT_PORT=6333 # &lt;port of the cluster, usually 6333&gt;\nCCAT_QDRANT_API_KEY=\"\" # optional &lt;api-key&gt;\n</code></pre>"},{"location":"production/administrators/docker-compose/#cat-reverse-proxy","title":"Cat + Reverse proxy","text":""},{"location":"production/administrators/docker-compose/#cat-with-caddy-automatic-https","title":"Cat with Caddy (automatic https)","text":"<p>This is your <code>compose.yml</code></p> <pre><code>services:\n\n  cat:\n    image: ghcr.io/cheshire-cat-ai/core:latest\n    container_name: cheshire_cat_core\n    env_file:\n      - .env\n    expose:\n      - 80\n    volumes:\n      - ./cat/static:/app/cat/static\n      - ./cat/plugins:/app/cat/plugins\n      - ./cat/data:/app/cat/data\n\n  caddy:\n    image: caddy:latest\n    depends_on:\n      - cat\n    ports:\n      - 80:80\n      - 443:443\n      - 443:443/udp\n    volumes:\n      - ./caddy/Caddyfile:/etc/caddy/Caddyfile\n      - ./caddy/data:/data\n      - ./caddy/config:/config\n    restart: unless-stopped\n</code></pre> <p>Add this enviroment variable in your <code>.env</code> file:</p> <pre><code>CCAT_HTTPS_PROXY_MODE=1\n</code></pre> <p>And in <code>caddy/Caddyfile</code>: </p><pre><code>localhost {\n    reverse_proxy cat:80\n}\n</code></pre> <p>Now you can reach your cat at <code>https://localhost</code>!</p>"},{"location":"production/administrators/env-variables/","title":"Environment Variables","text":""},{"location":"production/administrators/env-variables/#environment-variables","title":"Environment Variables","text":"<p>The Core can be configured using environment variables, the values are read during Cat bootstrap.</p>"},{"location":"production/administrators/env-variables/#how-to-set-environment-variables","title":"How to set Environment Variables","text":"<p>To set environment variables:</p> <ul> <li>Create a file named <code>.env</code> at the same level of the <code>compose.yml</code> file.</li> <li>Here there is a example you can use as a reference.</li> <li> <p>Add to <code>compose.yml</code> the command to read the <code>.env</code>:</p> <pre><code>services:\n  cheshire-cat-core:\n  image: ghcr.io/cheshire-cat-ai/core:latest\n  container_name: cheshire_cat_core\n  env_file:\n    - .env\n  ports:\n    - ${CORE_PORT:-1865}:80\n  volumes:\n    - ./static:/app/cat/static\n    - ./plugins:/app/cat/plugins\n    - ./data:/app/cat/data\n</code></pre> </li> <li> <p>The command <code>docker compose up</code> will now read the <code>.env</code> file and set the environment variables for the container.</p> </li> </ul>"},{"location":"production/administrators/env-variables/#network","title":"Network","text":""},{"location":"production/administrators/env-variables/#ccat_core_host","title":"CCAT_CORE_HOST","text":"<p>Default value: <code>localhost</code></p> <p>The host at which the Cat is running. The variable is used by Admin Portal to determine the host to connect to. If your installation has to be served on mywebsite.com, have in your <code>.env</code>: <code>CCAT_CORE_HOST=mywebsite.com</code></p>"},{"location":"production/administrators/env-variables/#ccat_core_port","title":"CCAT_CORE_PORT","text":"<p>Default value: <code>1865</code></p> <p>The port the Cat has to listen to, for both admin and REST API. Easter egg: <code>1865</code> is the year \"Alice in Wonderland\" was published.</p>"},{"location":"production/administrators/env-variables/#ccat_core_use_secure_protocols","title":"CCAT_CORE_USE_SECURE_PROTOCOLS","text":"<p>Default value: <code>false</code></p> <p>By default, the core APIs are exposed using the HTTP/WS protocol, set this parameter to <code>true</code> if you expose the API using the HTTPS/WSS protocol, for example using NGIX in front of the Cat.</p>"},{"location":"production/administrators/env-variables/#ccat_cors_allowed_origins","title":"CCAT_CORS_ALLOWED_ORIGINS","text":"<p>Default value: <code>*</code></p> <p>By default, the core APIs can be consumed from all origins, using the parameter you can restrict which origins can consume the APIs.</p>"},{"location":"production/administrators/env-variables/#ccat_https_proxy_mode","title":"CCAT_HTTPS_PROXY_MODE","text":"<p>Default value: <code>false</code></p> <p>Enable this variable if you are using a proxy like Nginx with SSL in front of the Cat, otherwise https will give redirection problems.</p> <p>This option is mapped to the <code>--proxy_headers</code> Uvicorn option, you can reference the Uvicorn HTTP setting page for more info.</p>"},{"location":"production/administrators/env-variables/#ccat_cors_forwarded_allow_ips","title":"CCAT_CORS_FORWARDED_ALLOW_IPS","text":"<p>Default value: <code>*</code></p> <p>When the <code>CCAT_HTTPS_PROXY_MODE</code> option is enabled, this option is mapped to the <code>--forwarded-allow-ips</code> Uvicorn option, you can reference the Uvicorn HTTP setting page for more info.</p>"},{"location":"production/administrators/env-variables/#security","title":"Security","text":""},{"location":"production/administrators/env-variables/#ccat_api_key","title":"CCAT_API_KEY","text":"<p>Default value: <code>[empty]</code></p> <p>By default, the core HTTP API does not require any authorization. If you set this variable all HTTP endpoints will require an <code>Authorization: Bearer &lt;ccat_api_key&gt;</code> header. Failure to provide the correct key will result in a 403 error. Websocket endpoints will remain open, unless you set <code>CCAT_API_KEY_WS</code> (see below).</p> <p>If along the HTTP API call you want to communicate the endpoint also which user is making the request, use the <code>user_id: &lt;my_user_id&gt;</code> header. If you don't, the Cat will assume <code>user_id: user</code>.</p> <p>Keep in mind that api keys are intended for machine-2-machine communication; If you are talking to the Cat from a browser, set the api keys to secure your installation, but only communicate with the Cat via JWT (TODO: insert JWT tutorial).</p>"},{"location":"production/administrators/env-variables/#ccat_api_key_ws","title":"CCAT_API_KEY_WS","text":"<p>Default value: <code>[empty]</code></p> <p>By default, WebSocket endpoints are open to the public. If you want to lock them down, set this environment variable, e.g. <code>CCAT_API_KEY_WS=meows</code>.</p> <p>To pass the gate, call the WS endpoint using a <code>token</code> query parameter: Example <code>ws://localhost:1865/ws/&lt;user_id&gt;?token=&lt;ccat_api_key_ws&gt;</code>.</p> <p>Keep in mind that api keys are intended for machine-2-machine communication; If you are talking to the Cat from a browser, set the api keys to secure your installation, but only communicate with the Cat via JWT (TODO: insert JWT tutorial).</p>"},{"location":"production/administrators/env-variables/#ccat_jwt_secret","title":"CCAT_JWT_SECRET","text":"<p>Default value: <code>secret</code></p> <p>Secret for issueing and validating JWTs. Must be personalized along <code>CCAT_API_KEY</code> and <code>CCAT_APIKEY_WS</code> to make the installation secure.</p>"},{"location":"production/administrators/env-variables/#ccat_jwt_algorithm","title":"CCAT_JWT_ALGORITHM","text":"<p>Default value: <code>HS256</code></p> <p>Algorithm to sign the JWT with <code>CCAT_JWT_SECRET</code>.</p>"},{"location":"production/administrators/env-variables/#ccat_jwt_expire_minutes","title":"CCAT_JWT_EXPIRE_MINUTES","text":"<p>Default value: <code>1440</code> </p> <p>By default a JWT expires after 1 day.</p>"},{"location":"production/administrators/env-variables/#debug","title":"Debug","text":""},{"location":"production/administrators/env-variables/#ccat_debug","title":"CCAT_DEBUG","text":"<p>Default value: <code>true</code></p> <p>By default changes to files in the root folder of the Cat force a restart of the Core. This is useful during the development of Plugins. This behavior can be switched off in production by setting to <code>false</code>.</p>"},{"location":"production/administrators/env-variables/#ccat_log_level","title":"CCAT_LOG_LEVEL","text":"<p>Default value: <code>INFO</code></p> <p>The log level, available levels are: - <code>DEBUG</code> - <code>INFO</code> - <code>WARNING</code> - <code>ERROR</code> - <code>CRITICAL</code> </p>"},{"location":"production/administrators/env-variables/#vector-db","title":"Vector DB","text":""},{"location":"production/administrators/env-variables/#ccat_qdrant_host","title":"CCAT_QDRANT_HOST","text":"<p>Default value: <code>[empty]</code></p> <p>The host on which Qdrant is running. Cat provides a ready-to-use file based Qdrant, embedded in <code>cat/data/local_vector_memory</code>. If you want to use an external instance of Qdrant or a separated container in <code>compose.yml</code>, use this parameter to specify the host where it is running. You can also optionally specify the protocol to use in the URL to make a secure connection (for example <code>https://example.com</code>).</p> <p>See the <code>local-cat</code> repo for an example usage of Qdrant as a container.</p>"},{"location":"production/administrators/env-variables/#ccat_qdrant_port","title":"CCAT_QDRANT_PORT","text":"<p>Default value: <code>6333</code></p> <p>The port on which Qdrant is running, in case you use an external host or another container inside the <code>compose.yml</code>.</p>"},{"location":"production/administrators/env-variables/#ccat_qdrant_api_key","title":"CCAT_QDRANT_API_KEY","text":"<p>Default value: <code>[empty]</code></p> <p>This is used to set the Qdrant Api Key in the client connection statement. It should be configured if an Api Key is set up on the Qdrant Server, or if you are using the cloud version.</p>"},{"location":"production/administrators/env-variables/#ccat_save_memory_snapshots","title":"CCAT_SAVE_MEMORY_SNAPSHOTS","text":"<p>Default value: <code>false</code></p> <p>Set to <code>ftrue</code> to turn on Vector Database snapshots, so when you change embedder an automatic backup will be saved on disk. Please note:</p> <ul> <li>Snapshots are painfully slow.</li> <li>We have not implemented a routine to reimport the snapshot.</li> </ul>"},{"location":"production/administrators/env-variables/#others","title":"Others","text":""},{"location":"production/administrators/env-variables/#ccat_metadata_file","title":"CCAT_METADATA_FILE","text":"<p>Default value: <code>cat/data/metadata.json</code></p> <p>The name of the file that contains all the Cat settings.</p>"},{"location":"production/administrators/make_the_cat_private/","title":"Make the Cat Private","text":"<p>The Cat installation is open by default, which means that all APIs are publicly accessible. Below is a summary of the steps necessary to secure the installation.</p> <ul> <li> <p>Change the password for all the users using via Admin Portal</p> </li> <li> <p>Secure the REST APIs and WebSocket by setting the following environment variables: </p><pre><code>CCAT_API_KEY=a-very-long-and-alphanumeric-secret\nCCAT_API_KEY_WS=another-very-long-and-alphanumeric-secret\n</code></pre> </li> <li>Set a JWT hash secret to encrypt user data: <pre><code>CCAT_JWT_SECRET=yet-another-very-long-and-alphanumeric-secret\n</code></pre></li> </ul>"},{"location":"production/administrators/tests/","title":"Automatic Tests","text":""},{"location":"production/administrators/tests/#testing","title":"\ud83d\udd2c Testing","text":"<p>Tests will run with mock databases and a mock plugin folder, so your instance will not be impacted. End to end (e2e) tests are found in <code>tests/routes</code>, while all the other folders contain unit tests and mocks / utilities.</p>"},{"location":"production/administrators/tests/#run-all-tests","title":"Run all tests","text":"<p>Open a terminal in the same folder your Cat is, then launch:</p> <pre><code>docker compose run --rm cheshire-cat-core python -m pytest --color=yes .\n</code></pre>"},{"location":"production/administrators/tests/#run-a-specific-test-file","title":"Run a specific test file","text":"<p>If you want to run specific test files and not the whole suite, just specify the path:</p> <pre><code>docker compose run --rm cheshire-cat-core python -m pytest --color=yes tests/routes/memory/test_memory_recall.py\n</code></pre>"},{"location":"production/administrators/tests/#run-a-specific-test-function-in-a-specific-test-file","title":"Run a specific test function in a specific test file","text":"<p>You can also launch only one specific test function, using the <code>::</code> notation and the name of the function:</p> <pre><code>docker compose run --rm cheshire-cat-core python -m pytest --color=yes tests/routes/memory/test_memory_recall.py::test_memory_recall_with_k_success\n</code></pre>"},{"location":"production/advanced/contributing/","title":"Contributing","text":""},{"location":"production/advanced/contributing/#contributing","title":"Contributing","text":"<p>Thank you for considering code contribution. If you wanto to learn how the Cat works and join its development, there is a different installation process to follow.</p>"},{"location":"production/advanced/contributing/#development-setup","title":"Development setup","text":"<ul> <li>Clone the repository on your machine</li> </ul> <pre><code>git clone https://github.com/cheshire-cat-ai/core.git cheshire-cat\n</code></pre> <ul> <li>Enter the created folder</li> </ul> <pre><code>cd cheshire-cat\n</code></pre> <ul> <li>Run docker container</li> </ul> <pre><code>docker compose up\n</code></pre> <p>The first time you run the <code>docker compose up</code> command, it will take several minutes to build the Docker Cat image. Once finished, the Cat will be living and running!  </p> <p>To stop the Cat hit CTRL-C in the terminal, you should see the logs stopping. Then run:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"production/advanced/contributing/#update-development-setup","title":"Update development setup","text":"<p>Remember to update often both your fork and your local clone. Before each session, follow these steps:</p> <ul> <li>Enter the folder where you cloned the repository</li> </ul> <pre><code>cd cheshire-cat\n</code></pre> <ul> <li>Pull updates from the GitHub repository</li> </ul> <pre><code>git pull\n</code></pre> <ul> <li>Build again the docker container</li> </ul> <pre><code>docker compose build --no-cache\n</code></pre> <ul> <li>Remove dangling images (optional)</li> </ul> <pre><code>docker rmi -f $(docker images -f \"dangling=true\" -q)\n</code></pre> <ul> <li>Run docker containers</li> </ul> <pre><code>docker compose up\n</code></pre>"},{"location":"production/advanced/contributing/#your-first-code-contribution","title":"Your First Code Contribution","text":"<ol> <li>Checkout the <code>develop</code> branch (<code>git checkout -b develop</code> and then <code>git pull origin develop</code>)</li> <li>Create your Feature Branch (<code>git checkout -b feature/AmazingFeature</code>)</li> <li>Commit your Changes (<code>git commit -m 'Add some AmazingFeature'</code>)</li> <li>Push to the Branch (<code>git push origin feature/AmazingFeature</code>)</li> <li>Open a Pull Request against the <code>develop</code> branch (if it contains lots of code, please discuss it beforehand opening a issue)</li> </ol>"},{"location":"production/advanced/contributing/#important-notes","title":"Important notes","text":"<ul> <li>try to discuss your contribution beforehand in an issue, to make an actually useful PR</li> <li>try to keep your PR small, single feature / fix and to the point</li> <li>branch out from <code>develop</code> and make your PR against <code>develop</code>; branch <code>main</code> is only used for releases</li> </ul>"},{"location":"production/advanced/contributing/#improving-the-documentation","title":"Improving The Documentation","text":"<p>Docs contribution are highly valuable for the project. See details on how to help with the docs here.</p>"},{"location":"production/auth/authentication/","title":"Authentication","text":""},{"location":"production/auth/authentication/#authentication","title":"Authentication","text":"<p>All the Cat endpoints are wide open by default, and so anyone can authenticate and administer the installation. Before going to production, follow the steps below to secure communication. We will provide Javascript and Python examples to test each step and show you how to authenticate from external clients.</p> <p>TL;DR: to secure your Cat add the following to your <code>.env</code>:</p> <pre><code>CCAT_API_KEY=a-very-long-and-alphanumeric-secret\nCCAT_API_KEY_WS=another-very-long-and-alphanumeric-secret\nCCAT_JWT_SECRET=yet-another-very-long-and-alphanumeric-secret\n</code></pre> <p>Make sure docker is loading your <code>.env</code> file containing these environment variables. See here.</p>"},{"location":"production/auth/authentication/#1-securing-api-keys","title":"1. Securing API keys","text":"<p>Two environment variables allow you to secure Cat's endpoints and require authentication for each request:</p> <ul> <li><code>CCAT_API_KEY</code>: locks down HTTP endpoints</li> <li><code>CCAT_API_KEY_WS</code>: locks down WebSocket endpoints</li> </ul> <p>Warning</p> <p>Even if you set both <code>CCAT_API_KEY</code> and <code>CCAT_API_KEY_WS</code>, an intruder can still hack you by self signing a JWT. See below how to secure JWT.</p>"},{"location":"production/auth/authentication/#http-key","title":"HTTP key","text":"<p>On a fresh installation, the Cat is talking to strangers:</p> JavaScriptPython <pre><code>let response = await fetch(\"http://localhost:1865\")\nlet json = await response.json()\n\nconsole.log(response.status, json)\n</code></pre> <pre><code>import requests\n\nresponse = requests.get(\"http://localhost:1865\")\n\nprint(response.status_code, response.json())\n</code></pre> <p>Output will be: </p><pre><code>200 {\"status\": \"We're all mad here, dear!\", \"version\": \"x.y.z\"}\n</code></pre> <p>To secure HTTP endpoints, set the <code>CCAT_API_KEY</code> environment variable. This will restrict access requiring the value of <code>CCAT_API_KEY</code> to authenticate requests.</p> <p>Example <code>.env</code>: </p><pre><code>CCAT_API_KEY=meow\n</code></pre> <p>If we try the same client code as above, we obtain: </p><pre><code>403 {\"detail\": {\"error\": \"Invalid Credentials\"}}\n</code></pre> <p>To pass the gate, we need to set an header in the format <code>Authorization: Bearer &lt;CCAT_API_KEY&gt;</code>. We can optionally set a <code>user_id</code> header to tell the Cat which specific user is making the request, otherwise <code>user_id: user</code> will be assumed.</p> JavaScriptPython <pre><code>let response = await fetch(\n    \"http://localhost:1865\",\n    {\n        \"headers\": {\n            \"Authorization\": \"Bearer meow\",\n            \"user_id\": \"Caterpillar\" // optional\n        }\n    }\n)\nlet json = await response.json()\n\nconsole.log(response.status, json)\n</code></pre> <pre><code>import requests\n\nresponse = requests.get(\n    \"http://localhost:1865\",\n    headers = {\n        \"Authorization\": \"Bearer meow\",\n        \"user_id\": \"Caterpillar\" # optional\n    }\n)\n\nprint(response.status_code, response.json())\n</code></pre> <p>The Cat is open again, but only to real friends.</p>"},{"location":"production/auth/authentication/#websocket-key","title":"WebSocket key","text":"<p>To secure WebSocket communication, set the <code>CCAT_API_KEY_WS</code> environment variable. This will lock down the Cat\u2019s WebSocket endpoints used for conversations. You'll need to provide the <code>CCAT_API_KEY_WS</code> value to establish a connection.</p> <p>On a fresh installation we can chat to the Cat like this:</p> JavaScriptPython <pre><code>let ws = new WebSocket(\"ws://localhost:1865/ws\")\n\nws.onopen = function() {\n    ws.send(JSON.stringify({\"text\": \"It's late\"}))\n}\n\nws.onmessage = function(msg){\n    console.log(JSON.parse(msg.data))\n}\n</code></pre> <pre><code>import json\nimport asyncio\nfrom websockets.asyncio.client import connect\n\nasync def cat_chat():\n    async with connect(f\"ws://localhost:1865/ws\") as websocket:\n\n        await websocket.send(json.dumps({\"text\": \"It's late\"}))\n\n        async for message in websocket:\n            cat_response = json.loads(message)\n            print(cat_response[\"content\"])\n            if cat_response[\"type\"] == \"chat\":\n                break\n\nasyncio.run(cat_chat())\n</code></pre> <p>You will see a stream of tokens and the final message. Now set <code>CCAT_API_KEY_WS</code> in our <code>.env</code>:</p> <p></p><pre><code>CCAT_API_KEY_WS=meow_ws\n</code></pre> Running the code above will result in a <code>403</code> status code and refused connection, just as we wanted. <p>To communicate again with the Cat using the WebSocket key, we just add a <code>token</code> query parameter to the connection URL: <code>ws://localhost:1865/ws?token=&lt;CCAT_API_KEY_WS&gt;</code>.</p> JavaScriptPython <pre><code>let ws = new WebSocket(\"ws://localhost:1865/ws?token=meow_ws\")\n\nws.onopen = function() {\n    ws.send(JSON.stringify({\"text\": \"It's late\"}))\n}\n\nws.onmessage = function(msg){\n    console.log(JSON.parse(msg.data))\n}\n</code></pre> <pre><code>import json\nimport asyncio\nfrom websockets.asyncio.client import connect\n\nasync def cat_chat():\n    async with connect(f\"ws://localhost:1865/ws?token=meow_ws\") as websocket:\n\n        await websocket.send(json.dumps({\"text\": \"It's late\"}))\n\n        async for message in websocket:\n            cat_response = json.loads(message)\n            print(cat_response[\"content\"])\n            if cat_response[\"type\"] == \"chat\":\n                break\n\nasyncio.run(cat_chat())\n</code></pre> <p>In the case of WebSocket, to indicate the <code>user_id</code> we can insert it into the address. For example if our user id is <code>Caterpillar</code>, we connect to <code>ws://localhost:1865/ws/Caterpillar?token=meow_ws</code></p>"},{"location":"production/auth/authentication/#2-securing-jwt","title":"2. Securing JWT","text":"<p>Even if we lock down HTTP and WebSocket endpoints with API keys, there is another access channel in the Cat we need to secure that is independent from the ones described above.</p> <p>JSON Web Token (JWT) authentication works for both HTTP and WebSocket endpoints, and it's recommended especially when using the Cat via browser:</p> <ul> <li>JWTs have a temporary lifespan, afterwards a new token must be generated. If somebody steals it from the browser, it can only be used for a short time.</li> <li>JWTs can contain additional information, in our case user id and permissions. No need to specify user id in the headers or in the address.</li> <li>We can reserve <code>CCAT_API_KEY</code> and <code>CCAT_API_KEY_WS</code> only for machine-to-machine communication.</li> </ul> <p>JWTs are secured by the environment variable <code>CCAT_JWT_SECRET</code>, which is used to generate and validate tokens. On a fresh installation, JWTs are already enabled with <code>CCAT_JWT_SECRET=secret</code>. This value must be changed.</p> <p>To complete our security setup, our <code>.env</code> should contain three secrets: </p> <pre><code>CCAT_API_KEY=meow\nCCAT_API_KEY_WS=meow_ws\nCCAT_JWT_SECRET=meow_jwt\n</code></pre> <p>Admin Panel already uses JWT</p> <p>The Admin panel handles authentication asking username and password, then asks the core to generate a JWT and then uses it to communicate with Cheshire Cat APIs. Choose a strong admin password and set a custom <code>CCAT_JWT_SECRET</code> if you expose the admin panel to the public.</p>"},{"location":"production/auth/authentication/#obtaining-a-jwt","title":"Obtaining a JWT","text":"<p>To generate a valid token make an HTTP POST request to <code>/auth/token</code>, including your username and password in the payload.</p> JavaScriptPython <pre><code>let response = await fetch(\n    \"http://localhost:1865/auth/token\",\n    {\n        \"method\": \"POST\",\n        \"headers\": {\n            \"Content-type\": \"application/json\",\n        },\n        \"body\": JSON.stringify({\n            \"username\": \"admin\",\n            \"password\": \"admin\"\n        })\n    }\n)\nlet json = await response.json()\nlet jwt = json[\"access_token\"]\n\nconsole.log(jwt)\n</code></pre> <pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:1865/auth/token\",\n    json = {\n        \"username\": \"admin\",\n        \"password\": \"admin\"\n    }\n)\n\n# Here's your JWT\njwt = response.json()[\"access_token\"]\nprint(jwt)\n</code></pre> <p>As you can see having an internal user in the Cat with specific username and password is a prerequisite to obtain a JWT. The same is not true for API key authentication, in which user can be created on the fly (see public users).</p> <p>You can make JWT work also for external users (e.g via identity provider or CMS) implementing a custom <code>AuthHandler</code></p>"},{"location":"production/auth/authentication/#using-jwt-for-http","title":"Using JWT for HTTP","text":"<p>Once you get the JWT token you can easily append it to your HTTP requests, like so:</p> JavaScriptPython <pre><code>response = await fetch(\n    \"http://localhost:1865\",\n    {\n        \"headers\": {\n            \"Authorization\": `Bearer ${jwt}`\n        }\n    }\n)\njson = await response.json()\n\nconsole.log(json)\n</code></pre> <pre><code>response = requests.get(\n    \"http://localhost:1865\",\n    headers={\n        \"Authorization\": f\"Bearer {jwt}\"\n    }\n)\n\nprint(response.json())\n</code></pre>"},{"location":"production/auth/authentication/#using-jwt-for-websocket","title":"Using JWT for WebSocket","text":"<p>Same goes to authenticate WebSocket:</p> JavaScriptPython <pre><code>let url = `ws://localhost:1865/ws?token=${jwt}`\nlet ws = new WebSocket(url)\n\nws.onopen = function() {\n    ws.send(JSON.stringify({\"text\": \"It's late\"}))\n}\n\nws.onmessage = function(msg){\n    console.log(JSON.parse(msg.data))\n}\n</code></pre> <pre><code>import json\nimport asyncio\nfrom websockets.asyncio.client import connect\n\nasync def cat_chat():\n    url = f\"ws://localhost:1865/ws?token={jwt}\"\n    async with connect(url) as websocket:\n\n        await websocket.send(json.dumps({\"text\": \"It's late\"}))\n\n        async for message in websocket:\n            cat_response = json.loads(message)\n            print(cat_response[\"content\"])\n            if cat_response[\"type\"] == \"chat\":\n                break\n\nasyncio.run(cat_chat())\n</code></pre>"},{"location":"production/auth/authentication/#refresh-jwt","title":"Refresh JWT","text":"<p>TODO</p>"},{"location":"production/auth/authentication/#3-secure-custom-endpoints","title":"3. Secure Custom Endpoints","text":"<p>Custom endpoints in plugins are open by default, unless you specify required permissions.</p>"},{"location":"production/auth/authentication/#4-use-secure-protocols","title":"4. Use Secure Protocols","text":"<p>The final step is to have the Cat behind a reverse proxy, hopefully with automatic TLS certificates.</p> <p>There are many open source reverse proxies available, most of them automatically manage certificates via Let's Encrypt. A few example setups are available here.</p>"},{"location":"production/auth/authorization/","title":"Authorization","text":""},{"location":"production/auth/authorization/#authorization","title":"Authorization","text":"<p>The Cat comes with its own roles and permissions to authorize requests and resource access. Each resource comes with granular roles for writing, editing, listing, reading and deleting (e.g. <code>CONVERSATION_WRITE</code>).  You can view the available permissions at <code>/auth/available-permissions</code>.</p> <p>If you have secured your HTTP/WS Cat endpoints, every incoming request using JWT authorization header will check for proper permissions for that user, otherwise, if you are using api keys you'll get full permission and will access every available resource.</p> <p>If you want to manage users permissions you easily do that via restful APIs or via Admin panel, head over to user-management for more info.</p>"},{"location":"production/auth/custom-auth/","title":"Custom Auth","text":""},{"location":"production/auth/custom-auth/#custom-auth-handler","title":"Custom Auth Handler","text":"<p>Want to bring in your own authentication system with your own users? The Cat provides a useful API to customize the default authentication flow and add a custom auth handler which delegates authentication and authorization to third party systems.</p>"},{"location":"production/auth/custom-auth/#preliminary-steps","title":"Preliminary steps","text":"<p>Since the Cat has its own auth handler mechanism, which is always inserted in the auth handler pipe, you need to lock your instance following the instructions in the authentication section.</p>"},{"location":"production/auth/custom-auth/#building-a-custom-authhandler","title":"Building a Custom AuthHandler","text":"<p>Let's walk through the process of creating a custom <code>AuthHandler</code>. While the exact implementation is subject to the provider or system you choose to rely on, the overall structure will remain the same.</p>"},{"location":"production/auth/custom-auth/#create-the-custom-handler","title":"Create the Custom Handler","text":"<p>Start by extending the <code>BaseAuthHandler</code> class to define the behaviour for handling tokens and validating users. Here's an example:</p> <pre><code>from cat.factory.custom_auth_handler import BaseAuthHandler\nfrom cat.auth.permissions import (\n    AuthPermission, AuthResource, AuthUserInfo, get_base_permissions\n)\nimport requests\n\nclass CustomAuthHandler(BaseAuthHandler):\n\n    def __init__(self, **config):\n        self.server_url = config.get(\"server_url\")\n\n\n    def authorize_user_from_jwt(self, token: str, auth_resource: str, auth_permission: str):\n        jwt_validation_url = f\"{self.server_url}/validate-token\"\n\n        response = requests.get(jwt_validation_url, headers={\n            \"Authorization\": f\"Bearer {token}\"\n        })\n\n        if response.status_code == 200:\n            user_data = response.json()\n\n            # Return the user information mapped to AuthUserInfo\n            return AuthUserInfo(\n                id=user_data.get(\"username\"),\n                name=user_data.get(\"name\", \"\"),\n                extra=user_data.get(\"extra\") # Optional data to be passed to the Cat\n            )\n        else:\n            return None  # If the server responds with an error or doesn't respond at all\n\n\n    def authorize_user_from_key(self, protocol: str, user_id: str, api_key: str, auth_resource, auth_permission):\n        # Optional: Handle API key authentication, if applicable\n        return None\n</code></pre>"},{"location":"production/auth/custom-auth/#create-the-custom-handler-config","title":"Create the Custom Handler Config","text":"<p>Create a new <code>AuthHandlerConfig</code> class that inherits from <code>BaseAuthHandlerConfig</code>. This will be the configuration class that contains all the necessary settings to initialize your custom handler.</p> <pre><code>from cat.factory.auth_handler import AuthHandlerConfig\nfrom typing import Type\nfrom pydantic import ConfigDict, Field\n\nclass CustomAuthHandlerConfig(AuthHandlerConfig):\n    _pyclass: Type = CustomAuthHandler\n\n    server_url: str = Field(..., description=\"The URL of the identity provider\")\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"humanReadableName\": \"Custom Auth Handler\",\n            \"description\": \"Delegate auth to a custom identity provider.\"\n        }\n    )\n</code></pre>"},{"location":"production/auth/custom-auth/#register-the-custom-handler","title":"Register the Custom Handler","text":"<p>Register the new handler with the <code>factory_auth_handlers</code> hook.</p> <pre><code>from cat.mad_hatter.decorators import hook\nfrom typing import List\n\n@hook(priority=0)\ndef factory_allowed_auth_handlers(allowed: List[AuthHandlerConfig], cat) -&gt; List:\n    allowed.append(CustomAuthHandlerConfig)\n    return allowed\n</code></pre>"},{"location":"production/auth/custom-auth/#activate-the-custom-handler","title":"Activate the Custom Handler","text":"<p>Activate the new handler using the proper endpoint in your Cat installation.</p> <pre><code>curl --location --request PUT 'http://{your_cat_instance}/auth_handler/settings/CustomAuthHandlerConfig' \\\n--header 'Content-Type: application/json' \\\n--data '{\n   \"server_url\": {your_idp_url},\n}'\n</code></pre> <p>That's it! Now every incoming request will be authenticated using the custom auth handler.</p>"},{"location":"production/auth/custom-auth/#key-components-and-concepts","title":"Key Components and Concepts","text":""},{"location":"production/auth/custom-auth/#baseauthhandler","title":"BaseAuthHandler","text":"<p><code>BaseAuthHandler</code> is the class that you will extend to implement your custom auth flow. This is an abstract class that forces you to implement two basic methods: <code>authorize_user_from_jwt</code> and <code>authorize_user_from_key</code>. You'll write your own auth logic in there inserting, for example, credentials validation request to external identity providers.</p> <ul> <li><code>authorize_user_from_jwt</code>: Validates and processes incoming JWT tokens, extracting user information and </li> <li>mapping it into Cheshire Cat\u2019s <code>AuthUserInfo</code> structure. </li> <li><code>authorize_user_from_key</code>: Allows validation of requests based on an API key, usually used for machine to machine communication.</li> </ul>"},{"location":"production/auth/custom-auth/#authhandlerconfig","title":"AuthHandlerConfig","text":"<p>The <code>AuthHandlerConfig</code> class provides a base configuration that you can extend to define custom settings for your  authentication handler. It includes all the necessary settings to initialize your custom handler. These may include: information about the external identity provider, authentication endpoint, client secrets, etc.</p>"},{"location":"production/auth/custom-auth/#allowed-auth-handlers-hook","title":"Allowed Auth Handlers Hook","text":"<p>The <code>factory_allowed_auth_handlers</code> hook allows you to add your own auth handlers to the list of allowed auth handlers.</p>"},{"location":"production/auth/custom-auth/#authuserinfo","title":"AuthUserInfo","text":"<p>The <code>AuthUserInfo</code> class represents the decoded content of an authentication token. This class is used to standardize the  output of AuthHandlers, ensuring that token details are consistent across different authentication systems.  The AuthUserInfo object is crucial for session management within Cat's core, as it either retrieves or creates a user  session, known as a StrayCat. </p>"},{"location":"production/auth/user-management/","title":"User Management","text":""},{"location":"production/auth/user-management/#user-management","title":"User Management","text":"<p>We divide users in 3 categories:</p> User type Use case Where are they stored Authentication method Public Website support chat Nowhere API keys (<code>CCAT_API_KEY</code>, <code>CCAT_API_KEY_WS</code>) and <code>user_id</code> Internal Company assistant Cat core JSON Web Token (JWT) Custom Your imagination Your identity provider Your custom <code>AuthHandler</code> <p>If you are developing an application that supports multiple users, it's crucial to ensure that each user's session and memories are isolated, with granular access.</p> <p>Each user interacts with the Cat via a dedicated <code>StrayCat</code> instance, which in turn you will be able to use in your plugins as <code>cat</code>.</p> <p>Let's now see the 3 types of users in detail.</p>"},{"location":"production/auth/user-management/#public-users","title":"Public Users","text":"<p>Users are public when they are not stored in core or in an external identity provider (like KeyCloak), but they are still allowed to use endpoints. A typical use case is a customer support AI on a public website, where you don't want to register and store users.</p> <p>Public users will be created, kept during the conversation, then permanently deleted. To allow them access to the Cat you need to provide in each request a <code>user_id</code> and a credential (if required).</p>"},{"location":"production/auth/user-management/#id","title":"ID","text":"<p>By default, the Cat requires a unique user identifier to associate sessions and memory data with individual users.  Generate this temporary identifier as you see fit and pass it as <code>user_id</code> to HTTP endpoints or to the WebSocket messaging interface.</p> <p>You can pass a user ID to WebSocket endpoint by changing the address: <code>ws://localhost:1865/ws/caterpillar_123456</code></p> <p>For HTTP endpoints, just include a <code>user_id</code> header in the request: <code>user_id</code>: <code>caterpillar_123456</code></p> <p>Note</p> <p>If no user id is provided, the Cat will assume <code>user_id = \"user\"</code>.</p>"},{"location":"production/auth/user-management/#credentials","title":"Credentials","text":"<p>If you set up <code>CCAT_API_KEY</code> and <code>CCAT_API_KEY_WS</code>, public users still need to provide those credentials, respectively via http and websocket.</p> <p>If you did not set them, at your own peril all endpoints are wide open and you just need to specify the <code>user_id</code>.  </p> <p>Most people building public chatbots leave websocket open and lock down all http endpoints. In any case it is recommended that you do this kind of requests server side, from a smartphone app, or between containers in a private docker network. Avoid using your keys in a browser or any other transparent client.</p>"},{"location":"production/auth/user-management/#internal-users","title":"Internal Users","text":"<p>In use cases in which a specific and restricted set of users will use your Cat, you want dedicated credentials, detailed management and permanent storage.</p>"},{"location":"production/auth/user-management/#management","title":"Management","text":"<p>The Cat framework includes a simple, internal user management system for creating, retrieving, updating, and deleting users. Endpoints for user management can be found on your installation under <code>/docs</code>.</p> <p>If you're looking for a straightforward way to manage users, you can use the Admin panel. Simply click on the <code>Settings</code> tab and you'll see a <code>User Management</code> section.</p>"},{"location":"production/auth/user-management/#credentials_1","title":"Credentials","text":"<p>Once registered into the Cat with username and password, a user can be assigned granular permissions and can access endpoints with a JWT (JSON Web Token).</p> <p>Detailed instructions on how to obtain and use the JWT are here. Remember to customize your JWT secret via the <code>CCAT_JWT_SECRET</code> environment variable.</p> <p>Note</p> <p>When authenticating requests with a JWT token, you do not need to pass the <code>user_id</code>; The Cat will automatically extract it from the token.</p>"},{"location":"production/auth/user-management/#custom-users","title":"Custom Users","text":"<p>If you already have a user management system or identity provider, you can easily integrate it with the Cat by implementing a custom <code>AuthHandler</code>.</p> <p>This allows you to use your own authentication logic and pass the necessary <code>AuthUserInfo</code> to the Cat. For more details, refer to the custom auth guide.</p>"},{"location":"production/auth/user-management/#credentials_2","title":"Credentials","text":"<p>While you can customize the <code>AuthHandler</code> to assign identity and permissions via any identity provider, we enforce a standard on how credentials must be sent to the Cat:</p> <ul> <li>for http, via <code>Authentication: Bearer &lt;credential&gt;</code> header</li> <li>for websocket, via <code>?token=&lt;credential&gt;</code> query parameter </li> <li>the two above are valid for both api keys and JWT</li> </ul> <p>This allows for auth and user management customization without breaking the many client libraries and tools the community is building.</p>"},{"location":"production/auth/user-management/#examples","title":"Examples","text":""},{"location":"production/auth/user-management/#access-current-user-from-a-plugin","title":"Access current user from a plugin","text":"<p>In hooks, tools, forms and custom endpoints you can easily obtain user information from the <code>cat</code> variable, instance of <code>StrayCat</code>.</p> <pre><code>from cat.mad_hatter.decorators import tool\n\n@tool(return_direct=True)\ndef who_am_i(arg, cat):\n    \"\"\"Use to retrieve info about the current user.\"\"\"\n    return f\"Hello {cat.user_id}, here is some info about you: {cat.user_data.model_dump_json()}\"\n</code></pre>"},{"location":"production/auth/user-management/#users-and-memories","title":"Users and memories","text":"<p>In the Cat, users and memories are closely related. Without user-specific memory isolation, The Cat cannot maintain context across conversations. By default, the user system affects only the working memory and the episodic memory. The other memories are shared among users, but you could easily think about a custom plugin to store and restrict access to data in Long Term Memory based on <code>user_id</code>.</p> <p>Here's an example of how the <code>user_id</code> could be used to filter declarative memories both on the uploading and retrieval side:</p> <pre><code>from cat.mad_hatter.decorators import hook\n\n@hook\ndef before_rabbithole_insert_memory(doc, cat):\n    # insert the user id metadata\n    doc.metadata[\"author\"] = cat.user_id\n    return doc\n\n@hook\ndef before_cat_recalls_declarative_memories(declarative_recall_config, cat):\n    # filter memories using the user_id as metadata. \n    declarative_recall_config[\"metadata\"] = {\"author\": cat.user_id}\n    return declarative_recall_config\n</code></pre>"},{"location":"production/network/clients/","title":"Clients","text":""},{"location":"production/network/clients/#clients","title":"Clients","text":"<p>How do you talk with the Cat from a script, from the browser or from a server? The Cat is an AI microservice, which means it is intended to be added to any application as a conversational layer.</p> <p>You can check out the REST API playground directly on your installation, at <code>localhost:1865/docs</code>.</p> <p>There are several client libraries that may help you to easily chat with the Cat and call its endpoints from another application. Below we list client libraries in many languages, web widgets and client apps.</p> <p>Some of the clients are maintained from the community, some others come from individual contributors. If you find any problem, open an issue or PR on the dedicated repository - be nice!</p>"},{"location":"production/network/clients/#client-libraries","title":"Client Libraries","text":"Language Repository Typescript/Javascript Client Python Client C# Client PHP Client Ruby Client Go Client Java Client"},{"location":"production/network/clients/#frontend-chat-widgets","title":"Frontend Chat Widgets","text":"Framework Repository Vue Widget Alpine Widget Svelte Widget React + TS Widget React Widget Angular Widget"},{"location":"production/network/clients/#apps-integrations","title":"Apps &amp; Integrations","text":"Platform Repository Telegram bot Discord bot Nginx composeNginx compose with SSL Tipi App Laravel SDK WordPress plugin Flutter-Cat"},{"location":"production/network/clients/#command-line-interfaces","title":"Command Line Interfaces","text":"Language Repository Meow CLI Cheshire Cat CLI"},{"location":"production/network/http-endpoints/","title":"HTTP Endpoints","text":"<p>You can play around with the HTTP endpoints directly on your installation, under <code>localhost:1865/docs</code>. You will find there most of the documentation you need, alongside code snippets in various languages and a useful playground to try them out.  </p>"},{"location":"production/network/http-endpoints/#examples","title":"Examples","text":"<p>Check out also the the community client libraries.</p> <p>When you feel ready you can also create custom endpoints direclty in your plugins.</p>"},{"location":"production/network/ws-endpoints/","title":"WebSocket Endpoints","text":""},{"location":"production/network/ws-endpoints/#why-websocket","title":"Why WebSocket","text":"<p>While most network communication with LLMs is performed via HTTP, the Cat features full duplex communication using the WebSocket protocol, for the following reasons:</p> <ul> <li>two way communication:   you send messages to the Cat, and the Cat can send messages to you, even if you did not summon it (think about an alarm clock). This would not be possible via HTTP.</li> <li>multiprompt agent: each message to the Cat may involve the execution of multiple prompts and API calls done from plugins, making HTTP streaming risky of a timeout.</li> <li>token streaming: each token will have its own dedicated ws message.</li> <li>notifications: your client will receive ws notifications, e.g. when an upload is finished or when a plugin wants to send one.</li> </ul> <p>If you are new to WebSocket, we suggest you use one of the community client libraries that do most of the work for you.</p>"},{"location":"production/network/ws-endpoints/#example","title":"Example","text":"<p>TODO</p>"},{"location":"quickstart/conclusion/","title":"Conclusion","text":""},{"location":"quickstart/conclusion/#conclusion","title":"\ud83d\ude80 Conclusion","text":"<p>Congratulations on completing the quickstart! You should now have an overview of how to install the Cat and its main capabilities!</p>"},{"location":"quickstart/conclusion/#whats-next","title":"What's Next?","text":"<p>We encourage you to explore the Cat further and try writing and publishing your own plugins to enhance your experience.  </p> <p>If you have any questions or need assistance, feel free to join our  Discord community to:</p> <ul> <li>connect with other developers</li> <li>ask for support to the contributors</li> <li>see what people have already done with the Cat</li> <li>join regular dev &amp; user meetings</li> <li>follow monthly live tutorials called Meow Talks</li> </ul>"},{"location":"quickstart/installation-configuration/","title":"Installation and First Configuration","text":""},{"location":"quickstart/installation-configuration/#installation-and-first-configuration","title":"\ud83d\ude80 Installation and First configuration","text":""},{"location":"quickstart/installation-configuration/#requirements","title":"Requirements","text":"<p>To run the Cheshire Cat, you need to have <code>Docker</code> (instructions) and <code>docker compose</code> (instructions) already installed on your system.</p> <p>The Cat is not a LLM, it uses a LLM. Hence, when you run the Cat for the first time, you need to configure the LLM and the embedder. Most people use ChatGPT, it's quite cheap and powerful enough. We will do the same during the next steps.</p> <p>To use <code>ChatGPT</code>, you need an API key. You can request one on the provider's website:</p> <ul> <li>visit your OpenAI API Keys page</li> <li>create an API key with <code>+ Create new secret key</code> and copy it</li> </ul>"},{"location":"quickstart/installation-configuration/#setup","title":"Setup","text":"<p>Create a folder on your machine, we will use <code>cheshire-cat-ai</code>, and inside it create a file named <code>compose.yml</code>. Copy/paste the following inside:</p> <pre><code>services:\n\n  cheshire-cat-core:\n    image: ghcr.io/cheshire-cat-ai/core:latest\n    container_name: cheshire_cat_core\n    ports:\n      - 1865:80\n      - 5678:5678\n    volumes:\n      - ./static:/app/cat/static\n      - ./plugins:/app/cat/plugins\n      - ./data:/app/cat/data\n</code></pre>"},{"location":"quickstart/installation-configuration/#starting-the-cat","title":"Starting the Cat","text":"<ul> <li>Open a terminal inside the new folder and run:</li> </ul> <pre><code>docker compose up\n</code></pre> <p>The first time you run the <code>docker compose up</code> command, it will take several minutes to pull the Docker Cat image depending on network connection. Once the download is complete, the startup process will begin.</p> <p>When you see the fantastic Cheshire Cat logo in terminal, it means that everything it's up and running!</p> <p></p> <p>Inside the new folder, you will see three newly created directories:</p> <ul> <li><code>data</code>: where long term memory and settings are stored</li> <li><code>plugins</code>: where we will install and develop plugins</li> <li><code>static</code>: folder to serve static files from </li> </ul> <p>These directories will retain your work even if the container is deleted.</p>"},{"location":"quickstart/installation-configuration/#stopping-the-cat","title":"Stopping the Cat","text":"<p>Stop the terminal with <code>CTRL + C</code>.</p>"},{"location":"quickstart/installation-configuration/#starting-the-cat-in-background","title":"Starting the Cat in background","text":"<p>Now start again the container but in background mode, use the <code>--detach</code> or <code>-d</code> flag to the command, as: </p><pre><code>docker compose up -d\n</code></pre> In this way the terminal won't be locked by the docker compose execution. <p>To check the logs do the following:</p> <pre><code>docker compose logs -f\n</code></pre> <p>To stop the container, use this command in a separate terminal session:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"quickstart/installation-configuration/#first-configuration-of-the-llm","title":"First configuration of the LLM","text":"<ul> <li>Start the Cat if it's stopped</li> <li>Open the <code>Admin Portal</code> in your browser at <code>localhost:1865/admin</code></li> <li>Authenticate as administrator with user <code>admin</code> and password <code>admin</code>: </li> <li>In the <code>Settings</code> tab configure the <code>Large Language Model</code> and the <code>Embedder</code> and paste your API key: video here</li> </ul>"},{"location":"quickstart/installation-configuration/#next-step","title":"Next step","text":"<p>In the next step, you will learn how to play with the Cat.</p>"},{"location":"quickstart/installing-plugin/","title":"Installing a Plugin","text":""},{"location":"quickstart/installing-plugin/#installing-a-plugin-from-the-registry","title":"\ud83d\udce5 Installing a Plugin from the Registry","text":"<p>Plugins are a way to enhance Cheshire Cat AI. Developers from around the world create plugins and can choose to publish them in our public registry.</p> <p>Installing plugins from the registry is a seamless process that further enhances your Cheshire Cat AI experience. Whether you're seeking specific functionalities or exploring new features, our registry offers a diverse range of plugins ready for installation.</p>"},{"location":"quickstart/installing-plugin/#through-the-admin-dashboard","title":"Through the Admin Dashboard","text":"<ol> <li>Navigation: Access the Cheshire Cat AI Admin.</li> <li>Plugins Tab: Click on the \"Plugins\" tab within the dashboard.</li> <li>Search and Filter: Use the search or filter options to locate your desired plugin.</li> <li>Installation: Once you've found the plugin, click the \"Install\" button.</li> <li>Wait for Completion: The admin will show a loading spinner until the plugin installation is not completed.</li> </ol>"},{"location":"quickstart/installing-plugin/#manual-installation","title":"Manual Installation","text":"<p>For those inclined towards manual installation, follow these steps:</p> <ol> <li>Download the Zip: Access the plugin of interest in the registry following the GitHub URL and download its zip file.</li> <li>Upload: In the top right corner of the Plugins page, locate and click the \"Upload Plugin\" button.</li> <li>Upload Zip: Upload the downloaded zip file using this feature.</li> </ol> <p>Manual installation grants users more control over the process and facilitates the installation of specific plugins outside the registry itself.</p>"},{"location":"quickstart/installing-plugin/#post-installation-steps","title":"Post-Installation Steps","text":"<p>After installing a plugin, consider these steps:</p> <ul> <li>Refresh: The admin refreshes automatically after the installation but if for some reason the plugin does not show, refresh the page or check cat logs for any errors;</li> <li>Settings Configuration: If the newly installed plugin requires setup or configuration, look for the cog icon associated with the plugin. Click on it to access and adjust the plugin's settings according to your preferences.</li> </ul>"},{"location":"quickstart/installing-plugin/#next-step","title":"Next Step","text":"<p>In the next step, you will learn how easy it is to create your own plugin.</p>"},{"location":"quickstart/introduction/","title":"Introduction","text":""},{"location":"quickstart/introduction/#introduction","title":"Introduction","text":"<p>There are a few things you need to know about the Cheshire Cat. If you are eager to launch and start hacking, jump to the installation page, but please be sure to return here.</p> <p>The Cheshire Cat is a ready-to-use AI micro-framework. Once installed and connected to a Large Language Model (LLM), it can be queried through APIs. These APIs return the responses provided by the LLM.</p> <p>But this is just the beginning.</p>"},{"location":"quickstart/introduction/#previous-conversation-history","title":"Previous Conversation History","text":"<p>All previous conversations are stored in a local database called <code>episodic memory</code>. When you ask a question, the Cat answers taking into account the past conversations.</p>"},{"location":"quickstart/introduction/#loading-documents","title":"Loading Documents","text":"<p>You can load text documents as well. These documents are also saved in a local database called <code>declarative memory</code>. When answering, the Cat will consider the information within these documents. Documents can be uploaded through the APIs or the <code>Admin Portal</code>.</p> <p>The <code>Rabbit Hole</code> is the component responsible for the document ingestion.</p>"},{"location":"quickstart/introduction/#performing-actions","title":"Performing Actions","text":"<p>The Cheshire Cat isn't limited to just answering questions; it can also perform actions. You can write Python functions called <code>Tools</code> and have the LLM execute this code. The only limit to the Python code's capabilities is your imagination.</p>"},{"location":"quickstart/introduction/#executing-actions-in-the-future","title":"Executing Actions in the Future","text":"<p>Actions are not limited to being triggered immediately after a chat initiated by a human, they can be scheduled for future execution and, if needed, set to recur, or even initiated without any chat trigger.</p> <p>The <code>White Rabbit</code> is the component that triggers the scheduled Actions.</p>"},{"location":"quickstart/introduction/#extending-the-core","title":"Extending the Core","text":"<p>Additionally, it's possible to customize the Cheshire Cat's core. In the main process flow, there are predefined adaptation points called <code>Hooks</code>. You can write Python functions that can be attached onto these <code>Hooks</code>. The attached code will be invoked during the flow's execution and can modify the Cheshire Cat's internal behavior, without directly modifying the core of the Cheshire Cat.</p> <p><code>Tools</code> and <code>Hooks</code> are packaged into <code>Plugins</code> that can be installed by placing files in a specific folder or using the <code>Admin Portal</code>.</p> <p>The <code>Mad Hatter</code> is the component that manages plugins.</p>"},{"location":"quickstart/introduction/#sharing-plugins","title":"Sharing Plugins","text":"<p>If desired, you can  <code>publish your Plugins</code> on the public registry. Other users will be able to install them with just a single click.</p>"},{"location":"quickstart/introduction/#admin-portal","title":"Admin Portal","text":"<p>A web portal for admin users completes the framework. Using this portal, the admin can configure the settings, upload documents, install plugins and use it as a playground tool. You can chat with the Cheshire Cat, inspect its responses and directly query its memories.</p>"},{"location":"quickstart/introduction/#next-step","title":"Next step","text":"<p>In the next step, you will learn how to install the Cat, set the LLM and the basics of this all.</p> <p>We will be transforming the Cat into a sock seller. More in detail, we will upload some knowledge (documents) about socks knitting. Also, the Cat will be able to tell the price of socks according to the requested color (using a <code>Tools</code>). In the end, we will transform the sock seller into a poetic socks seller, changing its personality (using a <code>Hooks</code>). </p> <p>The example is light and fun, it should give you an idea of what is possible.</p>"},{"location":"quickstart/play-with-the-cat/","title":"Playing with the Cat","text":""},{"location":"quickstart/play-with-the-cat/#play-with-the-cat","title":"Play with the Cat","text":""},{"location":"quickstart/play-with-the-cat/#chatting-with-the-cat-the-admin-portal-playground","title":"Chatting with the Cat: the Admin Portal playground","text":"<p>The Cat is an API-first framework, and it doesn't provide a ready-to-use UI for the end user. It is your responsibility to implement this UI. However, the Cat offers a playground that you can use to quickly test the AI you are implementing.</p> <p>To access playground, go to the Admin Portal at <code>localhost:1865/admin</code>, and click on the <code>Home</code> tab. This tab serves as the playground for chatting with the Cat.</p> <p>Try to ask something about socks, e.g. \"what do you know about socks?\". The Cat will give a generic answer. Afterward, we will expand this general knowledge with more specific information.</p> <p></p>"},{"location":"quickstart/play-with-the-cat/#chatting-with-the-cat-api-interaction","title":"Chatting with the Cat: API interaction","text":"<p>The Cat is an API-first framework, you can chat with it using the WebSocket protocol. Here is an example of how to do it in Javascript and Python:</p> JavaScriptPython <pre><code>let ws = new WebSocket(\"ws://localhost:1865/ws\")\n\nlet humanTurn = function(){\n    let msg = prompt(\"Human message:\")\n    if(msg) {\n        ws.send(JSON.stringify({\"text\": msg}))\n    }\n}\n\nws.onopen = function(){\n    humanTurn()\n}\n\nws.onmessage = function(msg){\n    let msg_parsed = JSON.parse(msg.data)\n    console.log(msg_parsed.content)\n    if(msg_parsed.type == \"chat\") {\n        humanTurn()   \n    }\n}\n</code></pre> <pre><code>import json\nimport asyncio\nfrom websockets.asyncio.client import connect\n\nasync def cat_chat():\n    # Creating a websocket connection\n    async with connect(\"ws://localhost:1865/ws\") as websocket:\n        while True:\n            # Taking user input and sending it through the websocket\n            user_input = input(\"Human: \")\n            await websocket.send(json.dumps({\"text\": user_input}))\n\n            # Receiving and printing the cat's response\n            async for message in websocket:\n                cat_response = json.loads(message)\n                print(cat_response[\"content\"])\n\n                # Human turn!\n                if cat_response[\"type\"] == \"chat\":\n                    break\n\n# To stop the script: CTRL + c\nasyncio.run(cat_chat())\n</code></pre> <p>Run it and ask <code>what do you know about socks?</code> again, the output in the terminal should look like:</p> <pre><code>\u276f python main.py\nHuman: what do you know about socks?\nCheshire Cat: Socks, my dear, are like the hidden wonders of the wardrobe! They can be plain or patterned, warm or cool, and often seem to vanish into thin air. Do you have a favorite pair?\n</code></pre>"},{"location":"quickstart/play-with-the-cat/#more-info","title":"More Info","text":"<p>This example explains the Raw use of the Cat APIs, however there are convenient and ready-made libraries available for various languages! <code>Production \u2192 Client Libraries</code></p> <p>For more methods of authentication look at <code>Production \u2192 Authentication</code></p>"},{"location":"quickstart/play-with-the-cat/#next-step","title":"Next Step","text":"<p>In the next step, you will learn how to load information into the Cat by uploading documents.</p>"},{"location":"quickstart/prepare-plugin/","title":"Creating a Plugin","text":""},{"location":"quickstart/prepare-plugin/#your-first-plugin","title":"\ud83d\udd0c Your first Plugin","text":"<p>Plugins are packages of <code>Tools</code> and <code>Hooks</code>. You don't distribute a <code>Tool</code> or a <code>Hook</code> directly, you will distribute a Plugin containing them. Don't worry, we'll explore what tools and hooks are in details in the upcoming steps. For now, let's start by creating an empty plugin.</p>"},{"location":"quickstart/prepare-plugin/#creating-the-plugin","title":"Creating the Plugin","text":"<p>To create a plugin just create a new subfolder in directory <code>plugins/</code>, for our first plugin the folder name will be <code>poetic_sock_seller</code>.</p> <p>You need two files in your plugin folder:</p> <pre><code>cheshire-cat-ai\n\u251c\u2500\u2500 compose.yml\n\u251c\u2500\u2500 data\n\u251c\u2500\u2500 plugins\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 poetic_sock_seller\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 plugin.json\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 poetic_sock_seller.py\n\u2514\u2500\u2500 static\n</code></pre> <p>The <code>plugin.json</code> file contains plugin's title and description, and is useful in the Admin Portal to recognize the plugin and activate/deactivate it.</p> <p><code>plugin.json</code> example:</p> <pre><code>{\n    \"name\": \"Poetic Sock Seller\",\n    \"description\": \"Description of poetic_sock_seller\"\n}\n</code></pre> <p>The <code>poetic_sock_seller.py</code> file will contain <code>Tools</code> and <code>Hooks</code> source code and can be left completely empty for this step.</p>"},{"location":"quickstart/prepare-plugin/#activating-the-plugin","title":"Activating the Plugin","text":"<p>Now, go to the <code>Plugin</code> tab of the Admin Portal. Your empty plugin will be there, activate it:</p> <p></p>"},{"location":"quickstart/prepare-plugin/#more-info","title":"More Info","text":"<p>Here the plugins reference: <code>Plugins</code></p> <p>If you plan to publish your plugin, also take a look at this <code>Plugins \u2192 Registry</code></p>"},{"location":"quickstart/prepare-plugin/#next-step","title":"Next Step","text":"<p>In the next step, you will learn how to create your first <code>Tool</code> inside the plugin.</p>"},{"location":"quickstart/upload-document/","title":"Uploading a Document","text":""},{"location":"quickstart/upload-document/#upload-a-document","title":"Upload a Document","text":"<p>Documents can be uploaded via the Admin Portal (and it's also using APIs). The Cat will consider uploaded documents to generate the answer to your question. These documents are saved in a local database called <code>declarative memory</code>.</p>"},{"location":"quickstart/upload-document/#improve-the-cat-knowledge","title":"Improve the Cat knowledge","text":"<p>The Cat's knowledge about socks is quite basic; we will upload more specific knowledge.</p> <p>Go to the Admin Portal at <code>localhost:1865/admin</code> on the <code>Home</code> tab, click on the <code>Flash Icon</code>, then click on <code>Upload url</code> and use this url <code>https://en.wikipedia.org/wiki/N%C3%A5lebinding</code>:</p> <p></p> <p>You receive a notification of the ingesting operation:</p> <p></p> <p>You receive notification of the finished read:</p> <p></p>"},{"location":"quickstart/upload-document/#trying-new-knowledge","title":"Trying new knowledge","text":"<p>The Cat can answer with more detailed answers:</p> <p></p>"},{"location":"quickstart/upload-document/#why-the-response","title":"Why the response?","text":"<p>By clicking on the question mark next to the answer, you can understand what prompted the Cat to provide the response. In this case, you can see that it used the knowledge coming from the documents (<code>declarative memory</code>):</p> <p></p>"},{"location":"quickstart/upload-document/#next-step","title":"Next Step","text":"<p>In the next step, you will learn what a plugin is and how to install it.</p>"},{"location":"quickstart/writing-hook/","title":"Writing the first Hook","text":""},{"location":"quickstart/writing-hook/#writing-the-first-hook","title":"Writing the first Hook","text":"<p>Hooks are Python functions that can be attached onto specific parts of the Cat's core. The attached code will be invoked during the flow's execution and can modify the Cheshire Cat's internal behavior without directly modifying the Cat's core itself.</p>"},{"location":"quickstart/writing-hook/#transform-the-cat-into-a-poetic-socks-seller","title":"Transform the Cat into a Poetic Socks Seller","text":"<p>At the moment, if you ask the Cat 'Who are you?', it will introduce itself as the Cheshire Cat AI. To change the Cat's personality, for example, to make it a poetic socks seller, you can use the <code>agent_prompt_prefix</code> hook. Copy and append the following source code to the file <code>poetic_sock_seller.py</code>:</p> <pre><code>from cat.mad_hatter.decorators import hook\n\n@hook\ndef agent_prompt_prefix(prefix, cat):\n\n    prefix = \"\"\"You are Marvin the socks seller, a poetic vendor of socks.\nYou are an expert in socks, and you reply with exactly one rhyme.\n\"\"\"\n\n    return prefix\n</code></pre>"},{"location":"quickstart/writing-hook/#testing-the-hook","title":"Testing the Hook","text":"<p>Now, let\u2019s ask again \u201cwho are you?\u201d and for our favorite socks color:</p> <p></p>"},{"location":"quickstart/writing-hook/#explaining-the-code-step-by-step","title":"Explaining the code step by step","text":"<pre><code>from cat.mad_hatter.decorators import hook\n</code></pre> <p>Let\u2019s import from the Cat the hook decorator. If you don\u2019t know what decorators are in coding, don\u2019t worry: they will help us attach our python functions to the Cat. The <code>mad_hatter</code> is the Cat component that manages and runs plugins.</p> <pre><code>@hook\ndef agent_prompt_prefix(prefix, cat):\n\n    prefix = \"\"\"You are Marvin the socks seller, a poetic vendor of socks.\nYou are an expert in socks, and you reply with exactly one rhyme.\n\"\"\"\n\n    return prefix\n</code></pre> <p>Here, we've defined a Python function called <code>agent_prompt_prefix</code>. It takes <code>cat</code> as an argument and is decorated with <code>@hook</code>. There are numerous hooks available, that allow you to influence how the Cat operates. The <code>agent_prompt_prefix</code> hook, in particular, allows instructing the Cat about who it is and how he should answer.</p>"},{"location":"quickstart/writing-hook/#more-info","title":"More Info","text":"<p>Hooks reference: Plugins \u2192 Hooks</p> <p>The plugin management flow also is customizable (using the <code>plugin</code> decorator instead of <code>hook</code> ones). Check out this for more information</p>"},{"location":"quickstart/writing-hook/#next-step","title":"Next Step","text":"<p>In the next step, we will summarizes and wraps up.</p>"},{"location":"quickstart/writing-tool/","title":"Writing the first Tool","text":""},{"location":"quickstart/writing-tool/#writing-the-first-tool","title":"Writing the first Tool","text":"<p>Tools are Python functions called by the LLM to execute actions. They are made of two parts: the first one contains instructions that explain the LLM when and how to call function; the second one contains the actual code to execute.</p>"},{"location":"quickstart/writing-tool/#creating-the-tool","title":"Creating the Tool","text":"<p>Now, let\u2019s get down to business. A real socks sales representative offers a quantity of socks, with many colors and corresponding price. Let\u2019s say a customer wants to know the price for socks of a specific color. We could write a tool to answer the question. Therefore, copy and past this source code inside the file <code>poetic_sock_seller.py</code>:</p> <pre><code>from cat.mad_hatter.decorators import tool\n\n@tool\ndef socks_prices(color, cat):\n    \"\"\"How much do socks cost? Input is the sock color.\"\"\"\n    prices = {\n        \"black\": 5,\n        \"white\": 10,\n        \"pink\": 50,\n    }\n    if color not in prices.keys():\n        return f\"No {color} socks\"\n    else:\n        return f\"{prices[color]} \u20ac\" \n</code></pre>"},{"location":"quickstart/writing-tool/#testing-the-tool","title":"Testing the Tool","text":"<p>Now, let\u2019s ask for our favorite socks color:</p> <p></p>"},{"location":"quickstart/writing-tool/#why-the-response","title":"Why the response?","text":"<p>By clicking on the question mark next to the answer, you can understand what prompted the Cat to provide the response. In this case, you can see that our tool \"socks_prices\" was used:</p> <p></p>"},{"location":"quickstart/writing-tool/#explaining-the-code-step-by-step","title":"Explaining the code step by step","text":"<pre><code>from cat.mad_hatter.decorators import tool\n</code></pre> <p>Let\u2019s import the tool decorator from the Cat. If you don\u2019t know what decorators are in coding, don\u2019t worry: they will help us attach our python functions to the Cat. The <code>mad_hatter</code> is the Cat component that manages and runs plugins.</p> <pre><code>@tool\ndef socks_prices(color, cat):\n    \"\"\"How much do socks cost? Input is the sock color.\"\"\"\n</code></pre> <p>We define a function called \"socks_prices,\" which takes as input the color of the desired socks and a cat instance.</p> <p>The <code>@tool()</code> decorator has the main function of letting the Cat know that the following function is a tool.</p> <p>The docstring just after the function signature reads as follows:</p> <p>\"How much do socks cost? Input is the sock color.\"</p> <p>This description instructs the LLM on when to call this tool and describes what input to provide.</p> <p>Going back to the tool actual code:</p> <pre><code>    prices = {\n        \"black\": 5,\n        \"white\": 10,\n        \"pink\": 50,\n    }\n\n    if color not in prices.keys():\n        return f\"No {color} socks\"\n    else:\n        return f\"{prices[color]} \u20ac\" \n</code></pre> <p>Not much to say here: we just check if the color is present in the dictionary and output the price. What is indeed interesting is that, in a tool, you can connect your AI to any service, database, file, device, or whatever you need. Imagine turning on and off the light in your room, searching an e-commerce or writing an email. The only limit is your fantasy \ud83d\ude00.</p>"},{"location":"quickstart/writing-tool/#watchfiles-detected-changes-reloading","title":"WatchFiles detected changes... reloading","text":"<p>When changes to the plugin's source code are detected, the Cat automatically restarts. Feel free to make changes within the code and observe the results.</p>"},{"location":"quickstart/writing-tool/#more-info","title":"More Info","text":"<p>Tools reference: Plugins \u2192 Tools</p> <p>Sometimes debugging can be necessary, please refer to the <code>Plugins \u2192 Debugging</code> section for more information.</p> <p>If you need info about running actions in future see <code>Framework \u2192 White Rabbit</code></p>"},{"location":"quickstart/writing-tool/#next-step","title":"Next Step","text":"<p>In the next step, you will learn how to create your first <code>Hook</code>.</p>"}]}